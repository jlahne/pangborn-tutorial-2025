# Wrangling data with `tidyverse`

```{r setup, include = FALSE}
library(tidyverse)
beer_data <- read_csv("data/clt-berry-data.csv")
```

A common saying in data science is that about 90% of the effort in an analysis workflow is in getting data wrangled into the right format and shape, and 10% is actual analysis.  In a point and click program like SPSS or XLSTAT we don't think about this as much because the activity of reshaping the data--making it longer or wider as required, finding and cleaning missing values, selecting columns or rows, etc--is often temporally and programmatically separated from the "actual" analysis. 

In `R`, this can feel a bit different because we are using the same interface to manipulate our data and to analyze it.  Sometimes we'll want to jump back out to a spreadsheet program like Excel or even the command line (the "shell" like `bash` or `zsh`) to make some changes.  But in general the tools for manipulating data in `R` are both more powerful and more easily used than doing these activities by hand, and you will make yourself a much more effective analyst by mastering these basic tools.

Here, we are going to emphasize the set of tools from the [`tidyverse`](https://www.tidyverse.org/), which are extensively documented in Hadley Wickham's and Garrett Grolemund's book [*R for Data Science*](https://r4ds.had.co.nz/).  If you want to learn more, start there!

<center>

![The `tidyverse` is associated with this hexagonal iconography.](img/tidyverse-iconography.png)

</center>

Before we move on to actually learning the tools, let's make sure we've got our data loaded up.

```{r reload the berry data in case you started your session over}
library(tidyverse)
beer_data <- read_csv("data/clt-berry-data.csv")
```


## Subsetting data

R's system for **indexing** data frames is clear and sensible for those who are used to programming languages, but it is not necessarily easy to read.  

A common situation in R is wanting to select some rows and some columns of our data--this is called "**subsetting**" our data.  But this is less easy than it might be for the beginner in R.  Happily, the `tidverse` methods are much easier to read (and modeled after syntax from **SQL**, which may be helpful for some users.)  We are going to focus on 

### `select()` for columns

The first thing we often want to do in a data analysis is to pick a subset of columns, which usually represent variables.  If we take a look at our beer data, we see that, for example, we have some columns that contain the answers to survey questions, some columns that are about the test day or panelist, and some that identify the panelist or berry:

```{r use glimpse() to examine the berry data}
glimpse(berry_data)
```

So, for example, we might want to determine whether there are differences in liking between the berries, test days, or scales, in which case perhaps we only want the columns starting with `us_`, `lms_`, and `9pt_` along with `berry` and `test_day`.  We learned previously that we can do this with numeric indexing:

```{r reminder of base R subsetting with column numbers}
berry_data[, c(10,25,28,45,56:64,81,89:91)]
```

However, this is both difficult for novices to `R` and difficult to read if you are not intimately familiar with the data.  It is also rather fragile--what if someone else rearranged the data in your import file?  You're just selecting the last 6 columns, which are not guaranteed to contain the rating data you're interested in. Not to mention all of the counting was annoying!

Subsetting using names is a little more stable, but still not that readable and takes a lot of typing.

```{r reminder of base R subsetting with column names}
berry_data[, c("9pt_aroma","9pt_overall","9pt_taste","9pt_texture","9pt_appearance",
               "lms_aroma","lms_overall","lms_taste","lms_texture","lms_appearance",
               "us_aroma","us_overall","us_taste","us_texture","us_appearance",
               "berry","test_day")]
```


The `select()` function in `tidyverse` (actually from the `dplyr` package) is the smarter, easier way to do this.  It works on data frames, and it can be read as "from \<data frame\>, select the columns that meet the criteria we've set."

`select(<data frame>, <column 1>, <column 2>, ...)`

The simplest way to use `select()` is just to name the columns you want!

```{r using select() with explicit column names}
select(berry_data, berry, test_day,
       lms_aroma, lms_overall, lms_taste, lms_texture, lms_appearance) # note the lack of quoting on the column names
```

This is much clearer to the reader.

Getting rid of the double quotes around `"column names"` we saw before can have some consequences, though. R variable names can *only* contain letters, numbers, and underscores (_), no spaces ( ) or dashes (-), and can't start with numbers, but the tidyverse *will* let you make column names that break these rules. So if you try to do the exact same thing as before to get the data from the 9-point hedonic scale instead of the labeled magnitude scale:

```{r errors from select() with unusual column names}
select(berry_data, berry, test_day,
       9pt_aroma, 9pt_overall, 9pt_taste, 9pt_texture, 9pt_appearance) # this will cause an error
```

You'll run into an error. The solution to this is a different kind of quote called a backtick (`) which is usually next to the number 1 key in the very top left of QWERTY (US and UK) keyboards. On QWERTZ and AZERTY keyboards, it may be next to the backspace key or one of the alternate characters made by the 7 key. Look [at this page](https://superuser.com/questions/254076/how-do-i-type-the-tick-and-backtick-characters-on-windows) for help finding it in common international keyboard layouts.

```{r using select() with escaped column names}
select(berry_data, berry, test_day, #these ruly column names don't need escaping
       `9pt_aroma`, #only ones starting with numbers...
       `9pt_overall`, `9pt_taste`, `9pt_texture`, `9pt_appearance`,
       `Sample Name`) #or containing spaces
```

The backticks are only necessary when a column name breaks one of the variable-naming rules, and RStudio will fill them in for you if you use tab autocompletion when writing your select() and other tidyverse functions.

You can also use `select()` with a number of helper functions, which use logic to select columns that meet whatever conditions you set.  For example, the `starts_with()` helper function lets us give a set of characters we want columns to start with:

```{r using select() with a character search}
select(berry_data, starts_with("lms_")) #the double-quotes are back because this isn't a full column name
```

There are equivalents for the end of column names (`ends_with()`) and text found anywhere in the name (`contains()`). 

You can combine these statements together to get subsets of columns however you want:

```{r combining character search and explicit select()}
select(berry_data, starts_with("lms_"), starts_with("9pt_"), starts_with("us_"),
       berry, test_day)

select(berry_data, starts_with("jar_"), ends_with("_overall"),
       berry, test_day)
```

If you're annoyed at the order that your columns are printing in and how far to the right you have to scroll in your previews, `select()` can also be used to rearrange your columns with a helper function called `everything()`:

```{r everything() is a select helper that selects everything (else)}
select(berry_data, everything()) #This selects everything--it doesn't change at all.
select(berry_data, berry, contains("_overall"),
       everything()) #only type the columns you want to move to the front, and everything() will keep the rest
```

You may sometimes want to select columns where the data meets some criteria, rather than the column name or position. The `where()` helper function allows you to insert this kind of programmatic logic into `select()` statements, which gives you the ability to specify columns using any arbitrary function.

```{r using where() with select() is more advanced}
select(berry_data, where(is.numeric)) #a few functions return one logical value per vector
select(berry_data, where(~!any(is.na(.)))) #otherwise we can use "lambda" functions
```

The little squiggly symbol, called a tilde (~), is above the backtick on QWERTY keyboards, and it turns everything that comes after it into a "lambda function". We'll talk more about lambda functions later, when we talk about `across()`. You can read it the above as "`select()` the columns from `berry_data` `where()` there are not (`!`) `any()` NA values (`is.na(.)`)".

Besides being easier to write conditions for than indexing with `[]`, `select()` is code that is much closer to how you or I think about what we're actually doing, making code that is more human readable.

### `filter()` for rows

So `select()` lets us pick which columns we want.  Can we also use it to pick particular observations?  No.  But for that, there's `filter()`.

We learned that, using `[]` indexing, we'd specify a set of rows we want.  If we want the first 10 rows of `berry_data`, we'd write

```{r reminder of how to get rows from a data frame in base R}
berry_data[1:10, ]
```

Again, this is not very human readable, and if we reorganize our rows this won't be useful anymore.  The `tidyverse` answer to this approach is the `filter()` function, which lets you filter your dataset into specific rows according to *data stored in the table itself*.

```{r first example of filter() with conditional logic}
filter(berry_data, pre_expectation > 3) # let's get survey responses with had a higher-than-average expectation
```

When using `filter()`, we can specify multiple logical conditions.  For example, let's get only raspberries with initially high expectations.  If we want only exact matches, we can use the direct `==` operator:

```{r using the equality operator in filter()}
filter(berry_data, pre_expectation > 3, berry == "raspberry")
```

But this won't return, for example, any berries labeled as "Raspberry", with an uppercase R.

```{r case-sensitivity in filter()}
filter(berry_data, pre_expectation > 3, berry == "Raspberry")
```

Luckily, we don't have a mix of raspberries and Raspberries. Maybe we want both raspberries and strawberries that panelists had high expectations of:

```{r using the OR operator in filter()}
filter(berry_data, pre_expectation > 3, berry == "raspberry" | berry == "strawberry")
```

In `R`, the `|` means Boolean `OR`, and the `&` means Boolean `AND`. If you're trying to figure out which one to use, phrase what you want to do with your `filter()` statement by starting "keep only rows that have...". We may want to look at raspberries *and* strawberries, but we want to "keep only rows that have a `berry` type of raspberry *or* strawberry".

We can combine any number of conditions with `&` and `|` to search within our data table.  But this can be a bit tedious.  The `stringr` package, part of `tidyverse`, gives a lot of utility functions that we can use to reduce the number of options we're individually writing out, similar to `starts_with()` or `contains()` for columns. Maybe we want the results from the first day of each berry type:

```{r using character searches in filter()}
filter(berry_data, str_detect(test_day, "Day 1"))
```

Here, the `str_detect()` function searched for any text that **contains** "Day 1" in the `test_day` column.

## Combining steps with the pipe: `%>%`

It isn't hard to imagine a situation in which we want to **both** `select()` some columns and `filter()` some rows.  There are 3 ways we can do this, one of which is going to be the best for most situations.

Let's imagine we want to get only information about the berries, overall liking, and cata attributes for blackberries

First, we can nest functions:

```{r nesting functions}
select(filter(berry_data, berry == "blackberry"), `Sample Name`, contains("_overall"), contains("cata_"))
```

The problem with this approach is that we have to read it "inside out".  First, `filter()` will happen and get us only berries whose `berry` is exactly "blackberry".  Then `select()` will get `Sample Name` along with columns that match "_overall" or "cata_ in their names.  Especially as your code gets complicated, this can be very hard to read.

So we might take the second approach: creating intermediates.  We might first `filter()`, store that step somewhere, then `select()`:

```{r storing results as intermediates}
blackberries <- filter(berry_data, berry == "blackberry")
select(blackberries, `Sample Name`, contains("_overall"), contains("cata_"))
```

But now we have this intermediate we don't really need cluttering up our `Environment` tab.  This is fine for a single step, but if you have a lot of steps in your analysis this is going to get old (and confusing) fast.  You'll have to remove a lot of these using the `rm()` command to keep your code clean.

<font color="red">**warning**:</font> `rm()` will *permanently* delete whatever objects you run it on from your `Environment`, and you will only be able to restore them by rerunning the code that generated them in the first place.

```{r the rm() function deletes objects from the Environment}
rm(blackberries)
```

The final method, and what is becoming standard in modern `R` coding, is the **pipe**, which is written in `tidyverse` as `%>%`.  This garbage-looking set of symbols is actually your best friend, you just don't know it yet.  I use this tool constantly in my R programming, but I've been avoiding it up to this point because it wasn't a part of base R for the vast majority of its history.


OK, enough background, what the heck _is_ a pipe?  The term "pipe" comes from what it does: like a pipe, `%>%` let's whatever is on it's left side flow through to the right hand side.  It is easiest to read `%>%` as "**AND THEN**". 

```{r the mighty pipe!}
berry_data %>%                             # Start with the berry_data
  filter(berry == "blackberry") %>%        # AND THEN filter to blackberries
  select(`Sample Name`,                    # AND THEN select sample name, overall liking, etc
         contains("_overall"), contains("cata_"))
```

In this example, each place there is a `%>%` I've added a comment saying "AND THEN".  This is because that's exactly what the pipe does: it passes whatever happened in the previous step to the next function.  Specifically, `%>%` passes the **results** of the previous line to the **first argument** of the next line.

### Pipes require that the lefthand side be a single functional command

This means that we can't directly do something like rewrite `sqrt(1 + 2)` with `%>%`:

```{r order of operations with the pipe}
1 + 2 %>% sqrt # this is instead computing 1 + sqrt(2)
```

Instead, if we want to pass binary operationse in a pipe, we need to enclose them in `()` on the line they are in:

```{r parentheses will make the pipe work better}
(1 + 2) %>% sqrt() # Now this computes sqrt(1 + 2) = sqrt(3)
```

More complex piping is possible using the curly braces (`{}`), which create new R environments, but this is more advanced than you will generally need to be.

### Pipes always pass the result of the lefthand side to the *first* argument of the righthand side

This sounds like a weird logic puzzle, but it's not, as we can see if we look at some simple math.  Let's define a function for use in a pipe that computes the difference between two numbers:

```{r an example of a custom function}
subtract <- function(a, b) a - b
subtract(5, 4)
```

If we want to rewrite that as a pipe, we can write:

```{r argument order still matters with piped functions}
5 %>% subtract(4)
```

But we can't write 

```{r the pipe will always send the previous step to the first argument of the next step}
4 %>% subtract(5) # this is actually making subtract(4, 5)
```

We can explicitly force the pipe to work the way we want it to by using `.` **as the placeholder for the result of the lefthand side**:

```{r using the "." pronoun lets you control order in pipes}
4 %>% subtract(5, .) # now this properly computes subtract(5, 4)
```

So, when you're using pipes, make sure that the output of the lefthand side *should* be going into the first argument of the righthand side--this is often but not always the case, especially with non-`tidyverse` functions.

### Pipes are a pain to type

Typing `%>%` is no fun.  But, happily, RStudio builds in a shortcut for you: macOS is `cmd + shift + M`, Windows is `ctrl + shift + M`.

## Make new columns: `mutate()`

You hopefully are starting to be excited by the relative ease of doing some things in R with `tidyverse` that are otherwise a little bit abstruse.  Here's where I think things get really, really cool.  The `mutate()` function *creates a new column in the existing dataset*. 

We can do this easily in base R by setting a new name for a column and using the assign (`<-`) operator, but this is clumsy. Often, we want to create a new column temporarily, or to combine several existing columns.  We can do this using the `mutate()` function.

Let's say that we want to create a quick categorical variable that tells us whether a berry was rated higher after the actual tasting event than the pre-tasting expectation.

We know that we can use `filter()` to get just the berries with `post_expectation > pre_expectation`:


```{r using pipes with filter()}
berry_data %>%
  filter(post_expectation > pre_expectation)
```

But what if we want to be able to just see this?

```{r first example of mutate()}
berry_data %>%
  mutate(improved = post_expectation > pre_expectation) %>%
  # We'll select just a few columns to help us see the result
  select(`Participant Name`, `Sample Name`, pre_expectation, post_expectation, improved)
```

What does the above function do?

`mutate()` is a very easy way to edit your data mid-pipe.  So we might want to do some calculations, create a temporary variable using `mutate()`, and then continue our pipe.  **Unless we use `<-` to store our `mutate()`d data, the results will be only temporary.**

We can use the same kind of functional logic we've been using in other `tidyverse` commands in `mutate()` to get real, powerful results.  For example, we might want to know if a beer is rated better than the `mean()` of the ratings.  We can do this easily using `mutate()`:

```{r mutate() makes new columns}
# Let's find out the average (mean) rating for the berries on the 9-point scale
berry_data$`9pt_overall` %>% #we need the backticks for $ subsetting just like for tidy-selecting 
  mean(na.rm = TRUE)

# Now, let's create a column that tells us if a beer is rated > average
berry_data %>%
  mutate(better_than_average = `9pt_overall` > mean(`9pt_overall`, na.rm = TRUE)) %>%
  # Again, let's select just a few columns
  select(`Sample Name`, `9pt_overall`, better_than_average)
```

## Split-apply-combine analyses with `group_by()` and `summarize()`

Many basic data analyses can be described as *split-apply-combine*: *split* the data into groups, *apply* some analysis into groups, and then *combine* the results.

For example, in our `berry_data` we might want to split the data by each berry sample, calculate the average overall rating and standard deviation of the rating for each, and the generate a summary table telling us these results.  Using the `filter()` and `select()` commands we've learned so far, you could probably cobble together this analysis without further tools.

However, `tidyverse` provides two powerful tools to do this kind of analysis:

1.  The `group_by()` function takes a data table and groups it by **categorical** values of any column (generally don't try to use `group_by()` on a numeric variable)
2.  The `summarize()` function is like `mutate()` for groups created with `group_by()`: 
  1.  First, you specify 1 or more new columns you want to calculate for each group
  2.  Second, the function produces 1 value for each group for each new column
  
To accomplish the example above, we'd do the following:

```{r split-apply-combine pipeline example}
berry_summary <- 
  berry_data %>%
  filter(!is.na(`9pt_overall`)) %>%
  group_by(`Sample Name`) %>%                          # we will create a group for each berry sample
  summarize(n_responses = n(),                         # n() counts number of ratings for each berry
            mean_rating = mean(`9pt_overall`),         # the mean rating for each species
            sd_rating = sd(`9pt_overall`),             # the standard deviation in rating
            se_rating = sd(`9pt_overall`) / sqrt(n())) # multiple functions in 1 row

#Equivalent to:
berry_data %>%
  filter(!is.na(`9pt_overall`)) %>%
  summarize(n_responses = n(),                         # n() counts number of ratings for each berry
            mean_rating = mean(`9pt_overall`),         # the mean rating for each species
            sd_rating = sd(`9pt_overall`),             # the standard deviation in rating
            se_rating = sd(`9pt_overall`) / sqrt(n()),
            .by = `Sample Name`)

berry_summary
```

We can use this approach to even get a summary stats table - for example, confidence limits according to the normal distribution:

```{r simple stat summaries with split-apply-combine}
berry_summary %>%
  mutate(lower_limit = mean_rating - 1.96 * se_rating,
         upper_limit = mean_rating + 1.96 * se_rating)
```

Note that in the above example we use `mutate()`, *not* `summarize()`, because we had saved our summarized data.  We could also have calculated `lower_limit` and `upper_limit` directly as part of the `summarize()` statement if we hadn't saved the intermediate.

## Utilities for data management

Honestly, the amount of power in `tidyverse` is way more than we can cover today, and is covered more comprehensively (obviously) by [Wickham and Grolemund](https://r4ds.had.co.nz/).  However, I want to name 4 more utilities we will make a lot of use of today (and you will want to know about for your own work).  

### Rename your columns

Often you will import data with bad column names or you'll realize you need to rename variables during your workflow. This is one way to get around having to type a bunch of backticks forever. For this, you can use the `rename()` function:

```{r renaming columns}
names(berry_data)

berry_data %>%
  rename(Sample = `Sample Name`,
         Subject = `Participant Name`) %>%
  select(Subject, Sample, everything()) #no more backticks!
```

You can also rename by position, but be sure you have the right order and don't change the input data later:

```{r rename() works with positions as well as explicit names}
berry_data %>%
  rename(Subject = 1)
```

### Relocate your columns

If you `mutate()` columns or just have a big data set with a lot of variables, often you want to move columns around.  This is a pain to do with `[]`, but again `tidyverse` has a utility to move things around easily: `relocate()`.

```{r reordering columns in a tibble}
berry_data %>%
  relocate(`Sample Name`) # giving no other arguments will move to front
```

You can also use `relocate()` to specify positions

```{r using relative positions with relocate()}
berry_data %>%
  relocate(Gender, Age, `Subject Code`, `Start Time (UTC)`, `End Time (UTC)`, `Sample Identifier`, .after = berry) # move repetitive and empty columns to the end
```

### Remove missing values

Missing values (the `NA`s you've been seeing so much) can be a huge pain, because they make more of themselves.

```{r missing values make more of themselves}
mean(berry_data$price) #This column had no NAs, so we can take the average
mean(berry_data$`9pt_overall`) #This column has some NAs, so we get NA
```
Many base R functions that take a vector and return some mathematical function (e.g., `mean()`, `sum()`, `sd()`) have an argument called `na.rm` that can be set to just act as if the values aren't there at all.

```{r na.rm in base R functions}
mean(berry_data$`9pt_overall`, na.rm = TRUE) #We get the average of only the valid numbers
sum(berry_data$`9pt_overall`, na.rm = TRUE) /
  length(berry_data$`9pt_overall`) #The denominator is NOT the same as the total number of values anymore
sum(berry_data$`9pt_overall`, na.rm = TRUE) /
  sum(!is.na(berry_data$`9pt_overall`)) #The denominator is the number of non-NA values
```
However, this isn't always convenient. Sometimes it may be easier to simply get rid of all observations with any missing values, which tidyverse has a handy `drop_na()` function for:

```{r removing rows with na values}
berry_data %>%
  drop_na() #All of our rows have *some* NA values, so this returns nothing

berry_data %>%
  select(`Participant Name`, `Sample Name`, contains("9pt_")) %>%
  drop_na() #Now we get only respondants who answered all 9-point liking questions.
```

Or you may want to remove any columns/variables that have some missing data, which is one of the most common uses of `where()`:

```{r removing columns with missing values}
berry_data %>%
  select(where(~!any(is.na(.)))) #Only 38 columns with absolutely no missing values.
#This loses all of the liking data.
```

Both of the above methods guarantee that you will have an output with absolutely no missing data, but may be over-zealous if, say, everyone answered overall liking on one of the three scales and we want to do some work to combine those later. `filter()` and `select()` can be combined to do infinitely complex missing value removal.

```{r removing rows that don't have at least one non-missing aroma liking rating}
berry_data %>%
  select(where(~!all(is.na(.)))) %>% #remove columns with no data
  filter(!(is.na(`9pt_aroma`) & is.na(lms_aroma) & is.na(us_aroma)))
#You'll notice that this is all strawberries, actually
```

### Sort your data

More frequently, we will want to rearrange our rows, which can be done with `arrange()`.  All you have to do is give `arrange()` one or more columns to sort the data by.  You can use either the `desc()` or the `-` shortcut to sort in reverse order. Whether ascending or descending, `arrange()` places missing values at the bottom.

```{r arrange() lets you sort your data}
berry_data %>%
  arrange(desc(lms_overall)) %>% # which berries had the highest liking on the lms?
  select(`Sample Name`, `Participant Name`, lms_overall)
```

You can sort alphabetically as well:

```{r arrange() works on both numeric and character data}
tibble(state_name = state.name, area = state.area) %>% # using a dataset of US States for demonstration
  arrange(desc(state_name))                            # sort states reverse-alphabetically
```

### Pivot tables

Users of Excel may be familiar with the idea of pivot tables.  These are functions that let us make our data tidier.  To quote Wickham and Grolemund:

> here are three interrelated rules which make a dataset tidy:
>
> 1.  Each variable must have its own column.
> 2.  Each observation must have its own row.
> 3.  Each value must have its own cell.

While these authors present "tidiness" of data as an objective property, I'd argue that data is always tidy **for a specific purpose**.  For example, our data is relatively tidy with one row per tasting event (one person tasting one berry), but this data still has an unruly number of variables (92 columns!!). You've already learned some tricks for dealing with large numbers of columns at once like `across()` and other functions using select helpers, but we have to do this *every time* we use `mutate()`, `summarize()`, or a similar function.

We could also treat the attribute or question as an independent variable affecting the response. If we take this view, then the tidiest dataset actually has one row for each person's response to a single `question`. If we want to make plots or do other modelling, this **longer** form is often more tractable and lets us do operations on the whole dataset with less code.

We can use the `pivot_longer()` function to change our data to make the implicit variable explicit and to make our data tidier.

```{r pivoting data tables}
berry_data %>%
  select(`Subject Code`, `Sample Name`, berry, starts_with("cata_"), starts_with("9pt")) %>% # for clarity
  pivot_longer(cols = starts_with("cata_"),
               names_prefix = "cata_",
               names_to = "attribute",
               values_to = "presence") -> berry_data_cata_long

berry_data_cata_long
```

Remember that `tibble`s and `data.frame`s can only have one data type per column (`logical > integer > numeric > character`), however! If we have one row for each CATA, JAR, hedonic scale, AND free response question, the `value` column would have a mixture of different data types. This is why we have to tell `pivot_longer()` which `cols` to pull the `names` and `values` from.

Now for each unique combination of `Sample Name` and `Subject Code`, we have 36 rows, one for each CATA question that was asked. The variables that weren't listed in the `cols` argument are just replicated on each of these rows. Each of the 36 rows that represent `Subject Code` 1001's CATA responses for `raspberry 6` has the same `Subject Code`, `Sample Name`, `berry`, and various `9pt_` ratings as the other 35.

Sometimes we want to have "wider" or "untidy" data.  We can use `pivot_wider()` to reverse the effects of `pivot_longer()`.

```{r reversing the effects of pivot_longer() with pivot_wider()}
berry_data_cata_long %>%
  pivot_wider(names_from = "attribute",
              values_from = "presence",
              names_prefix = "cata_")
```

Pivoting is an incredibly powerful and incredibly common data manipulation technique that will become even more powerful when we need to make complex graphs later. Different functions and analyses may require the data in different longer or wider formats, and you will often find yourself starting with even less tidy data than what we've provided.

For an example of this power, let's imagine that we want to compare the 3 different liking scales by normalizing each by the `mean()` and `sd()` of that particular scale, then comparing average liking for each attribute of each berry across the three scales.

```{r}
berry_data %>%
  pivot_longer(cols = starts_with(c("9pt_","lms_","us_")),
               names_to = c("scale", "attribute"),
               names_sep = "_",
               values_to = "rating",
               values_drop_na = TRUE) %>%
  group_by(scale) %>%
  mutate(normalized_rating = (rating - mean(rating)) / sd(rating)) %>%
  group_by(scale, attribute, berry) %>%
  summarize(avg_liking = mean(normalized_rating)) %>%
  pivot_wider(names_from = scale,
              values_from = avg_liking)
```

While pivoting may seem simple at first, it can also get pretty confusing! That example required two different pivots! We'll be using these tools throughout the rest of the tutorial, so I wanted to give exposure, but mastering them takes trial and error. I recommend taking a look at the [relevant chapter in Wickham and Grolemund](https://r4ds.had.co.nz/tidy-data.html) for details.

It's not a bad idea to restart your R session here.  Make sure to save your work, but a clean `Environment` is great when we're shifting topics.

You can accomplish this by going to `Session > Restart R` in the menu.

Then, we want to make sure to re-load our packages and import our data.

```{r making sure that we have loaded all packages and data}
# The packages we're using
library(tidyverse)
library(ca)

# The dataset
berry_data <- read_csv("data/clt-berry-data.csv")
```
