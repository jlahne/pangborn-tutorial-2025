# Wrangling data with `tidyverse`

```{r setup, include = FALSE}
library(tidyverse)
library(tidytext)
beer_data <- read_csv("data/ba_2002.csv")
```

A common saying in data science is that about 90% of the effort in an analysis workflow is in getting data wrangled into the right format and shape, and 10% is actual analysis.  In a point and click program like SPSS or XLSTAT we don't think about this as much because the activity of reshaping the data--making it longer or wider as required, finding and cleaning missing values, selecting columns or rows, etc--is often temporally and programmatically separated from the "actual" analysis. 

In `R`, this can feel a bit different because we are using the same interface to manipulate our data and to analyze it.  Sometimes we'll want to jump back out to a spreadsheet program like Excel or even the command line (the "shell" like `bash` or `zsh`) to make some changes.  But in general the tools for manipulating data in `R` are both more powerful and more easily used than doing these activities by hand, and you will make yourself a much more effective analyst by mastering these basic tools.

Here, we are going to emphasize the set of tools from the [`tidyverse`](https://www.tidyverse.org/), which are extensively documented in Hadley Wickham's and Garrett Grolemund's book [*R for Data Science*](https://r4ds.had.co.nz/).  If you want to learn more, start there!

<center>

![The `tidyverse` is associated with this hexagonal iconography.](img/tidyverse-iconography.png)

</center>

Before we move on to actually learning the tools, let's make sure we've got our data loaded up.

```{r reload the beer data in case you started your session over}
library(tidyverse)
beer_data <- read_csv("data/ba_2002.csv")
```


## Subsetting data

R's system for **indexing** data frames is clear and sensible for those who are used to programming languages, but it is not necessarily easy to read.  

A common situation in R is wanting to select some rows and some columns of our data--this is called "**subsetting**" our data.  But this is less easy than it might be for the beginner in R.  Happily, the `tidverse` methods are much easier to read (and modeled after syntax from **SQL**, which may be helpful for some users.)  We are going to focus on 

### `select()` for columns

The first thing we often want to do in a data analysis is to pick a subset of columns, which usually represent variables.  If we take a look at our beer data, we see that, for example, we have some columns that are data about our beers, and some columns that are user-generated responses:

```{r use glimpse() to examine the beer data}
glimpse(beer_data)
```

So, for example, we might want to try to reverse engineer the way that the `rating` is generated from the sub-modalities, in which case perhaps we only want the last 6 columns.  We learned previously that we can do this with numeric indexing:

```{r reminder of base R subsetting}
beer_data[, 9:14]
```

However, this is both difficult for novices to `R` and difficult to read if you are not intimately familiar with the data.  It is also rather fragile--what if someone else rearranged the data in your import file?  You're just selecting the last 6 columns, which are not guaranteed to contain the rating data you're interested in.


The `select()` function in `tidyverse` (actually from the `dplyr` package) is the smarter, easier way to do this.  It works on data frames, and it can be read as "from \<data frame\>, select the columns that meet the criteria we've set." 

`select(<data frame>, <column 1>, <column 2>, ...)`

The simplest way to use `select()` is just to name the columns you want!

```{r using select() with explicit column names}
select(beer_data, appearance, aroma, palate, taste, overall, rating) # note the lack of quoting on the column names
```

This is much clearer to the reader.

You can also use `select()` with a number of helper functions, which use logic to select columns that meet whatever conditions you set.  For example, the `starts_with()` helper function lets us give a set of characters we want columns to start with:

```{r using select() with a character search}
select(beer_data, starts_with("beer"))
```

There are equivalents for the end of column names (`ends_with()`) and text found anywhere in the name (`contains()`). 

You can combine these statements together to get subsets of columns however you want:

```{r combining character search and explicit select()}
select(beer_data, starts_with("beer"), rating, abv)
```

You can also use programmatic logic in `select()` by using the `where()` helper function, which gives you the ability to specify columns by any arbitrary function.

```{r using where() with select() is more advanced}
select(beer_data, where(~is.numeric(.)))
```

Besides being easier to write conditions for than indexing with `[]`, `select()` is code that is much closer to how you or I think about what we're actually doing, making code that is more human readable.

### `filter()` for rows

So `select()` lets us pick which columns we want.  Can we also use it to pick particular observations?  No.  But for that, there's `filter()`.

We learned that, using `[]` indexing, we'd specify a set of rows we want.  If we want the first 10 rows of `beer_data`, we'd write

```{r reminder of how to get rows from a data frame in base R}
beer_data[1:10, ]
```

Again, this is not very human readable, and if we reorganize our rows this won't be useful anymore.  The `tidyverse` answer to this approach is the `filter()` function, which lets you filter your dataset into specific rows according to *data stored in the table itself*.

```{r first example of filter() with conditional logic}
filter(beer_data, abv > 10) # let's get some heavy beers
```

When using `filter()`, we can specify multiple logical conditions.  For example, let's get only Barleywines that are more than 10% ABV.  If we wanted only exact matches, we could use the direct `==` operator:

```{r using the equality operator in filter()}
filter(beer_data, abv > 10, style == "American Barleywine")
```

But this won't return, for example, any beer labeled as an "English Barleywine".  

```{r using the OR operator in filter()}
filter(beer_data, abv > 10, style == "American Barleywine" | style == "English Barleywine")
```

In `R`, the `|` means Boolean `OR`, and the `&` means Boolean `AND`.  We can use these to combine conditions for searching our data table.  But this can be a bit tedious.  The `stringr` package, part of `tidyverse`, gives a lot of utility functions that we can use instead of this effortful searching.

```{r using character searches in filter()}
filter(beer_data, abv > 10, str_detect(style, "Barleywine"))
```

Here, the `str_detect()` function searched for any text that **contains** "Barleywine" in the `style` column.

## Combining steps with the pipe: `%>%`

It isn't hard to imagine a situation in which we want to **both** select some columns and filter some rows.  There are 3 ways we can do this, one of which is going to be the best for most situations.

Let's imagine we want to get only information about the beers, abv, and rating for stouts

First, we can nest functions:

```{r nesting functions}
select(filter(beer_data, str_detect(style, "Stout")), contains("beer"), abv, rating)
```

The problem with this approach is that we have to read it "inside out".  First, `filter()` will happen and get us only beers that match "Stout" in their `style`.  Then `select()` will get columns that match "beer" in their names, `abv`, and `rating`.  Especially as your code gets complicated, this can be very hard to read.

So we might take the second approach: creating intermediates.  We might first `filter()`, store that step somewhere, then `select()`:

```{r storing results as intermediates}
beer_stouts <- filter(beer_data, str_detect(style, "Stout"))
select(beer_stouts, contains("beer"), abv, rating)
```

But now we have this intermediate we don't really need cluttering up our `Environment` tab.  This is fine for a single step, but if you have a lot of steps in your analysis this is going to get old (and confusing) fast.  You'll have to remove a lot of these using the `rm()` command to keep your code clean.

<font color="red">**warning**:</font> `rm()` will *permanently* delete whatever objects you run it on from your `Environment`, and you will only be able to restore them by rerunning the code that generated them in the first place.

```{r the rm() function deletes objects from the Environment}
rm(beer_stouts)
```

The final method, and what is becoming standard in modern `R` coding, is the **pipe**, which is written in `tidyverse` as `%>%`.  This garbage-looking set of symbols is actually your best friend, you just don't know it yet.  I use this tool constantly in my R programming, but I've been avoiding it up to this point because it's not part of base R (in fact that's no longer strictly true, but it is kind of complicated at the moment).  


OK, enough background, what the heck _is_ a pipe?  The term "pipe" comes from what it does: like a pipe, `%>%` let's whatever is on it's left side flow through to the right hand side.  It is easiest to read `%>%` as "**AND THEN**".  

```{r the mighty pipe!}
beer_data %>%                              # Start with the beer_data
  filter(str_detect(style, "Stout")) %>%   # AND THEN filter to stouts
  select(contains("beer"), abv, rating)    # AND THEN select the beer columns, etc
```

In this example, each place there is a `%>%` I've added a comment saying "AND THEN".  This is because that's exactly what the pipe does: it passes whatever happened in the previous step to the next function.  Specifically, `%>%` passes the **results** of the previous line to the **first argument** of the next line.

### Pipes require that the lefthand side be a single functional command

This means that we can't directly do something like rewrite `sqrt(1 + 2)` with `%>%`:

```{r order of operations with the pipe}
1 + 2 %>% sqrt # this is instead computing 1 + sqrt(2)
```

Instead, if we want to pass binary operationse in a pipe, we need to enclose them in `()` on the line they are in:

```{r parentheses will make the pipe work better}
(1 + 2) %>% sqrt() # Now this computes sqrt(1 + 2) = sqrt(3)
```

More complex piping is possible using the curly braces (`{}`), which create new R environments, but this is more advanced than you will generally need to be.

### Pipes always pass the result of the lefthand side to the *first* argument of the righthand side

This sounds like a weird logic puzzle, but it's not, as we can see if we look at some simple math.  Let's define a function for use in a pipe that computes the difference between two numbers:

```{r an example of a custom function}
subtract <- function(a, b) a - b
subtract(5, 4)
```

If we want to rewrite that as a pipe, we can write:

```{r argument order still matters with piped functions}
5 %>% subtract(4)
```

But we can't write 

```{r the pipe will always send the previous step to the first argument of the next step}
4 %>% subtract(5) # this is actually making subtract(4, 5)
```

We can explicitly force the pipe to work the way we want it to by using `.` **as the placeholder for the result of the lefthand side**:

```{r using the "." pronoun lets you control order in pipes}
4 %>% subtract(5, .) # now this properly computes subtract(5, 4)
```

So, when you're using pipes, make sure that the output of the lefthand side *should* be going into the first argument of the righthand side--this is often but not always the case, especially with non-`tidyverse` functions.

### Pipes are a pain to type

Typing `%>%` is no fun.  But, happily, RStudio builds in a shortcut for you: macOS is `cmd + shift + M`, Windows is `ctrl + shift + M`.

## Make new columns: `mutate()`

You hopefully are starting to be excited by the relative ease of doing some things in R with `tidyverse` that are otherwise a little bit abstruse.  Here's where I think things get really, really cool.  The `mutate()` function *creates a new column in the existing dataset*.  

We can do this easily in base R by setting a new name for a column and using the assign (`<-`) operator, but this is clumsy. Often, we want to create a new column temporarily, or to combine several existing columns.  We can do this using the `mutate()` function.

Let's say that we want to create a quick categorical variable that tells us whether a beer was rated as more than the central value (2.5) in the 5-pt rating scale.  This is kind of like doing a median split, which we'll get to in a moment.

We know that we can use `filter()` to get just the beers with `rating > 2.5`:


```{r using pipes with filter()}
beer_data %>%
  filter(rating > 2.5)
```

But what if we want to be able to just see this?

```{r first example of mutate()}
beer_data %>%
  mutate(better_than_2.5 = rating > 2.5) %>%
  # We'll select just a few columns to help us see the result
  select(beer_name, rating, better_than_2.5) 
```

What does the above function do?

`mutate()` is a very easy way to edit your data mid-pipe.  So we might want to do some calculations, create a temporary variable using `mutate()`, and then continue our pipe.  **Unless we use `<-` to store our `mutate()`d data, the results will be only temporary.**

We can use the same kind of functional logic we've been using in other `tidyverse` commands in `mutate()` to get real, powerful results.  For example, we might want to know if a beer is rated better than the `mean()` of the ratings.  We can do this easily using `mutate()`:

```{r mutate() makes new columns}
# Let's find out the average (mean) rating for these beers
beer_data$rating %>% 
  mean() 

# Now, let's create a column that tells us if a beer is rated > average
beer_data %>%
  mutate(better_than_average = rating > mean(rating)) %>%
  # Again, let's select just a few columns
  select(beer_name, rating, better_than_average)
```

## Split-apply-combine analyses with `group_by()` and `summarize()`

Many basic data analyses can be described as *split-apply-combine*: *split* the data into groups, *apply* some analysis into groups, and then *combine* the results.  

For example, in our `beer_data` we might want to split the data into beer styles, calculate the average overall rating and standard deviation of the rating for each beer style, and the generate a summary table telling us these results.  Using the `filter()` and `select()` commands we've learned so far, you could probably cobble together this analysis without further tools.

However, `tidyverse` provides two powerful tools to do this kind of analysis:

1.  The `group_by()` function takes a data table and groups it by **categorical** values of any column (generally don't try to use `group_by()` on a numeric variable)
2.  The `summarize()` function is like `mutate()` for groups created with `group_by()`: 
  1.  First, you specify 1 or more new columns you want to calculate for each group
  2.  Second, the function produces 1 value for each group for each new column
  
To accomplish the example above, we'd do the following:

```{r split-apply-combine pipeline example}
beer_summary <- 
  beer_data %>%
  group_by(style) %>%                           # we will create a group for each unique style
  summarize(n_beers = n(),                      # n() counts rows in each style
            mean_rating = mean(rating),         # the mean rating for each style
            sd_rating = sd(rating),             # the standard deviation in rating
            se_rating = sd(rating) / sqrt(n())) # multiple functions in 1 row

beer_summary
```

We can use this approach to even get a summary stats table - for example, confidence limits according to the normal distribution:

```{r simple stat summaries with split-apply-combine}
beer_summary %>%
  mutate(lower_limit = mean_rating - 1.96 * se_rating,
         upper_limit = mean_rating + 1.96 * se_rating)
```

Note that in the above example we use `mutate()`, *not* `summarize()`, because we had saved our summarized data.  We could also have calculated `lower_limit` and `upper_limit` directly as part of the `summarize()` statement if we hadn't saved the intermediate.

## Utilities for data management

Honestly, the amount of power in `tidyverse` is way more than we can cover today, and is covered more comprehensively (obviously) by [Wickham and Grolemund](https://r4ds.had.co.nz/).  However, I want to name 4 more utilities we will make a lot of use of today (and you will want to know about for your own work).  

### Rename your columns

Often you will import data with bad column names or you'll realize you need to rename variables during your workflow.  For this, you can use the `rename()` function:

```{r renaming columns}
names(beer_data)

beer_data %>%
  rename(review_text = reviews)
```

You can also rename by position, which is helpful for quick changes:

```{r rename() works with positions as well as explicit names}
beer_data %>%
  rename(review_text = 1)
```

### Relocate your columns

If you `mutate()` columns or just have a big data set with a lot of variables, often you want to move columns around.  This is a pain to do with `[]`, but again `tidyverse` has a utility to move things around easily: `relocate()`.

```{r reordering columns in a tibble}
beer_data %>%
  relocate(beer_name) # giving no other arguments will move to front
```

You can also use `relocate()` to specify positions

```{r using relative positions with relocate()}
beer_data %>%
  relocate(reviews, .after = rating) # move the long text to the end
```

### Sort your data

More frequently, we will want to rearrange our rows, which can be done with `arrange()`.  All you have to do is give `arrange()` one or more columns to sort the data by.  You can use either the `desc()` or the `-` shortcut to sort in reverse order.

```{r arrange() lets you sort your data}
beer_data %>%
  arrange(desc(rating)) %>% # what are the highest rated beers?
  select(beer_name, rating)
```

You can sort alphabetically as well:

```{r arrange() works on both numeric and character data}
beer_data %>%
  arrange(brewery_name, beer_name) %>% # get beers sorted within breweries
  select(brewery_name, beer_name) %>%  # show only relevant columns
  distinct()                           # discard duplicate rows
```

### Pivot tables

Users of Excel may be familiar with the idea of pivot tables.  These are functions that let us make our data tidier.  To quote Wickham and Grolemund:

> here are three interrelated rules which make a dataset tidy:
>
> 1.  Each variable must have its own column.
> 2.  Each observation must have its own row.
> 3.  Each value must have its own cell.

While these authors present "tidiness" of data as an objective property, I'd argue that data is always tidy **for a specific purpose**.  For example, our data is relatively tidy here, except our numerical ratings: we have 6 different ratings for each beer, which means we have encoded an **implicit variable** in the column names: `rating type`.  If we want to use our data for summarization, the form we have is fine.  But if we want to make plots and do some other modeling, this form may be no good to us.

We can use the `pivot_longer()` function to change our data to make the implicit variable explicit and to make our data tidier.

```{r pivoting data tables}
beer_data %>%
  select(beer_name, user_id, appearance:rating) %>% # for clarity
  pivot_longer(cols = appearance:rating,
               names_to = "rating_type",
               values_to = "rating")
```

Now for each unique combination of `beer_name` and `user_id`, we have 6 rows, one for each type of rating they can generate.

Sometimes we want to have "wider" or "untidy" data.  We can use `pivot_wider()` to reverse the effects of `pivot_longer()`.

While the ideas of pivoting can seem simple, they are both powerful and subtly confusing.  We'll be using these tools throughout the rest of the tutorial, so I wanted to give exposure, but mastering them takes trial and error.  I recommend taking a look at the [relevant chapter in Wickham and Grolemund](https://r4ds.had.co.nz/tidy-data.html) for details.

It's not a bad idea to restart your R session here.  Make sure to save your work, but a clean `Environment` is great when we're shifting topics.

You can accomplish this by going to `Session > Restart R` in the menu.

Then, we want to make sure to re-load our packages and import our data.

```{r making sure that we have loaded all packages and data}
# The packages we're using
library(tidyverse)
library(tidytext)

# The dataset
beer_data <- read_csv("data/ba_2002.csv")
```

## Text Analysis Pt. I: Getting your text data

There are many sources of text data, which is one of the strengths of this approach.  Some quick and obvious examples include:

1.  Open-comment responses on a survey (sensory study in lab, remote survey, etc)
2.  Comment cards or customer responses
3.  Qualitative data (transcripts from interviews and focus groups)
4.  Compiled/database from previous studies
5.  Scraped data from websites and/or social media

The last (#5) is often the most interesting data source, and there are a number of papers in recent years that have discussed the use of this sort of data.  I am not going to discuss how to obtain text data in this presentation, because it is its own, rich topic.  However, you can [review my presentation and tutorial from Eurosense 2020](https://github.com/jlahne/text-mining-eurosense-2020) if you want a quick crash course on web scraping for sensory evaluation.

### Basic import and cleaning with text data

So far we've focused our attention on the `beer_data` data set on the various (`numeric`) rating variables.  But the data in the `reviews` column is clearly more rich.

```{r getting a viewable example}

set.seed(2)

reviews_example <- 
  beer_data %>%
  slice_sample(n = 5)

reviews_example %>%
  select(beer_name, reviews) %>%
  gt::gt()
```

<p>

However, this has some classic features of scraped text data--notice the "\&quot", for example--this is `HTML` placeholder for a quotation mark.  Other common problems that you will see with text data include "parsing errors" between different forms of character data--the most common is between `ASCII` and `UTF-8` standards.  You will of course also encounter errors with typos, and perhaps idiosyncratic formatting characteristic of different text sources: for example, the use of `@usernames` and `#hashtags` in Twitter data.

We will use the `textclean` package to do some basic clean up, but this kind of deep data cleaning may take more steps and will change from project to project.

```{r cleaning our example data}
library(textclean)

cleaned_reviews_example <- 
  reviews_example %>%
  mutate(cleaned_review = reviews %>%
           # Fix some of the HTML placeholders
           replace_html(symbol = FALSE) %>%
           # Replace sequences of "..." 
           replace_incomplete(replacement = " ") %>%
           # Replace less-common or misformatted HTML
           str_replace_all("#.{1,4};", " ") %>%
           # Replace some common non-A-Z, 1-9 symbols
           replace_symbol() %>% 
           # Remove non-word text like ":)"
           replace_emoticon() %>%
           # Remove numbers from the reviews, as they are not useful
           str_remove_all("[0-9]"))

cleaned_reviews_example %>%
  select(beer_name, cleaned_review) %>%
  gt::gt()
```

<p>

This will take slightly longer to run on the full data set, but should improve our overall data quality.

```{r cleaning the full data set}
beer_data <- 
  beer_data %>%
  mutate(cleaned_review = reviews %>%
           replace_html(symbol = FALSE) %>%
           replace_incomplete(replacement = " ") %>%
           str_replace_all("#.{1,4};", " ") %>%
           replace_symbol() %>%
           replace_emoticon() %>%
           str_remove_all("[0-9]"))
```

Again, this is not an exhaustive cleaning step--rather, these are some basic steps that I took based on common parsing errors I observed.  Each particular text dataset will come with its own challenges and require a bit of time to develop the cleaning step.

### Where is the data in `character`?  A *tidy* approach.

As humans who understand English, we can see that meaning is easily found from these reviews.  For example, we can guess that the rating of `r cleaned_reviews_example$beer_name[2]` will be a low number (negative), while the rating of `r cleaned_reviews_example$beer_name[3]` will be much more positive.  And, indeed, this is the case:

```{r the numeric part of our data}
cleaned_reviews_example %>% 
  select(beer_name, appearance:rating)
```

But what part of the structure of the `reviews` text actually tells us this?  This is a complicated topic that is well beyond the scope of this workshop--we are going to propose a single approach based on **tokenization** that is effective, but is certainly not exhaustive.  

If you are interested in a broader view of approaches to text analysis, I recommend the seminal textbook from [Jurafsky and Martin, *Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/), especially chapters 2-6.  The draft version is freely available on the web as of the date of this workshop.  

In the first half of this workshop we reviewed the `tidyverse` approach to data, in which we emphasized an approach to data in which:

> *  Each variable is a column
> *  Each observation is a row
> *  Each type of observational unit is a table

This type of approach can be applied to text data **if we can specify a way to standardize the "observations" within the text**.  We will be applying and demonstrating the approach defined and promoted by [Silge and Robinson in *Text Mining with R*](https://www.tidytextmining.com/index.html), in which we will focus on the following syllogism to create tidy text data:

<center>
`observation == token`
</center>

A **token** is a *meaningful* unit of text, usually but not always a single word, which will be the observation for us.  In our example data, let's identify some possible tokens:

```{r what does text data look like?}
cleaned_reviews_example$cleaned_review[1]
```

We will start with only examining single words, also called "**unigrams**" in the linguistics jargon.  Thus, `Medium`, `straw`, and `cloudy` are the first tokens in this dataset.  We will mostly ignore punctuation and spacing, which means we are giving up some meaning for convenience.

We could figure out how to manually break this up into tokens with enough work, using functions like `str_separate()`, but happily there are a large set of competing `R` tools for tokenization, for example:

```{r a viewable tokenization example}
cleaned_reviews_example$cleaned_review[1] %>%
  tokenizers::tokenize_words(simplify = TRUE)
```

Note that this function also (by default) turns every word into its lowercase version (e.g., `Medium` becomes `medium`) and strips out punctuation.  This is because, to a program like `R`, upper- and lowercase versions of the same string are *not* equivalent.  If we have reason to believe preserving this difference is important, we might want to rethink allowing this behavior.

Now, we have 46 observed tokens for our first review in our example dataset.  But of course this is not super interesting--we need to apply this kind of transformation to every single one of our ~20,000 reviews.  With some fiddling around with `mutate()` we might be able to come up with a solution, but happily this is where we start using the `tidytext` package.  The `unnest_tokens()` function built into that package will allow us to transform our text directly into the format we want, with very little effort.

```{r tokenizing our entire data set into words}
beer_data_tokenized <-
  beer_data %>%
  # We may want to keep track of a unique ID for each review
  mutate(review_id = row_number()) %>%
  unnest_tokens(output = token,
                input = cleaned_review,
                token = "words") %>%
  # Here we do a bit of extra cleaning
  mutate(token = str_remove_all(token, "\\.|_"))

beer_data_tokenized %>%
  # We show just a few of the columns for printing's sake
  select(beer_name, rating, token)
```

The `unnest_tokens()` function actually applies the `tokenizers::tokenize_words()` function in an efficient, easy way: it takes a column of raw `character` data and then tokenizes it as specified, outputting a tidy format of 1-token-per-row.  Now we have our observations (tokens) each on its own line, ready for further analysis.

In order to make what we're doing easier to follow, let's also take a look at our example data.

```{r what does tidy tokenized data look like}
cleaned_reviews_tokenized <- 
  cleaned_reviews_example %>%
  unnest_tokens(input = cleaned_review, output = token) %>%
  # Here we do a bit of extra cleaning
  mutate(token = str_remove_all(token, "\\.|_"))

cleaned_reviews_tokenized %>%
  filter(beer_name == "Kozel") %>%
  # Let's select a few variables for printing
  select(beer_name, rating, token)
```

When we use `unnest_tokens()`, all of the non-text data gets treated as information about each token--so `beer_name`, `rating`, etc are duplicated for each token that now has its own row.  This will allow us to perform a number of types of text analysis.

### A quick note: saving data

We've talked about wanting to clear our workspace from `R` in between sessions to make sure our work and code are reproducible.  But at this point we've done a lot of work, and we might want to save this work for later.  Or, we might want to share our work with others.  The `tidyverse` has a good utility for this when we're working with data frames and tibbles: the `write_csv()` function is the opposite of the `read_csv()` function we've gotten comfortable with: it takes a data frame and writes it into a `.csv` file that is easily shared with others or reloaded in a later session.

```{r example of writing csv files, eval=FALSE}
# Let's store our cleaned and tokenized reviews
write_csv(x = beer_data_tokenized, file = "data/tokenized_beer_reviews.csv")
```

## Text Analysis Pt. II: Basic text analysis approaches using tokens

We've now seen how to import and clean text, and to transform text data into one useful format for analysis: a tidy table of tokens.  (There are certainly other useful formats, but we will not be covering them here.)

Now we get to the actually interesting part.  We're going to tackle some basic but powerful approaches for parsing the meaning of large volumes of text.

### Word counts

A common approach for getting a quick, analytic summary of the nature of the tokens (observations) in our reviews might be to look at the frequency of word use across reviews: this is pretty closely equivalent to a Check-All-That-Apply (CATA) framework: we will look how often each term is used, and will then extend this to categories of interest to our analysis.  So, for example, we might have hypotheses like the following that we could hope to investigate using word frequencies:

1.  Overall, flavor words are the most frequently used words in reviews of beer online.
2.  The frequency of flavor-related words will be different for major categories of beer, like "hop"-related words for IPAs and "chocolate"/"coffee" terms for stouts.
3.  The top flavor words will be qualitatively more positive for reviews associated with the top 10% of ratings, and negative for reviews associated with the bottom 10%.

I will not be exploring each of these in this section of the workshop because of time constraints, but hopefully you'll be able to use the tools I will demonstrate to solve these problems on your own.

Let's start by combining our tidy data wrangling skills from `tidyverse` with our newly tidied text data from `unnest_tokens()` to try to test the first answer.

```{r what are the most frequent words in our beer data}
beer_data_tokenized %>%
  # The count() function gives the number of rows that contain unique values
  count(token) %>%
  # get the 20 most frequently occurring words
  slice_max(order_by = n, n = 20) %>%
  # Here we do some wrangling for visualization
  mutate(token = as.factor(token) %>% fct_reorder(n)) %>%
  # let's visualize this just for fun
  ggplot(aes(x = token, y = n)) + 
  geom_col() + 
  coord_flip() + 
  theme_bw() +
  labs(x = NULL, y = NULL, title = "The 20 most frequent words are not that useful.")
```

Unfortunately for our first hypothesis, we have quickly encountered a common problem in token-based text analysis.  The most frequent words in most languages are "helper" words that are necessary for linguistic structure and communication, but are not unique to a particular topic or context.  For example, in English the articles `a`, and `the` tend to be the most frequent tokens in any text because they are so ubiquitous.

It takes us until the 15th most frequent word, `head`, to find a word that might have sensory or product implications.

So, in order to find meaning in our text, we need to find some way to look past these terms, whose frequency vastly outnumbers words we are actually interested in.

#### "stop words"

In computational linguistics, this kind of word is often called a **stop word**: a word that is functionally important but does not tell us much about the meaning of the text.  There are many common lists of such stop words, and in fact the `tidytext` package provides one such list in the `stop_words` tibble:

```{r stop words look like:}
stop_words
```

The most basic approach to dealing with stop words is just to outright remove them from our data.  This is easy when we've got a tidy, one-token-per-row structure.

We could use some version of `filter()` to remove stop words from our `beer_data_tokenized` tibble, but this is exactly what the `anti_join()` function (familiar to those of you who have used SQL) will do: with two tibbles, `X` and `Y`, `anti_join(X, Y)` will remove all rows in `X` that match rows in `Y`.

```{r what are the most frequent NON stop words in beer data}
beer_data_tokenized %>%
  # "by = " tells what column to look for in X and Y
  anti_join(y = stop_words, by = c("token" = "word")) %>%
  count(token) %>%
  # get the 20 most frequently occurring words
  slice_max(order_by = n, n = 20) %>%
  # Here we do some wrangling for visualization
  mutate(token = as.factor(token) %>% fct_reorder(n)) %>%
  # let's visualize this just for fun
  ggplot(aes(x = token, y = n)) + 
  geom_col() + 
  coord_flip() + 
  theme_bw() +
  labs(x = NULL, y = NULL, title = "These tokens are much more relevant.", subtitle = "We removed ~1200 stop words.")
```

Now we are able to address our very first hypothesis: while the most common term (beer) might be in fact a stop word for this data set, the next 11 top terms all seem to have quite a lot to do with flavor.

We can extend this approach just a bit to provide the start of one answer to our second question: are there different most-used flavor terms for different beer styles?  We can start to tackle this by first defining a couple of categories (since there are a lot of different styles in this dataset):

```{r what beer styles are in our beer data}
beer_data %>%
  count(style)
```

Let's just look at the categories I suggested, plus "lager".  We will do this by using mutate to create a `simple_style` column:

```{r what words are associated with some basic beer styles}
# First, we'll make sure our approach works
beer_data %>%
  # First we will set our style to lower case to make matching easier
  mutate(style = tolower(style),
         # Then we will use a conditional match to create our new style
         simple_style = case_when(str_detect(style, "lager") ~ "lager",
                                  str_detect(style, "ipa") ~ "IPA",
                                  str_detect(style, "stout|porter") ~ "dark",
                                  TRUE ~ "other")) %>%
  count(simple_style)

# Then we'll implement the approach for our tokenized data
beer_data_tokenized <- 
  beer_data_tokenized %>%
  mutate(style = tolower(style),
         # Then we will use a conditional match to create our new style
         simple_style = case_when(str_detect(style, "lager") ~ "lager",
                                  str_detect(style, "ipa") ~ "IPA",
                                  str_detect(style, "stout|porter") ~ "dark",
                                  TRUE ~ "other")) 

# Finally, we'll plot the most frequent terms for each simple_style
beer_data_tokenized %>%
  # filter out stop words
  anti_join(stop_words, by = c("token" = "word")) %>%
  # This time we count tokens WITHIN simple_style
  count(simple_style, token) %>%
  # Then we will group_by the simple_style
  group_by(simple_style) %>%
  slice_max(order_by = n, n = 20) %>%
  # Removing the group_by is necessary for some steps
  ungroup() %>%
  # A bit of wrangling for plotting
  mutate(token = as.factor(token) %>% reorder_within(by = n, within = simple_style)) %>%
  ggplot(aes(x = token, y = n, fill = simple_style)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~simple_style, scales = "free", ncol = 4) + 
  scale_x_reordered() + 
  theme_bw() + 
  coord_flip() +
  labs(x = NULL, y = NULL, title = "Different (sensory) words tend to be used with different styles.") +
  theme(axis.text.x = element_text(angle = 90))
```
