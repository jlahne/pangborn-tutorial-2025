[["index.html", "An introduction to text analysis with R for sensory and consumer scientists Introduction and welcome Introductions Today’s agenda How we’re going to run", " An introduction to text analysis with R for sensory and consumer scientists Jacob Lahne Introduction and welcome Welcome to the Eurosense Sensometrics Workshop “An introduction to text analysis with R for sensory and consumer scientists”! This workshop is going to be conducted not using slides, but through livecoding. That means I am going to run code lines in the console or highlight and run code in scripts and other files. It is also an opportunity and encouragement for you to follow along. Along with introducing myself and my helpers for today’s workshop, we’re going to discuss a bit about how that will work here. Introductions Jacob Lahne, PhD Jacob Lahne is an Assistant Professor of Food Science &amp; Technology at Virginia Tech, in the United States. He runs the Virginia Tech Sensory Evaluation Laboratory, as well as teaching courses in data analytics and coding for food-science research. His main research focuses are sensory data-analysis methodologies and investigating the sensory properties of fermented and distilled foods and beverages. Leah Hamilton, PhD Leah Hamilton is a postdoctoral researcher at the UC Davis Department of Food Science &amp; Technology, in the US. Her primary research interest is flavor language, including the ways that people talk about flavors using their own words in different contexts. Martha Calvert, MS Martha Calvert is a PhD candidate in the Department of Food Science and Technology at Virginia Tech, in the United States. Today’s agenda Today’s workshop is going to take ~3 hours, with a break for lunch, and we’ll be covering the following material: Crash course in using R Creating, importing, and manipulating data in R Principles of tidy data analysis using tidyverse Basics of data visualization in R/ggplot2 Basic text analysis with tidytext Dealing with character data Units of analysis: tokenization TF-IDF models Sentiment Analysis How we’re going to run This workshop is going to be run with livecoding, as noted above. This means I won’t be using slides or a prepared video, but running through code step by step to show how these tools are used in practice. I encourage you to also follow along with livecoding, because the best way to learn coding is to actually do it. Recommended approach for livecoding We recommend that you download the pre-made archive of code and data from the workshop github repo. This archive, when unzipped, will have a folder structure and a .Rproj file. We recommend that you close out RStudio, unzip the archive, and double click the .Rproj file in that folder, which will open a new session of RStudio with proper setting (like the home directory) for the files for this workshop. In that folder, you will find a data/ folder with the necessary data for the workshop, and a script named eurosense-workshop-all-code.R. This latter file contains all of the code demonstrated in this workshop for your future reference. You can also follow along with the code at the workshop’s page hosted on github.io (which you’re reading right now), and which will remain available after this workshop. Once you’re in RStudio, go to the File &gt; New File &gt; R Script menu to open a new script. We’ll talk about how these work in a minute, but this is basically a workbook for you to store sequential lines of code to be run in the Console. It is where you can livecode along! Even though we are giving you all of the code you need right now, you will learn a lot more if you actively follow along, rather than just run that code. Dealing with errors Coding means making mistakes. This is fine–as you will surely see today, I will make a ton of trivial errors and have to fix things on the fly. If you run into trouble, try looking carefully at what you’ve done and see if you can see what went wrong. If that fails, we are here to help! Because we have 3 instructors for this workshop, two of us are available to help at any time. When you run into trouble, please use the red sticky note by putting it on the back of your laptop. We’ll be keeping an eye out, and someone will come to help you. When you’ve resolved your problem, take the sticky note back off. This way you don’t have to raise your hand and interrupt the workshop, etc. However, if your issue is a common one or something we think is worth noting, don’t worry–we’ll make time to discuss it! "],["a-crash-course-in-r.html", "1 A crash course in R 1.1 R vs RStudio 1.2 The parts of RStudio 1.3 Extending R with packages 1.4 Getting help 1.5 Livecoding along", " 1 A crash course in R In this section, we’re going to go over the basics of R: what the heck you’re looking at, how the RStudio IDE works, how to extend R with packages, and some key concepts that will help you work well in R. 1.1 R vs RStudio In this workshop we are going to be learning the basics of coding for text analysis in R, but we will be using the RStudio interface/IDE! Why am I using R for this workshop? And why do we need this extra layer of program to deal with it? 1.1.1 What is R? R is a programming language that was built for statistical and numerical analysis. It is not unique in these spaces–most of you are probably familiar with a program like SAS, SPSS, Unscrambler, XLSTAT, JMP, etc. Unlike these, R is free and open-source. This has two main consequences: R is constantly being extended to do new, useful things because there is a vibrant community of analysts developing tools for it, and the barrier to entry is very low. R doesn’t have a fixed set of tasks that it can accomplish, and, in fact, I generally haven’t found a data-analysis task I needed to do that I couldn’t in R. Because it’s a programming language, R isn’t point-and-click–today we’re going to be typing commands into the console, hitting, enter, making errors, and repeating. But this is a good thing! The power and flexibility of R (and it’s ability to do most of the things we want) come from the fact that it is a programming language. While learning to use R can seem intimidating, the effort to do so will give you a much more powerful suite of tools than the more limited point-and-click alternatives. R is built for research programming (data analysis), rather than for production programming. The only other alternative that is as widely supported in the research community is Python, but–honesty time here–I have never learned Python very well, and so we are learning R. And, in addition, Python doesn’t have as good an Interactive Development Environment (IDE, explained further below) as RStudio! If you open your R.exe/R application, you’ll see something like this: The R graphical console You can also work with R from a shell interface, but I will not be discussing this approach in this workshop. 1.1.2 Great, why are we using RStudio then? RStudio is an “Interactive Development Environment” (IDE) for working with R. Without going into a lot of detail, that means that R lives on its own on your computer in a separate directory, and RStudio provides a bunch of better functionality for things like writing multiple files at once, making editing easier, autofilling code, and displaying plots. You can learn more about RStudio here. With that out of the way, I am going to be sloppy in terminology and say/type “R” a lot of the times I mean “RStudio”. I will be very clear if the distinction actually matters. RStudio is going to make your life way easier, and when you try to learn Python you are going to be sad :( 1.2 The parts of RStudio The default layout of RStudio looks something like this (font sizes may vary): RStudio default layout, courtesy of Data Carpentry RStudio always has 4 “panes” with various functions, which are in tabs (just like a web browser). The key ones for right now to pay attention are: The Console tab is the portal to interact directly with R. The &gt; “prompt” is where you can type and execute commands (by hitting return). You can try this out right now by using it like a calculator - try 1 + 1 if you like! The Files tab shows the files in your working directory: like in the Windows Explorer or macOS Finder, files are displayed within folders. You can click on files to open them. The Help tab shows documentation for R functions and packages–it is useful for learning how to use specific functions. The Plots tab shows graphical output, and this is where the data visualizations we’ll learn to make will (generally) appear. The Environment tab shows the objects that exist in memory in your current R session. Without going into details, this is “what you’ve done” so far: data tables and variables you’ve created, etc. Finally, the Scripts pane shows individual tabs for each script and other RStudio file. Scripts (and other, more exotic file types like RMarkdown/.Rmd files) are documents that contain multiple R commands, like you’d type into the Console. However, unlike commands in the Console, these commands don’t disappear as soon as they’re run, and we can string them together to make workflows or even programs. This is where the real power of R will come from. You can change the layout of your Panes (and many other options) by going to the RStudio menu: Tools &gt; Global Options and select Pane Layout. You’ll notice that my layout for RStudio looks quite different from the default, but you can always orient yourself by seeing what tab or pane I am in–these are always the same. I prefer giving myself more space for writing R scripts and markdown files, so I have given that specific Pane more space while minimizing the History pane. While we’re in Global Options, please make the following selections: Under General, uncheck all the boxes to do with restoring projects and workspaces. We want to make sure our code runs the same time every time (i.e., that our methods are reproducible), and letting RStudio load these will make this impossible: Uncheck the options to restore various data and projects at startup. Make your life easier by setting up autocompletion for your code. Under the Code &gt; Completion options, select the checkboxes to allow using tab for autocompletions, and also allowing multiline autocompletions. This means that RStudio will suggest functions and data for you if you hit tab, which will make you have to do way less typing: Check the boxes for tab and multiline autocompletions. 1.2.1 The “working directory” and why you should care Before we move on to using R for real, we have one key general computing concept to tackle: the “working directory”. The working directory is the folder on your computer in which R will look for files and save files. When you need to tell R to read in data from a file or output a file, you will have to do so in relation to your working directory. Therefore, it is important that you know how to find your working directory and change it. The easiest (but not best) way to do this is to use the Files pane. If you hit the “gear” icon in the Files pane menu, you’ll see two commands to do with the working directory. You can Go To Working Directory to show you whatever R currently has set as the working directory. You can then navigate to any directory you want on your hard drive, and use the Set As Working Directory command to make that the working directory. A better way to do this is to use the R commands getwd() and setwd(). getwd() # will print the current working directory ## [1] &quot;C:/Users/Leah/Documents/R/pangborn-r-tutorial-2023&quot; And we can manually change the working directory by using setwd(&quot;Enter/Your/Desired/Directory/Here&quot;) Notice that I am not running the second command, because it would cause an error! When we use R to navigate directories, I recommend always using the forward slash: /, even thouhg on Windows systems the typical slash is the backslash: \\. R will properly interpret the / for you in the context of your operating system, and this is more consistent with most modern code environments. 1.3 Extending R with packages One of the key advantages of R is that its open-source nature means that you can extend it to do all sorts of things. For example, for much of this workshop we are going to be going about basic text analysis using the tidytext package. There are various ways to install new packages, but the easiest way is to use the Packages tab. This will show you all the packages you currently have installed as an alphabetical list. 1.3.1 Installing packages To install a new package, you can select the Install button from the Packages tab, which will give you a prompt to type the package name in. You can get to the same prompt by going to the Tools &gt; Install Packages... menu. On this prompt, you can list packages separated by a comma (,), which is convenient. RStudio will also try to help you by autocompleting package names. You should have already installed the tidyverse package as part of your pre-work for this workshop. Now, let’s go ahead and install the tidytext package, which we’ll use later in this workshop. If you didn’t install the tidyverse package, you can list it along with the tidytext package. You’ll note that hitting Install made a line of code appear in your console, something like: install.packages(&quot;tidytext&quot;) This is the “true” R way to install packages–the function install.packages() can be run on the Console to install whatever package is quoted inside the parentheses. You can get R packages from a variety of sources. The most common are repositories, like CRAN, which is where you first downloaded R. There are others, like Bioconductor, which is used more by the bioinformatics community. You might also sometime download an install a package that isn’t on a repository, such one from github (for example this one), but I am not going to cover that in detail here. 1.3.2 Loading packages To actually use a package, you need to load it using the library(&lt;name of package&gt;) command. So, for example, to load the tidyverse package we will use the command library(tidyverse) You need to use multiple library() commands to load multiple packages, e.g., library(tidyverse) library(tidytext) If you want to know what packages you have loaded, you can run the sessionInfo() function, which will tell you a bunch of stuff, including the “attached” packages: sessionInfo() ## R version 4.3.0 (2023-04-21 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19044) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: America/Los_Angeles ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidytext_0.4.1 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 ## [5] dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [9] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] janeaustenr_1.0.0 sass_0.4.6 utf8_1.2.3 generics_0.1.3 ## [5] stringi_1.7.12 lattice_0.21-8 hms_1.1.3 digest_0.6.31 ## [9] magrittr_2.0.3 evaluate_0.21 grid_4.3.0 timechange_0.2.0 ## [13] bookdown_0.34 fastmap_1.1.1 jsonlite_1.8.4 Matrix_1.5-4 ## [17] fansi_1.0.4 scales_1.2.1 jquerylib_0.1.4 cli_3.6.1 ## [21] crayon_1.5.2 rlang_1.1.1 tokenizers_0.3.0 bit64_4.0.5 ## [25] munsell_0.5.0 withr_2.5.0 cachem_1.0.8 yaml_2.3.7 ## [29] parallel_4.3.0 tools_4.3.0 tzdb_0.3.0 colorspace_2.1-0 ## [33] vctrs_0.6.2 R6_2.5.1 lifecycle_1.0.3 bit_4.0.5 ## [37] vroom_1.6.3 pkgconfig_2.0.3 pillar_1.9.0 bslib_0.4.2 ## [41] gtable_0.3.3 glue_1.6.2 Rcpp_1.0.10 xfun_0.39 ## [45] tidyselect_1.2.0 rstudioapi_0.14 knitr_1.42 htmltools_0.5.5 ## [49] SnowballC_0.7.1 rmarkdown_2.21 compiler_4.3.0 Finally, you can also load (and unload) packages using the Packages tab, by clciking the checkbox next to the name of the package you want to load (or unload). 1.4 Getting help With more packages you’re going to more frequently run into the need to look up how to do things, which means dealing with help files. You can always get help on a particular function by typing ?&lt;search term&gt;, which will make the help documentation for whatever you’ve searched for appear. For example, try typing the following to get help for the sessionInfo() command: ?sessionInfo But what if you don’t know what to search for? By typing ??&lt;search term&gt; you will search all help files for the search term. R will return a list of matching articles to you in the help pane. This is considerably slower, since it’s searching hundreds or thousands of text files. Try typing ??install into your console to see how this works. You will notice that there are two types of results in the help list for install. The help pages should be familiar. But what are “vignettes”? Try clicking on one to find out. Vignettes are formatted, conversational walkthroughs that are increasingly common (and helpful!) for R packages. Rather than explaining a single function they usually explain some aspect of a package, and how to use it. And, even better for our purposes, they are written in R Markdown. Click the “source” link next to the vignette name in order to see how the author wrote it in R Markdown. This is a great way to learn new tricks. While you can find vignettes as we just did, a better way is to use the function browseVignettes(). This opens a web browser window that lists all vignettes installed on your computer. You can then use cmd/ctrl + F to search using terms in the web browser and quickly find package names, function names, or topics you are looking for. 1.5 Livecoding along We’ve now covered the Console tab and the Scripts pane. These are both areas in which you can write and execute code, but they work a little differently. The Console is the place to run code that is short and easy to type, or that you’re experimenting with. It will allow you to write a single line of code, and after you hit return, R will execute the command. This is great for “interactive programming”, but it isn’t so great for building up a complex workflow, or for following along with this workshop! This is why I have recommended that you create a new script to follow along with this workshop. Again, you get a new script by going to File &gt; New File &gt; R Script. You can write multiple lines of code and then execute each one in any order (although keeping a logical sequence from top to bottom will help you keep track of what you’re doing). In an R script, everything is expected to be valid R code. You can&#39;t write this in an R script because it is plain text. This will cause an error. # If you want to write text or notes to yourself, use the &quot;#&quot; symbol at the start of # every line to &quot;comment&quot; out that line. You can also put &quot;#&quot; in the middle of # a line in order to add a comment - everything after will be ignored. 1 + 1 # this is valid R syntax print(&quot;hello world&quot;) # this is also valid R syntax To run code from your R script, put your cursor on the line you want to run and either hit the run button with the green arrow at the top left or (my preferred method) type cmd + return (on Mac) or ctrl + return (on PC). "],["how-to-work-with-r.html", "2 How to work with R 2.1 Doing math and creating objects in R 2.2 Functions and their arguments in R 2.3 Reading data into R 2.4 Data in R 2.5 Subsetting and wrangling data tables 2.6 PSA: not-knowing is normal!", " 2 How to work with R Now that we’ve all got R up and running, we’re going to quickly go over the basic functionality of R to make sure everyone is on the same page. If you have ever used R, this might include some review, but my hope is that this will be helpful to everyone and get us all on the same page before we launch into some more advanced applications. We’re going to speed through the basics of the console (working interactively with R) and then some of the “programming”/“coding” capabilities in R. 2.1 Doing math and creating objects in R At it’s most basic, R can be a calculator. If you type math into the Console and hit return it will do math for you! 2 + 5 ## [1] 7 1000 / 3.5 ## [1] 285.7143 To get the most out of R, though, we are going to want to use its abilities as a programming language. Among some other topics that I won’t explicitly cover today, this means using R to store values (and later objects that contain multiple lines of values, like you’d get in an Excel spreadsheet). This set of characters is the assignment operator: &lt;-. It works like this: x &lt;- 100 hi &lt;- &quot;hello world&quot; data_set &lt;- rnorm(n = 100, mean = 0, sd = 1) … but that didn’t do anything! Where’s the output? Well, we can do two things. First, look at the “Environment” tab in your RStudio after you run the above code chunk. You’ll notice that there are 3 new things there: x, hi, and data_set. In general I am going to call those objects–they are stored variables, which R now knows about by name. How did it learn about them? You guessed it: the assignment operator: &lt;-. To be explicit: x &lt;- 100 can be read in English as “x gets 100” (what a lot of programmers like to say) or, in a clearer but longer way, “assign 100 to a variable called x”. NB: R also allows you to use = as an assignment operator. DO NOT DO THIS!. There are two good reasons. It is ambiguous, because it is not directional (how are you sure what is getting assigned where?) This makes your code super confusing because = is the only assignment operator for arguments in functions (as in print(quote = FALSE) see below for more on this) Anyone who has used R for a while who sees you do this will roll their eyes and kind of make fun of you a little bit NB2: Because it is directional, it is actually possible to use -&gt; as an assignment operator as well. What do you think it does? Check and find out. If you find typing &lt;- a pain (reasonably), RStudio provides a keyboard shortcut: either option + - on Mac or Alt + - on PC. I am going to use the terms “object” and “variable” interchangeably in this workshop–in other programming languages there are hard distinctions between these concepts, and even in some R packages this can be important, but for our purposes I mean the same thing if I am sloppy. The advantage of storing values into objects is that we can do math with them, change them, and duplicate and store them elsewhere. x / 10 ## [1] 10 x + 1 ## [1] 101 Note that doing math with a variable doesn’t change the variable itself. To do that, you have to use the assignment &lt;- to change the value of the variable again: x ## [1] 100 x &lt;- 100 + 1 x ## [1] 101 2.2 Functions and their arguments in R Obviously if we’re using R as a calculator we might want to do more than basic arithmetic. What about taking the square root of x? sqrt(x) ## [1] 10.04988 In fact, we can ask R to do lots of neat stuff, like generate random numbers for us. For example, here are 100 random numbers from a normal distribution with mean of 0 and standard deviation of 1. rnorm(n = 100, mean = 0, sd = 1) ## [1] 0.493314849 0.037875052 -0.211972307 -0.875487006 0.814358638 ## [6] -0.868587814 -0.020584111 0.389302201 0.045168552 -1.153879935 ## [11] 0.872436440 -1.854804848 0.752453147 -1.386239257 -1.495545322 ## [16] 0.142909754 -1.449700289 -1.445263805 1.806849572 -0.234379139 ## [21] -0.215623795 0.344922672 -1.559864288 1.864363246 1.146328013 ## [26] 0.705156965 0.182219071 -0.909343999 0.462941302 -1.739210947 ## [31] 2.128012614 0.976619954 -1.908483671 -1.285247803 -0.325279320 ## [36] 1.235528183 0.725011919 -0.631203797 0.019585759 -0.698657145 ## [41] 1.594744703 -1.336593507 0.421908848 -0.371551728 -1.418393033 ## [46] 0.359238602 0.070269750 -0.440425309 1.808341281 -0.422607262 ## [51] -1.076978311 2.090414396 1.644807946 0.826807026 -0.460110527 ## [56] 0.528209201 0.162983875 1.893506167 -0.349446812 -1.199174318 ## [61] 0.747865836 1.003744338 1.144803866 -0.004604193 0.889891940 ## [66] -1.064555795 0.560428654 -0.077482382 0.129229629 -1.529513215 ## [71] 2.066494468 1.418752470 0.998379563 -0.090186051 -2.226059548 ## [76] 0.654523496 1.423097012 -0.167062902 -0.137992001 1.654034282 ## [81] -0.388873449 -1.439625065 0.315172074 1.100587344 0.589404324 ## [86] 1.740988679 -2.812221802 0.516105557 -0.854028120 -0.059160053 ## [91] 1.378400882 -2.642380043 -1.204771792 1.244427027 -2.012964182 ## [96] 0.114693495 0.090963804 0.510090308 -0.311709831 -0.331538613 These forms are called a function in R. Functions lie at the heart of R’s power: they are pre-written scripts that are included with base R or added in packages, like the ones we installed. In general, an R function will have a form like &lt;name&gt;(&lt;argument&gt;, &lt;argument&gt;, ...). In other words, the function will have a name (that lets R know what you’re trying to do) followed by an open parenthesis, and inside that a list of arguments, which are variables, objects, values, etc that you “pass” to the function, finally followed by a close parenthesis. In the case of our sqrt() function, there is only a single argument: a variable to which the square-root operation will be applied. In the case of the rnorm() function there are 3 arguments: the number of values we want, n, and the mean and standard deviation sd of the normal distribution we wish to sample from. Functions are the R tools for which you can make the most use of the help operator: ?. Try typing ?rnorm into your console, and when you hit return you’ll see the help page for the function. Notice that in the rnorm() example we named the arguments–we told R which was the n, mean, and sd. This is because if we name arguments, we can give them in any order. Otherwise, R will try to match the provided values to the arguments in the order in which they are given in the help file. This is a major source of errors for newer R users! # This will give us 1 value from the normal distribution with mean = 0 and sd = 100 rnorm(1, 0, 100) ## [1] 47.18328 # But we can also use named arguments to provide them in any order we wish! rnorm(sd = 1, n = 100, mean = 0) ## [1] 0.057848995 -0.495057447 0.844496326 -0.153930514 0.743224961 ## [6] 0.669449721 1.106530676 0.721502710 0.298407278 -0.134521056 ## [11] 0.239202533 0.328215416 1.641046413 0.747961113 0.950868788 ## [16] 0.570694764 0.264835549 -1.389866238 -1.693586364 -0.712183326 ## [21] -0.151952622 -2.844356664 0.776679992 -0.496039287 0.256124599 ## [26] 1.008850441 2.201486250 0.080494920 1.339432324 0.234445699 ## [31] -0.911168498 0.083887666 -0.977560174 -1.167568364 -0.335448973 ## [36] -0.538334347 -0.433266880 1.843038176 0.620781663 -2.293211221 ## [41] -0.412376413 0.062321220 -0.889774392 2.326536173 -1.242482651 ## [46] 1.184015926 -1.643253063 0.094257223 1.678081274 -0.817034022 ## [51] 1.598765257 1.168040054 1.275514591 0.352142311 0.028311079 ## [56] 0.160118173 -1.314070883 0.815794134 0.847673124 0.782306176 ## [61] -1.246646743 -1.474180122 -0.297977942 -0.825252688 -0.898017230 ## [66] 0.754474976 0.336108217 -1.324008783 -0.583290890 0.252256470 ## [71] 1.167557005 0.909657307 0.340683562 -0.454838368 0.414714117 ## [76] -1.798439986 -1.487966336 1.041104430 2.039158453 0.735159865 ## [81] 0.549553336 -1.242439183 -0.929697566 -0.741427413 1.119578891 ## [86] 0.433362464 0.291564825 -0.761713502 1.058681234 -0.742053370 ## [91] 0.158985468 0.469426204 0.913799161 0.002775476 -1.868987754 ## [96] -1.255792369 -0.484834511 0.245653161 1.197361313 -0.015974368 Programming languages like R are very literal, and we need to be as literal as we can to make them work the way we want them to. 2.3 Reading data into R So far we’ve only done very basic things with R, which probably haven’t sold you on its power and utility. Let’s try doing some things that will hopefully get us a little further towards actual, useful applications. First off, make sure you have the tidyverse package loaded by using the library() function. library(tidyverse) Now, we’re going to read in the data we’re going to use for this workshop using the function read_csv(). If you want to learn about this function, use the ?read_csv command to get some details. In the workshop archive you downloaded, the data/ directory has a file called ba_2002.csv. If you double-click this in your file browser, it will (most likely) open in Excel, and you can take a look at it. It is 20,000 reviews of beer, posted in the year 2002. This is an edited (not original) and very delimited version of the dataset (which had &gt;1 million reviews), as the original one has been removed from the web at the original website owner’s request. It will be the training dataset we use for today. To get this data into R, we have to tell R where it lives on your computer, and what kind of data it is. 2.3.1 Where the data lives We touched on working directories because this is how R “sees” your computer. It will look first in the working directory, and then you will have to tell it where the file is relative to that directory. If you have been following along and opened up the .Rproj file in the downloaded archive, your working directory should be the archive’s top level, which will mean that we only need to point R towards the data/ folder and then the ba_2002.csv file. We can check the working directory with the getwd() function. getwd() ## [1] &quot;C:/Users/Leah/Documents/R/pangborn-r-tutorial-2023&quot; Therefore, relative to the working directory, the file path to this data is data/ba_2002.csv. Please note that this is the UNIX convention for file paths: in Windows, the backslash \\ is used to separate directories. Happily, RStudio will translate between the two conventions, so you can just follow along with the macOS/UNIX convention in this workshop. 2.3.2 What kind of file are we importing? The first step is to notice this is a .csv file, which stands for comma-separated value. This means our data, in raw format, looks something like this: # Comma-separated data cat_acquisition_order,name,weight\\n 1,Nick,9\\n 2,Margot,10\\n 3,Little Guy,13\\n Each line represents a row of data, and each field is separated by a comma (,). We can read this kind of data into R by using the read_csv() function. read_csv(file = &quot;data/ba_2002.csv&quot;) ## Rows: 20359 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): reviews, beer_name, brewery_name, style, user_id ## dbl (9): beer_id, brewery_id, abv, appearance, aroma, palate, taste, overall... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 20,359 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 This is a very… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 jisurf… ## 2 Great head cre… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 allbou… ## 3 Wow what a dif… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 frank4… ## 4 Tried this one… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 beergl… ## 5 Caffrey&#39;s on-t… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 proc.1… ## 6 The nitro can … Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… ## 7 Nitro can...Ty… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 adr.263 ## 8 Excellent pour… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 dogbri… ## 9 A very good Ir… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 goindo… ## 10 Looks very nic… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 irishr… ## # ℹ 20,349 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; Suddenly, we have tabular data (i.e., data in rows and columns), like we’d have in Excel! Now we’re getting somewhere. However, before we go forward we’ll have to store this data somewhere–right now we’re just reading it and throwing it away. beer_data &lt;- read_csv(file = &quot;data/ba_2002.csv&quot;) ## Rows: 20359 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): reviews, beer_name, brewery_name, style, user_id ## dbl (9): beer_id, brewery_id, abv, appearance, aroma, palate, taste, overall... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. As a note, in many countries the separator (delimiter) will be the semi-colon (;), since the comma is used as the decimal marker. To read files formatted this way, you can use the read_csv2() function. If you encounter tab-separated values files (.tsv) you can use the read_tsv() function. If you have more non-standard delimiters, you can use the read_delim() function, which will allow you to specify your own delimiter characters. You can also read many other formats of tabular data using the rio package (“read input/output”), which can be installed from CRAN. 2.4 Data in R Let’s take a look at the Environment tab. Among some other objects you may have created (like x), you should see beer_data listed. This is a type of data called a data.frame in R, and it is going to be, for the most part, the kind of data you interact with most. Let’s learn about how these types of objects work by doing a quick review of the basics. We started by creating an object called x and storing a number (100) into it. What kind of thing is this? x &lt;- 100 class(x) ## [1] &quot;numeric&quot; typeof(x) ## [1] &quot;double&quot; R has a bunch of basic data types, including the above “numeric” data type, which is a “real number” (in computer terms, a floating-point double as opposed to an integer). It can also store logical values (TRUE and FALSE), integers, characters/strings (which are what we’re really here to deal with) and some more exotic data types you won’t encounter very much. What R does that makes it good for data analysis is that it stores these all as vectors: 1-dimensional arrays of the same type of data. So, in fact, x is a length-1 vector of numeric data: length(x) ## [1] 1 The operator to explicitly make a vector in R is the c() function, which stands for “combine”. So if we want to make a vector of a few values, we use this function as so: y &lt;- c(1, 2, 3, 10, 50) y ## [1] 1 2 3 10 50 We can also use c() to combine pre-existing objects: c(x, y) ## [1] 100 1 2 3 10 50 You can have vectors of other types of objects: animals &lt;- c(&quot;fox&quot;, &quot;bat&quot;, &quot;rat&quot;, &quot;cat&quot;) class(animals) ## [1] &quot;character&quot; If we try to combine vectors of 2 types of data, R will “coerce” the data types to match to the less restrictive type, in the following order: logical &gt; integer &gt; numeric &gt; character. So if we combine y and animals, we’ll turn the numbers into their character representations. I mention this because it can be a source of error and confusion when we are working with large datasets, as we may see. c(y, animals) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;10&quot; &quot;50&quot; &quot;fox&quot; &quot;bat&quot; &quot;rat&quot; &quot;cat&quot; For example, we can divide all the numbers in y by 2, but if we try to divide c(y, animals) by 2 we will get an error: c(y, animals) / 2 ## Error in c(y, animals)/2: non-numeric argument to binary operator For vectors (and more complex objects), we can use the str() (“structure”) function to get some details about their nature and what they contain: str(y) ## num [1:5] 1 2 3 10 50 str(animals) ## chr [1:4] &quot;fox&quot; &quot;bat&quot; &quot;rat&quot; &quot;cat&quot; This str() function is especially useful when we have big, complicated datasets, like beer_data: str(beer_data) ## spc_tbl_ [20,359 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ reviews : chr [1:20359] &quot;This is a very good Irish ale. I&#39;m not sure if it would be considered a cream ale, but it&#39;s very creamy in colo&quot;| __truncated__ &quot;Great head creation and retention from the Nitro can. A pinky finger width head stuck with the beer all the way&quot;| __truncated__ &quot;Wow what a different brew in England. I still am not a huge fan but the brew is improved on tap and fresh. It t&quot;| __truncated__ &quot;Tried this one in a steak house. Pours a light reddish colour topped by a thick white head that settles rather &quot;| __truncated__ ... ## $ beer_name : chr [1:20359] &quot;Caffrey&#39;s Irish Ale&quot; &quot;Caffrey&#39;s Irish Ale&quot; &quot;Caffrey&#39;s Irish Ale&quot; &quot;Caffrey&#39;s Irish Ale&quot; ... ## $ beer_id : num [1:20359] 825 825 825 825 825 825 825 825 825 825 ... ## $ brewery_name: chr [1:20359] &quot;Thomas Caffrey Brewing Co.&quot; &quot;Thomas Caffrey Brewing Co.&quot; &quot;Thomas Caffrey Brewing Co.&quot; &quot;Thomas Caffrey Brewing Co.&quot; ... ## $ brewery_id : num [1:20359] 297 297 297 297 297 297 297 297 297 297 ... ## $ style : chr [1:20359] &quot;Irish Red Ale&quot; &quot;Irish Red Ale&quot; &quot;Irish Red Ale&quot; &quot;Irish Red Ale&quot; ... ## $ abv : num [1:20359] 3.8 3.8 3.8 3.8 3.8 3.8 3.8 3.8 3.8 3.8 ... ## $ user_id : chr [1:20359] &quot;jisurfer.1152&quot; &quot;allboutbierge.746&quot; &quot;frank4sail.48&quot; &quot;beerglassescollector.1006&quot; ... ## $ appearance : num [1:20359] 5 5 4 3.5 4 4 3.5 4 4 4 ... ## $ aroma : num [1:20359] 4 3.5 3 3.5 3 3 3.5 3 4 3 ... ## $ palate : num [1:20359] 4 4 3 3.5 4 2.5 4 3 4 3.5 ... ## $ taste : num [1:20359] 4.5 3.5 3.5 3.5 4.5 3 3.5 3 4 3 ... ## $ overall : num [1:20359] 5 4 3 3.5 3.5 3.5 3.5 3 4 3.5 ... ## $ rating : num [1:20359] 4.46 3.74 3.26 3.5 3.86 3.11 3.55 3.06 4 3.21 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. reviews = col_character(), ## .. beer_name = col_character(), ## .. beer_id = col_double(), ## .. brewery_name = col_character(), ## .. brewery_id = col_double(), ## .. style = col_character(), ## .. abv = col_double(), ## .. user_id = col_character(), ## .. appearance = col_double(), ## .. aroma = col_double(), ## .. palate = col_double(), ## .. taste = col_double(), ## .. overall = col_double(), ## .. rating = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 2.4.1 Subsetting data Vectors, by nature, are ordered arrays of data (in this case, 1-dimensional arrays). That means they have a first element, a second element, and so on. Our y vector has 5 total elements, and our animals vector has 4 elements. In R, the way to subset vectors is to use the [] (square brackets) operator. For a 1-dimensional vector, we use this to select one or more elements: y[1] ## [1] 1 animals[4] ## [1] &quot;cat&quot; We can also select multiple elements by using a vector of indices animals[c(1, 2)] ## [1] &quot;fox&quot; &quot;bat&quot; A shortcut for a sequence of numbers in R is the : (colon) operator, so this is often used for indexing: 1:3 ## [1] 1 2 3 animals[1:3] ## [1] &quot;fox&quot; &quot;bat&quot; &quot;rat&quot; We often want to use programmatic (or “conditional”) logic to subset vectors and more complex datasets. For example, we might want to only select elements of y that are less than 10. To do that, we can use one of R’s conditional logic operators: &lt;, &gt;, &lt;=, &gt;=, ==, or !=. These, in order, stand for “less than,”greater than,“,”less than or equal to,” “greater than or equal to,” “equal to,” and “not equal to.” y &lt; 10 ## [1] TRUE TRUE TRUE FALSE FALSE We can then use this same set of logical values to select only the elements of y for which the condition is TRUE: y[y &lt; 10] ## [1] 1 2 3 This is useful if we have a long vector (frequently) and do not want to list or are not able to list all of the actual indices that we want to select. 2.4.2 Complex vectors/lists: data.frame and tibble Now that we have the basics of vectors, we can move on to the complex data object we’re really interested in: beer_data. This is a type of object called a tibble, which is a cute/fancy version of the more basic R object called a data.frame. These are R’s version of the .csv file or your typical Excel file: a rectangular matrix of data, with (usually) columns representing some variable and rows representing some kind of observation. Each row will have a value in each column or will be NA, which is R’s specific value to represent missing data. In a data.frame, every column has to have only a single data type: a column might be logical or integer or character, but it cannot be a mix. However, each column can be a different type. For example, the first column in our beer_data, called reviews, is a character vector, but the third column, beer_id, is a numeric column. We have now moved from 1-dimensional vectors to 2-dimensional data tables, which means we’re going to have some new properties to investigate. First off, we might want to know how many rows and columns our data table has: nrow(beer_data) ## [1] 20359 ncol(beer_data) ## [1] 14 length(beer_data) # Note that length() of a data.frame is the same as ncol() ## [1] 14 We already tried running str(beer_data), which gives us the data types of each column and an example. Some other ways to examine the data include the following: beer_data # simply printing the object head(beer_data) # show the first few rows tail(beer_data) # show the last few rows glimpse(beer_data) # a more compact version of str() names(beer_data) # get the variable/column names Note that some of these functions (for example, glimpse()) come from the tidyverse package, so if you are having trouble running a command, first make sure you have run library(tidyverse). 2.5 Subsetting and wrangling data tables Since we now have 2 dimensions, our old strategy of using a single number to select a value from a vector won’t work! But the [] operator still works on data frames and tibbles. We just have to specify coordinates, as [&lt;row&gt;, &lt;column&gt;]. beer_data[5, 1] # get the 5th row, 1st column value ## # A tibble: 1 × 1 ## reviews ## &lt;chr&gt; ## 1 Caffrey&#39;s on-tap is less creamy than the nitro can, but seems to be more subs… We can continue to use ranges or vectors of indices to select larger parts of the table beer_data[1:5, 1] # get the first 5 rows of the 1st column value ## # A tibble: 5 × 1 ## reviews ## &lt;chr&gt; ## 1 This is a very good Irish ale. I&#39;m not sure if it would be considered a cream… ## 2 Great head creation and retention from the Nitro can. A pinky finger width he… ## 3 Wow what a different brew in England. I still am not a huge fan but the brew … ## 4 Tried this one in a steak house. Pours a light reddish colour topped by a thi… ## 5 Caffrey&#39;s on-tap is less creamy than the nitro can, but seems to be more subs… If we only want to subset on a specific dimension and get everything from the other dimension, we just leave it blank. beer_data[, 2] # get all rows of the 2nd column ## # A tibble: 20,359 × 1 ## beer_name ## &lt;chr&gt; ## 1 Caffrey&#39;s Irish Ale ## 2 Caffrey&#39;s Irish Ale ## 3 Caffrey&#39;s Irish Ale ## 4 Caffrey&#39;s Irish Ale ## 5 Caffrey&#39;s Irish Ale ## 6 Caffrey&#39;s Irish Ale ## 7 Caffrey&#39;s Irish Ale ## 8 Caffrey&#39;s Irish Ale ## 9 Caffrey&#39;s Irish Ale ## 10 Caffrey&#39;s Irish Ale ## # ℹ 20,349 more rows We can also use logical subsetting, just like in vectors. This is very powerful but a bit complicated, so we are going to introduce some tidyverse based operators to do this that will make it a lot easier. I will just give an example: beer_data[beer_data$rating &gt; 4.5, ] # get all rows for which rating &gt; 3 ## # A tibble: 1,952 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 I must agree w… Old Engi… 875 Harviestoun… 323 Engl… 6 viking… ## 2 I had this bee… Schiehal… 6438 Harviestoun… 323 Euro… 4.8 thelon… ## 3 Haven&#39;t found … Deuchars… 2389 The Caledon… 188 Engl… 4.4 freed.… ## 4 Once again had… Deuchars… 2389 The Caledon… 188 Engl… 4.4 mark.65 ## 5 Hazy (bottle c… Edinburg… 6466 The Caledon… 188 Scot… 6.4 nerofi… ## 6 The first time… McEwan&#39;s… 911 The Caledon… 188 Engl… 4.7 jdhilt… ## 7 The brew pours… McEwan&#39;s… 1275 The Caledon… 188 Scot… 8 murph.… ## 8 Deep dark brow… McEwan&#39;s… 1275 The Caledon… 188 Scot… 8 zap.184 ## 9 Clear deep sca… Traquair… 36 Traquair Ho… 24 Scot… 7.2 bighug… ## 10 Reddish brown … Traquair… 36 Traquair Ho… 24 Scot… 7.2 mjohn2… ## # ℹ 1,942 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; In this last example I also introduced the final bit of tibble and data.frame wrangling we will cover here: the $ operator. This is the operator to select a single column from a data.frame or tibble. It gives you back the vector that makes up that column: beer_data$style # not printed because it is too long! One of the nice things about RStudio is that it provides tab-completion for $. Go to the console, type “bee” and hit tab. You’ll see a list of possible matches, with beer_data at the top. Hit enter, and this will fill out the typing for you! Now, type “$” and hit tab again. You’ll see a list of the columns in beer_data! This can save a huge amount of typing and memorizing the names of variables and objects. Now that we’ve gone over the basics of creating and manipulating objects in R, we’re going to run through the basics of data manipulation and visualization with the tidyverse. Before we move on to that topic, let’s address any questions. 2.6 PSA: not-knowing is normal! Above, I mentioned “help files”. How do we get help when we (inevitably) run into problems in R? There are a couple steps you will find helpful in the future: Look up the help file for whatever you’re doing. Do this by using the syntax ?&lt;search item&gt; (for example ?c gets help on the vector command) as a shortcut on the console. Search the help files for a term you think is related. Can’t remember the command for making a sequence of integers? Go to the “Help” pane in RStudio and search in the search box for “sequence”. See if some of the top results get you what you need. The internet. Seriously. I am not kidding even a little bit. R has one of the most active and (surprisingly) helpful user communities I’ve ever encountered. Try going to google and searching for “How do I make a sequence of numbers in R?” You will find quite a bit of useful help. I find the following sites particularly helpful Stack Overflow Cross Validated/Stack Exchange Seriously, Google will get you most of the way to helpful answers for many basic R questions. We may come back to this, but I want to emphasize that looking up help is normal. I do it all the time. Learning to ask questions in helpful ways, how to quickly parse the information you find, and how to slightly alter the answers to suit your particular situation are key skills. "],["wrangling-data-with-tidyverse.html", "3 Wrangling data with tidyverse 3.1 Subsetting data 3.2 Combining steps with the pipe: %&gt;% 3.3 Make new columns: mutate() 3.4 Split-apply-combine analyses with group_by() and summarize() 3.5 Utilities for data management", " 3 Wrangling data with tidyverse A common saying in data science is that about 90% of the effort in an analysis workflow is in getting data wrangled into the right format and shape, and 10% is actual analysis. In a point and click program like SPSS or XLSTAT we don’t think about this as much because the activity of reshaping the data–making it longer or wider as required, finding and cleaning missing values, selecting columns or rows, etc–is often temporally and programmatically separated from the “actual” analysis. In R, this can feel a bit different because we are using the same interface to manipulate our data and to analyze it. Sometimes we’ll want to jump back out to a spreadsheet program like Excel or even the command line (the “shell” like bash or zsh) to make some changes. But in general the tools for manipulating data in R are both more powerful and more easily used than doing these activities by hand, and you will make yourself a much more effective analyst by mastering these basic tools. Here, we are going to emphasize the set of tools from the tidyverse, which are extensively documented in Hadley Wickham’s and Garrett Grolemund’s book R for Data Science. If you want to learn more, start there! The tidyverse is associated with this hexagonal iconography. Before we move on to actually learning the tools, let’s make sure we’ve got our data loaded up. library(tidyverse) beer_data &lt;- read_csv(&quot;data/ba_2002.csv&quot;) ## Rows: 20359 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): reviews, beer_name, brewery_name, style, user_id ## dbl (9): beer_id, brewery_id, abv, appearance, aroma, palate, taste, overall... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 3.1 Subsetting data R’s system for indexing data frames is clear and sensible for those who are used to programming languages, but it is not necessarily easy to read. A common situation in R is wanting to select some rows and some columns of our data–this is called “subsetting” our data. But this is less easy than it might be for the beginner in R. Happily, the tidverse methods are much easier to read (and modeled after syntax from SQL, which may be helpful for some users.) We are going to focus on 3.1.1 select() for columns The first thing we often want to do in a data analysis is to pick a subset of columns, which usually represent variables. If we take a look at our beer data, we see that, for example, we have some columns that are data about our beers, and some columns that are user-generated responses: glimpse(beer_data) ## Rows: 20,359 ## Columns: 14 ## $ reviews &lt;chr&gt; &quot;This is a very good Irish ale. I&#39;m not sure if it would … ## $ beer_name &lt;chr&gt; &quot;Caffrey&#39;s Irish Ale&quot;, &quot;Caffrey&#39;s Irish Ale&quot;, &quot;Caffrey&#39;s … ## $ beer_id &lt;dbl&gt; 825, 825, 825, 825, 825, 825, 825, 825, 825, 825, 825, 82… ## $ brewery_name &lt;chr&gt; &quot;Thomas Caffrey Brewing Co.&quot;, &quot;Thomas Caffrey Brewing Co.… ## $ brewery_id &lt;dbl&gt; 297, 297, 297, 297, 297, 297, 297, 297, 297, 297, 297, 29… ## $ style &lt;chr&gt; &quot;Irish Red Ale&quot;, &quot;Irish Red Ale&quot;, &quot;Irish Red Ale&quot;, &quot;Irish… ## $ abv &lt;dbl&gt; 3.8, 3.8, 3.8, 3.8, 3.8, 3.8, 3.8, 3.8, 3.8, 3.8, 3.8, 3.… ## $ user_id &lt;chr&gt; &quot;jisurfer.1152&quot;, &quot;allboutbierge.746&quot;, &quot;frank4sail.48&quot;, &quot;b… ## $ appearance &lt;dbl&gt; 5.0, 5.0, 4.0, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.0, 2.… ## $ aroma &lt;dbl&gt; 4.0, 3.5, 3.0, 3.5, 3.0, 3.0, 3.5, 3.0, 4.0, 3.0, 4.0, 2.… ## $ palate &lt;dbl&gt; 4.0, 4.0, 3.0, 3.5, 4.0, 2.5, 4.0, 3.0, 4.0, 3.5, 3.0, 2.… ## $ taste &lt;dbl&gt; 4.5, 3.5, 3.5, 3.5, 4.5, 3.0, 3.5, 3.0, 4.0, 3.0, 3.0, 2.… ## $ overall &lt;dbl&gt; 5.0, 4.0, 3.0, 3.5, 3.5, 3.5, 3.5, 3.0, 4.0, 3.5, 3.5, 2.… ## $ rating &lt;dbl&gt; 4.46, 3.74, 3.26, 3.50, 3.86, 3.11, 3.55, 3.06, 4.00, 3.2… So, for example, we might want to try to reverse engineer the way that the rating is generated from the sub-modalities, in which case perhaps we only want the last 6 columns. We learned previously that we can do this with numeric indexing: beer_data[, 9:14] ## # A tibble: 20,359 × 6 ## appearance aroma palate taste overall rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 4 4 4.5 5 4.46 ## 2 5 3.5 4 3.5 4 3.74 ## 3 4 3 3 3.5 3 3.26 ## 4 3.5 3.5 3.5 3.5 3.5 3.5 ## 5 4 3 4 4.5 3.5 3.86 ## 6 4 3 2.5 3 3.5 3.11 ## 7 3.5 3.5 4 3.5 3.5 3.55 ## 8 4 3 3 3 3 3.06 ## 9 4 4 4 4 4 4 ## 10 4 3 3.5 3 3.5 3.21 ## # ℹ 20,349 more rows However, this is both difficult for novices to R and difficult to read if you are not intimately familiar with the data. It is also rather fragile–what if someone else rearranged the data in your import file? You’re just selecting the last 6 columns, which are not guaranteed to contain the rating data you’re interested in. The select() function in tidyverse (actually from the dplyr package) is the smarter, easier way to do this. It works on data frames, and it can be read as “from &lt;data frame&gt;, select the columns that meet the criteria we’ve set.” select(&lt;data frame&gt;, &lt;column 1&gt;, &lt;column 2&gt;, ...) The simplest way to use select() is just to name the columns you want! select(beer_data, appearance, aroma, palate, taste, overall, rating) # note the lack of quoting on the column names ## # A tibble: 20,359 × 6 ## appearance aroma palate taste overall rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 4 4 4.5 5 4.46 ## 2 5 3.5 4 3.5 4 3.74 ## 3 4 3 3 3.5 3 3.26 ## 4 3.5 3.5 3.5 3.5 3.5 3.5 ## 5 4 3 4 4.5 3.5 3.86 ## 6 4 3 2.5 3 3.5 3.11 ## 7 3.5 3.5 4 3.5 3.5 3.55 ## 8 4 3 3 3 3 3.06 ## 9 4 4 4 4 4 4 ## 10 4 3 3.5 3 3.5 3.21 ## # ℹ 20,349 more rows This is much clearer to the reader. You can also use select() with a number of helper functions, which use logic to select columns that meet whatever conditions you set. For example, the starts_with() helper function lets us give a set of characters we want columns to start with: select(beer_data, starts_with(&quot;beer&quot;)) ## # A tibble: 20,359 × 2 ## beer_name beer_id ## &lt;chr&gt; &lt;dbl&gt; ## 1 Caffrey&#39;s Irish Ale 825 ## 2 Caffrey&#39;s Irish Ale 825 ## 3 Caffrey&#39;s Irish Ale 825 ## 4 Caffrey&#39;s Irish Ale 825 ## 5 Caffrey&#39;s Irish Ale 825 ## 6 Caffrey&#39;s Irish Ale 825 ## 7 Caffrey&#39;s Irish Ale 825 ## 8 Caffrey&#39;s Irish Ale 825 ## 9 Caffrey&#39;s Irish Ale 825 ## 10 Caffrey&#39;s Irish Ale 825 ## # ℹ 20,349 more rows There are equivalents for the end of column names (ends_with()) and text found anywhere in the name (contains()). You can combine these statements together to get subsets of columns however you want: select(beer_data, starts_with(&quot;beer&quot;), rating, abv) ## # A tibble: 20,359 × 4 ## beer_name beer_id rating abv ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Caffrey&#39;s Irish Ale 825 4.46 3.8 ## 2 Caffrey&#39;s Irish Ale 825 3.74 3.8 ## 3 Caffrey&#39;s Irish Ale 825 3.26 3.8 ## 4 Caffrey&#39;s Irish Ale 825 3.5 3.8 ## 5 Caffrey&#39;s Irish Ale 825 3.86 3.8 ## 6 Caffrey&#39;s Irish Ale 825 3.11 3.8 ## 7 Caffrey&#39;s Irish Ale 825 3.55 3.8 ## 8 Caffrey&#39;s Irish Ale 825 3.06 3.8 ## 9 Caffrey&#39;s Irish Ale 825 4 3.8 ## 10 Caffrey&#39;s Irish Ale 825 3.21 3.8 ## # ℹ 20,349 more rows You can also use programmatic logic in select() by using the where() helper function, which gives you the ability to specify columns by any arbitrary function. select(beer_data, where(~is.numeric(.))) ## # A tibble: 20,359 × 9 ## beer_id brewery_id abv appearance aroma palate taste overall rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 825 297 3.8 5 4 4 4.5 5 4.46 ## 2 825 297 3.8 5 3.5 4 3.5 4 3.74 ## 3 825 297 3.8 4 3 3 3.5 3 3.26 ## 4 825 297 3.8 3.5 3.5 3.5 3.5 3.5 3.5 ## 5 825 297 3.8 4 3 4 4.5 3.5 3.86 ## 6 825 297 3.8 4 3 2.5 3 3.5 3.11 ## 7 825 297 3.8 3.5 3.5 4 3.5 3.5 3.55 ## 8 825 297 3.8 4 3 3 3 3 3.06 ## 9 825 297 3.8 4 4 4 4 4 4 ## 10 825 297 3.8 4 3 3.5 3 3.5 3.21 ## # ℹ 20,349 more rows Besides being easier to write conditions for than indexing with [], select() is code that is much closer to how you or I think about what we’re actually doing, making code that is more human readable. 3.1.2 filter() for rows So select() lets us pick which columns we want. Can we also use it to pick particular observations? No. But for that, there’s filter(). We learned that, using [] indexing, we’d specify a set of rows we want. If we want the first 10 rows of beer_data, we’d write beer_data[1:10, ] ## # A tibble: 10 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 This is a very… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 jisurf… ## 2 Great head cre… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 allbou… ## 3 Wow what a dif… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 frank4… ## 4 Tried this one… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 beergl… ## 5 Caffrey&#39;s on-t… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 proc.1… ## 6 The nitro can … Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… ## 7 Nitro can...Ty… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 adr.263 ## 8 Excellent pour… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 dogbri… ## 9 A very good Ir… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 goindo… ## 10 Looks very nic… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 irishr… ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; Again, this is not very human readable, and if we reorganize our rows this won’t be useful anymore. The tidyverse answer to this approach is the filter() function, which lets you filter your dataset into specific rows according to data stored in the table itself. filter(beer_data, abv &gt; 10) # let&#39;s get some heavy beers ## # A tibble: 571 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 This is a pers… Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 putnam… ## 2 While I a not … Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 proc.1… ## 3 1997. Head is … Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 nerofi… ## 4 Dark amber/bro… Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 john.1… ## 5 A brandy compa… Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 aracau… ## 6 Presentation: … Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 jason.3 ## 7 pours out blac… Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 budgoo… ## 8 I know, I know… Thomas H… 578 O&#39;Hanlon Br… 1533 Old … 11.7 mophie… ## 9 1988 must have… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 maxpow… ## 10 1997 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 suds.3… ## # ℹ 561 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; When using filter(), we can specify multiple logical conditions. For example, let’s get only Barleywines that are more than 10% ABV. If we wanted only exact matches, we could use the direct == operator: filter(beer_data, abv &gt; 10, style == &quot;American Barleywine&quot;) ## # A tibble: 99 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 For me, this i… Olde Deu… 2066 Alley Kat B… 711 Amer… 11 mab.734 ## 2 2000 vintage.T… Beer Line 3048 Lakefront B… 741 Amer… 12.5 bighug… ## 3 A Barleywine m… Beer Line 3048 Lakefront B… 741 Amer… 12.5 zap.184 ## 4 Well, Lagunita… Olde Gna… 1579 Lagunitas B… 220 Amer… 10.6 sinist… ## 5 Bottle reads 1… Olde Gna… 1579 Lagunitas B… 220 Amer… 10.6 bighug… ## 6 Presentation: … AleSmith… 6646 AleSmith Br… 396 Amer… 11 jason.3 ## 7 This is for th… AleSmith… 6646 AleSmith Br… 396 Amer… 11 aracau… ## 8 It&#39;s golden br… AleSmith… 6646 AleSmith Br… 396 Amer… 11 mickey… ## 9 The color is a… John Bar… 20500 Mad River B… 266 Amer… 11.4 beerma… ## 10 Pours a slight… John Bar… 20500 Mad River B… 266 Amer… 11.4 murph.… ## # ℹ 89 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; But this won’t return, for example, any beer labeled as an “English Barleywine”. filter(beer_data, abv &gt; 10, style == &quot;American Barleywine&quot; | style == &quot;English Barleywine&quot;) ## # A tibble: 169 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1988 must have… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 maxpow… ## 2 1997 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 suds.3… ## 3 2000 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 shired… ## 4 [email&amp;#160;pr… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 zap.184 ## 5 1997. Hazy che… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 marc77… ## 6 2000 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 bighug… ## 7 1999. Pours a … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 murph.… ## 8 Bottle reviews… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 bierma… ## 9 I must say my … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 viking… ## 10 For me, this i… Olde Deu… 2066 Alley Kat B… 711 Amer… 11 mab.734 ## # ℹ 159 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; In R, the | means Boolean OR, and the &amp; means Boolean AND. We can use these to combine conditions for searching our data table. But this can be a bit tedious. The stringr package, part of tidyverse, gives a lot of utility functions that we can use instead of this effortful searching. filter(beer_data, abv &gt; 10, str_detect(style, &quot;Barleywine&quot;)) ## # A tibble: 169 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1988 must have… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 maxpow… ## 2 1997 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 suds.3… ## 3 2000 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 shired… ## 4 [email&amp;#160;pr… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 zap.184 ## 5 1997. Hazy che… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 marc77… ## 6 2000 vintage. … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 bighug… ## 7 1999. Pours a … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 murph.… ## 8 Bottle reviews… Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 bierma… ## 9 I must say my … Harvest … 705 J.W. Lees &amp;… 178 Engl… 11.5 viking… ## 10 For me, this i… Olde Deu… 2066 Alley Kat B… 711 Amer… 11 mab.734 ## # ℹ 159 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; Here, the str_detect() function searched for any text that contains “Barleywine” in the style column. 3.2 Combining steps with the pipe: %&gt;% It isn’t hard to imagine a situation in which we want to both select some columns and filter some rows. There are 3 ways we can do this, one of which is going to be the best for most situations. Let’s imagine we want to get only information about the beers, abv, and rating for stouts First, we can nest functions: select(filter(beer_data, str_detect(style, &quot;Stout&quot;)), contains(&quot;beer&quot;), abv, rating) ## # A tibble: 1,666 × 4 ## beer_name beer_id abv rating ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Kinmount Willie Oatmeal Stout 149 4.2 3.49 ## 2 Kinmount Willie Oatmeal Stout 149 4.2 3.92 ## 3 Kinmount Willie Oatmeal Stout 149 4.2 3.43 ## 4 Kinmount Willie Oatmeal Stout 149 4.2 2.84 ## 5 Kinmount Willie Oatmeal Stout 149 4.2 4.13 ## 6 Kinmount Willie Oatmeal Stout 149 4.2 3.95 ## 7 Dragonhead Stout 5781 4 4.43 ## 8 Le Coq Imperial Extra Double Stout 438 10 4.53 ## 9 Le Coq Imperial Extra Double Stout 438 10 4.07 ## 10 Le Coq Imperial Extra Double Stout 438 10 2.93 ## # ℹ 1,656 more rows The problem with this approach is that we have to read it “inside out”. First, filter() will happen and get us only beers that match “Stout” in their style. Then select() will get columns that match “beer” in their names, abv, and rating. Especially as your code gets complicated, this can be very hard to read. So we might take the second approach: creating intermediates. We might first filter(), store that step somewhere, then select(): beer_stouts &lt;- filter(beer_data, str_detect(style, &quot;Stout&quot;)) select(beer_stouts, contains(&quot;beer&quot;), abv, rating) ## # A tibble: 1,666 × 4 ## beer_name beer_id abv rating ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Kinmount Willie Oatmeal Stout 149 4.2 3.49 ## 2 Kinmount Willie Oatmeal Stout 149 4.2 3.92 ## 3 Kinmount Willie Oatmeal Stout 149 4.2 3.43 ## 4 Kinmount Willie Oatmeal Stout 149 4.2 2.84 ## 5 Kinmount Willie Oatmeal Stout 149 4.2 4.13 ## 6 Kinmount Willie Oatmeal Stout 149 4.2 3.95 ## 7 Dragonhead Stout 5781 4 4.43 ## 8 Le Coq Imperial Extra Double Stout 438 10 4.53 ## 9 Le Coq Imperial Extra Double Stout 438 10 4.07 ## 10 Le Coq Imperial Extra Double Stout 438 10 2.93 ## # ℹ 1,656 more rows But now we have this intermediate we don’t really need cluttering up our Environment tab. This is fine for a single step, but if you have a lot of steps in your analysis this is going to get old (and confusing) fast. You’ll have to remove a lot of these using the rm() command to keep your code clean. warning: rm() will permanently delete whatever objects you run it on from your Environment, and you will only be able to restore them by rerunning the code that generated them in the first place. rm(beer_stouts) The final method, and what is becoming standard in modern R coding, is the pipe, which is written in tidyverse as %&gt;%. This garbage-looking set of symbols is actually your best friend, you just don’t know it yet. I use this tool constantly in my R programming, but I’ve been avoiding it up to this point because it’s not part of base R (in fact that’s no longer strictly true, but it is kind of complicated at the moment). OK, enough background, what the heck is a pipe? The term “pipe” comes from what it does: like a pipe, %&gt;% let’s whatever is on it’s left side flow through to the right hand side. It is easiest to read %&gt;% as “AND THEN”. beer_data %&gt;% # Start with the beer_data filter(str_detect(style, &quot;Stout&quot;)) %&gt;% # AND THEN filter to stouts select(contains(&quot;beer&quot;), abv, rating) # AND THEN select the beer columns, etc ## # A tibble: 1,666 × 4 ## beer_name beer_id abv rating ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Kinmount Willie Oatmeal Stout 149 4.2 3.49 ## 2 Kinmount Willie Oatmeal Stout 149 4.2 3.92 ## 3 Kinmount Willie Oatmeal Stout 149 4.2 3.43 ## 4 Kinmount Willie Oatmeal Stout 149 4.2 2.84 ## 5 Kinmount Willie Oatmeal Stout 149 4.2 4.13 ## 6 Kinmount Willie Oatmeal Stout 149 4.2 3.95 ## 7 Dragonhead Stout 5781 4 4.43 ## 8 Le Coq Imperial Extra Double Stout 438 10 4.53 ## 9 Le Coq Imperial Extra Double Stout 438 10 4.07 ## 10 Le Coq Imperial Extra Double Stout 438 10 2.93 ## # ℹ 1,656 more rows In this example, each place there is a %&gt;% I’ve added a comment saying “AND THEN”. This is because that’s exactly what the pipe does: it passes whatever happened in the previous step to the next function. Specifically, %&gt;% passes the results of the previous line to the first argument of the next line. 3.2.1 Pipes require that the lefthand side be a single functional command This means that we can’t directly do something like rewrite sqrt(1 + 2) with %&gt;%: 1 + 2 %&gt;% sqrt # this is instead computing 1 + sqrt(2) ## [1] 2.414214 Instead, if we want to pass binary operationse in a pipe, we need to enclose them in () on the line they are in: (1 + 2) %&gt;% sqrt() # Now this computes sqrt(1 + 2) = sqrt(3) ## [1] 1.732051 More complex piping is possible using the curly braces ({}), which create new R environments, but this is more advanced than you will generally need to be. 3.2.2 Pipes always pass the result of the lefthand side to the first argument of the righthand side This sounds like a weird logic puzzle, but it’s not, as we can see if we look at some simple math. Let’s define a function for use in a pipe that computes the difference between two numbers: subtract &lt;- function(a, b) a - b subtract(5, 4) ## [1] 1 If we want to rewrite that as a pipe, we can write: 5 %&gt;% subtract(4) ## [1] 1 But we can’t write 4 %&gt;% subtract(5) # this is actually making subtract(4, 5) ## [1] -1 We can explicitly force the pipe to work the way we want it to by using . as the placeholder for the result of the lefthand side: 4 %&gt;% subtract(5, .) # now this properly computes subtract(5, 4) ## [1] 1 So, when you’re using pipes, make sure that the output of the lefthand side should be going into the first argument of the righthand side–this is often but not always the case, especially with non-tidyverse functions. 3.2.3 Pipes are a pain to type Typing %&gt;% is no fun. But, happily, RStudio builds in a shortcut for you: macOS is cmd + shift + M, Windows is ctrl + shift + M. 3.3 Make new columns: mutate() You hopefully are starting to be excited by the relative ease of doing some things in R with tidyverse that are otherwise a little bit abstruse. Here’s where I think things get really, really cool. The mutate() function creates a new column in the existing dataset. We can do this easily in base R by setting a new name for a column and using the assign (&lt;-) operator, but this is clumsy. Often, we want to create a new column temporarily, or to combine several existing columns. We can do this using the mutate() function. Let’s say that we want to create a quick categorical variable that tells us whether a beer was rated as more than the central value (2.5) in the 5-pt rating scale. This is kind of like doing a median split, which we’ll get to in a moment. We know that we can use filter() to get just the beers with rating &gt; 2.5: beer_data %&gt;% filter(rating &gt; 2.5) ## # A tibble: 18,782 × 14 ## reviews beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 This is a very… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 jisurf… ## 2 Great head cre… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 allbou… ## 3 Wow what a dif… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 frank4… ## 4 Tried this one… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 beergl… ## 5 Caffrey&#39;s on-t… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 proc.1… ## 6 The nitro can … Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… ## 7 Nitro can...Ty… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 adr.263 ## 8 Excellent pour… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 dogbri… ## 9 A very good Ir… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 goindo… ## 10 Looks very nic… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 irishr… ## # ℹ 18,772 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; But what if we want to be able to just see this? beer_data %&gt;% mutate(better_than_2.5 = rating &gt; 2.5) %&gt;% # We&#39;ll select just a few columns to help us see the result select(beer_name, rating, better_than_2.5) ## # A tibble: 20,359 × 3 ## beer_name rating better_than_2.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Caffrey&#39;s Irish Ale 4.46 TRUE ## 2 Caffrey&#39;s Irish Ale 3.74 TRUE ## 3 Caffrey&#39;s Irish Ale 3.26 TRUE ## 4 Caffrey&#39;s Irish Ale 3.5 TRUE ## 5 Caffrey&#39;s Irish Ale 3.86 TRUE ## 6 Caffrey&#39;s Irish Ale 3.11 TRUE ## 7 Caffrey&#39;s Irish Ale 3.55 TRUE ## 8 Caffrey&#39;s Irish Ale 3.06 TRUE ## 9 Caffrey&#39;s Irish Ale 4 TRUE ## 10 Caffrey&#39;s Irish Ale 3.21 TRUE ## # ℹ 20,349 more rows What does the above function do? mutate() is a very easy way to edit your data mid-pipe. So we might want to do some calculations, create a temporary variable using mutate(), and then continue our pipe. Unless we use &lt;- to store our mutate()d data, the results will be only temporary. We can use the same kind of functional logic we’ve been using in other tidyverse commands in mutate() to get real, powerful results. For example, we might want to know if a beer is rated better than the mean() of the ratings. We can do this easily using mutate(): # Let&#39;s find out the average (mean) rating for these beers beer_data$rating %&gt;% mean() ## [1] 3.719406 # Now, let&#39;s create a column that tells us if a beer is rated &gt; average beer_data %&gt;% mutate(better_than_average = rating &gt; mean(rating)) %&gt;% # Again, let&#39;s select just a few columns select(beer_name, rating, better_than_average) ## # A tibble: 20,359 × 3 ## beer_name rating better_than_average ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Caffrey&#39;s Irish Ale 4.46 TRUE ## 2 Caffrey&#39;s Irish Ale 3.74 TRUE ## 3 Caffrey&#39;s Irish Ale 3.26 FALSE ## 4 Caffrey&#39;s Irish Ale 3.5 FALSE ## 5 Caffrey&#39;s Irish Ale 3.86 TRUE ## 6 Caffrey&#39;s Irish Ale 3.11 FALSE ## 7 Caffrey&#39;s Irish Ale 3.55 FALSE ## 8 Caffrey&#39;s Irish Ale 3.06 FALSE ## 9 Caffrey&#39;s Irish Ale 4 TRUE ## 10 Caffrey&#39;s Irish Ale 3.21 FALSE ## # ℹ 20,349 more rows 3.4 Split-apply-combine analyses with group_by() and summarize() Many basic data analyses can be described as split-apply-combine: split the data into groups, apply some analysis into groups, and then combine the results. For example, in our beer_data we might want to split the data into beer styles, calculate the average overall rating and standard deviation of the rating for each beer style, and the generate a summary table telling us these results. Using the filter() and select() commands we’ve learned so far, you could probably cobble together this analysis without further tools. However, tidyverse provides two powerful tools to do this kind of analysis: The group_by() function takes a data table and groups it by categorical values of any column (generally don’t try to use group_by() on a numeric variable) The summarize() function is like mutate() for groups created with group_by(): First, you specify 1 or more new columns you want to calculate for each group Second, the function produces 1 value for each group for each new column To accomplish the example above, we’d do the following: beer_summary &lt;- beer_data %&gt;% group_by(style) %&gt;% # we will create a group for each unique style summarize(n_beers = n(), # n() counts rows in each style mean_rating = mean(rating), # the mean rating for each style sd_rating = sd(rating), # the standard deviation in rating se_rating = sd(rating) / sqrt(n())) # multiple functions in 1 row beer_summary ## # A tibble: 99 × 5 ## style n_beers mean_rating sd_rating se_rating ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Altbier 186 3.87 0.499 0.0366 ## 2 American Adjunct Lager 961 2.55 0.752 0.0243 ## 3 American Amber / Red Ale 653 3.74 0.563 0.0220 ## 4 American Amber / Red Lager 235 3.29 0.698 0.0455 ## 5 American Barleywine 187 4.25 0.478 0.0349 ## 6 American Black Ale 10 3.98 0.490 0.155 ## 7 American Blonde Ale 251 3.52 0.608 0.0383 ## 8 American Brown Ale 166 3.95 0.503 0.0391 ## 9 American Dark Wheat Ale 11 3.67 0.453 0.136 ## 10 American Double / Imperial IPA 151 4.34 0.479 0.0390 ## # ℹ 89 more rows We can use this approach to even get a summary stats table - for example, confidence limits according to the normal distribution: beer_summary %&gt;% mutate(lower_limit = mean_rating - 1.96 * se_rating, upper_limit = mean_rating + 1.96 * se_rating) ## # A tibble: 99 × 7 ## style n_beers mean_rating sd_rating se_rating lower_limit upper_limit ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Altbier 186 3.87 0.499 0.0366 3.79 3.94 ## 2 American Adj… 961 2.55 0.752 0.0243 2.50 2.60 ## 3 American Amb… 653 3.74 0.563 0.0220 3.70 3.79 ## 4 American Amb… 235 3.29 0.698 0.0455 3.20 3.38 ## 5 American Bar… 187 4.25 0.478 0.0349 4.19 4.32 ## 6 American Bla… 10 3.98 0.490 0.155 3.67 4.28 ## 7 American Blo… 251 3.52 0.608 0.0383 3.45 3.60 ## 8 American Bro… 166 3.95 0.503 0.0391 3.88 4.03 ## 9 American Dar… 11 3.67 0.453 0.136 3.40 3.94 ## 10 American Dou… 151 4.34 0.479 0.0390 4.27 4.42 ## # ℹ 89 more rows Note that in the above example we use mutate(), not summarize(), because we had saved our summarized data. We could also have calculated lower_limit and upper_limit directly as part of the summarize() statement if we hadn’t saved the intermediate. 3.5 Utilities for data management Honestly, the amount of power in tidyverse is way more than we can cover today, and is covered more comprehensively (obviously) by Wickham and Grolemund. However, I want to name 4 more utilities we will make a lot of use of today (and you will want to know about for your own work). 3.5.1 Rename your columns Often you will import data with bad column names or you’ll realize you need to rename variables during your workflow. For this, you can use the rename() function: names(beer_data) ## [1] &quot;reviews&quot; &quot;beer_name&quot; &quot;beer_id&quot; &quot;brewery_name&quot; &quot;brewery_id&quot; ## [6] &quot;style&quot; &quot;abv&quot; &quot;user_id&quot; &quot;appearance&quot; &quot;aroma&quot; ## [11] &quot;palate&quot; &quot;taste&quot; &quot;overall&quot; &quot;rating&quot; beer_data %&gt;% rename(review_text = reviews) ## # A tibble: 20,359 × 14 ## review_text beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 This is a very… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 jisurf… ## 2 Great head cre… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 allbou… ## 3 Wow what a dif… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 frank4… ## 4 Tried this one… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 beergl… ## 5 Caffrey&#39;s on-t… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 proc.1… ## 6 The nitro can … Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… ## 7 Nitro can...Ty… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 adr.263 ## 8 Excellent pour… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 dogbri… ## 9 A very good Ir… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 goindo… ## 10 Looks very nic… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 irishr… ## # ℹ 20,349 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; You can also rename by position, which is helpful for quick changes: beer_data %&gt;% rename(review_text = 1) ## # A tibble: 20,359 × 14 ## review_text beer_name beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 This is a very… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 jisurf… ## 2 Great head cre… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 allbou… ## 3 Wow what a dif… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 frank4… ## 4 Tried this one… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 beergl… ## 5 Caffrey&#39;s on-t… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 proc.1… ## 6 The nitro can … Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… ## 7 Nitro can...Ty… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 adr.263 ## 8 Excellent pour… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 dogbri… ## 9 A very good Ir… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 goindo… ## 10 Looks very nic… Caffrey&#39;… 825 Thomas Caff… 297 Iris… 3.8 irishr… ## # ℹ 20,349 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; 3.5.2 Relocate your columns If you mutate() columns or just have a big data set with a lot of variables, often you want to move columns around. This is a pain to do with [], but again tidyverse has a utility to move things around easily: relocate(). beer_data %&gt;% relocate(beer_name) # giving no other arguments will move to front ## # A tibble: 20,359 × 14 ## beer_name reviews beer_id brewery_name brewery_id style abv user_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Caffrey&#39;s Irish … This i… 825 Thomas Caff… 297 Iris… 3.8 jisurf… ## 2 Caffrey&#39;s Irish … Great … 825 Thomas Caff… 297 Iris… 3.8 allbou… ## 3 Caffrey&#39;s Irish … Wow wh… 825 Thomas Caff… 297 Iris… 3.8 frank4… ## 4 Caffrey&#39;s Irish … Tried … 825 Thomas Caff… 297 Iris… 3.8 beergl… ## 5 Caffrey&#39;s Irish … Caffre… 825 Thomas Caff… 297 Iris… 3.8 proc.1… ## 6 Caffrey&#39;s Irish … The ni… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… ## 7 Caffrey&#39;s Irish … Nitro … 825 Thomas Caff… 297 Iris… 3.8 adr.263 ## 8 Caffrey&#39;s Irish … Excell… 825 Thomas Caff… 297 Iris… 3.8 dogbri… ## 9 Caffrey&#39;s Irish … A very… 825 Thomas Caff… 297 Iris… 3.8 goindo… ## 10 Caffrey&#39;s Irish … Looks … 825 Thomas Caff… 297 Iris… 3.8 irishr… ## # ℹ 20,349 more rows ## # ℹ 6 more variables: appearance &lt;dbl&gt;, aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, ## # overall &lt;dbl&gt;, rating &lt;dbl&gt; You can also use relocate() to specify positions beer_data %&gt;% relocate(reviews, .after = rating) # move the long text to the end ## # A tibble: 20,359 × 14 ## beer_name beer_id brewery_name brewery_id style abv user_id appearance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 jisurf… 5 ## 2 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 allbou… 5 ## 3 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 frank4… 4 ## 4 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 beergl… 3.5 ## 5 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 proc.1… 4 ## 6 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 zerk.4… 4 ## 7 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 adr.263 3.5 ## 8 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 dogbri… 4 ## 9 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 goindo… 4 ## 10 Caffrey&#39;s Iri… 825 Thomas Caff… 297 Iris… 3.8 irishr… 4 ## # ℹ 20,349 more rows ## # ℹ 6 more variables: aroma &lt;dbl&gt;, palate &lt;dbl&gt;, taste &lt;dbl&gt;, overall &lt;dbl&gt;, ## # rating &lt;dbl&gt;, reviews &lt;chr&gt; 3.5.3 Sort your data More frequently, we will want to rearrange our rows, which can be done with arrange(). All you have to do is give arrange() one or more columns to sort the data by. You can use either the desc() or the - shortcut to sort in reverse order. beer_data %&gt;% arrange(desc(rating)) %&gt;% # what are the highest rated beers? select(beer_name, rating) ## # A tibble: 20,359 × 2 ## beer_name rating ## &lt;chr&gt; &lt;dbl&gt; ## 1 McEwan&#39;s India Pale Ale 5 ## 2 Samuel Smith&#39;s Imperial Stout 5 ## 3 Samuel Smith&#39;s Imperial Stout 5 ## 4 Samuel Smith&#39;s Imperial Stout 5 ## 5 Samuel Smith&#39;s Oatmeal Stout 5 ## 6 Samuel Smith&#39;s Oatmeal Stout 5 ## 7 Samuel Smith&#39;s Oatmeal Stout 5 ## 8 Young&#39;s Ram Rod 5 ## 9 Fuller&#39;s 1845 5 ## 10 Mackeson Triple XXX Stout 5 ## # ℹ 20,349 more rows You can sort alphabetically as well: beer_data %&gt;% arrange(brewery_name, beer_name) %&gt;% # get beers sorted within breweries select(brewery_name, beer_name) %&gt;% # show only relevant columns distinct() # discard duplicate rows ## # A tibble: 4,734 × 2 ## brewery_name beer_name ## &lt;chr&gt; &lt;chr&gt; ## 1 21st Amendment Brewery Hell Or High Watermelon Wheat Beer ## 2 3 Floyds Brewing Co. Alpha King ## 3 3 Floyds Brewing Co. Alpha Klaus Christmas (Xmas) Porter ## 4 3 Floyds Brewing Co. Behemoth Blonde Barleywine ## 5 3 Floyds Brewing Co. Black Sun Stout ## 6 3 Floyds Brewing Co. Burnham Pilsner ## 7 3 Floyds Brewing Co. Calumet Queen ## 8 3 Floyds Brewing Co. Dark Lord Imperial Stout ## 9 3 Floyds Brewing Co. Dreadnaught IPA ## 10 3 Floyds Brewing Co. Extra Pale Ale ## # ℹ 4,724 more rows 3.5.4 Pivot tables Users of Excel may be familiar with the idea of pivot tables. These are functions that let us make our data tidier. To quote Wickham and Grolemund: here are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. While these authors present “tidiness” of data as an objective property, I’d argue that data is always tidy for a specific purpose. For example, our data is relatively tidy here, except our numerical ratings: we have 6 different ratings for each beer, which means we have encoded an implicit variable in the column names: rating type. If we want to use our data for summarization, the form we have is fine. But if we want to make plots and do some other modeling, this form may be no good to us. We can use the pivot_longer() function to change our data to make the implicit variable explicit and to make our data tidier. beer_data %&gt;% select(beer_name, user_id, appearance:rating) %&gt;% # for clarity pivot_longer(cols = appearance:rating, names_to = &quot;rating_type&quot;, values_to = &quot;rating&quot;) ## # A tibble: 122,154 × 4 ## beer_name user_id rating_type rating ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Caffrey&#39;s Irish Ale jisurfer.1152 appearance 5 ## 2 Caffrey&#39;s Irish Ale jisurfer.1152 aroma 4 ## 3 Caffrey&#39;s Irish Ale jisurfer.1152 palate 4 ## 4 Caffrey&#39;s Irish Ale jisurfer.1152 taste 4.5 ## 5 Caffrey&#39;s Irish Ale jisurfer.1152 overall 5 ## 6 Caffrey&#39;s Irish Ale jisurfer.1152 rating 4.46 ## 7 Caffrey&#39;s Irish Ale allboutbierge.746 appearance 5 ## 8 Caffrey&#39;s Irish Ale allboutbierge.746 aroma 3.5 ## 9 Caffrey&#39;s Irish Ale allboutbierge.746 palate 4 ## 10 Caffrey&#39;s Irish Ale allboutbierge.746 taste 3.5 ## # ℹ 122,144 more rows Now for each unique combination of beer_name and user_id, we have 6 rows, one for each type of rating they can generate. Sometimes we want to have “wider” or “untidy” data. We can use pivot_wider() to reverse the effects of pivot_longer(). While the ideas of pivoting can seem simple, they are both powerful and subtly confusing. We’ll be using these tools throughout the rest of the tutorial, so I wanted to give exposure, but mastering them takes trial and error. I recommend taking a look at the relevant chapter in Wickham and Grolemund for details. "],["data-visualization-basics-with-ggplot2.html", "4 Data visualization basics with ggplot2 4.1 Your first ggplot() 4.2 The aes() function and mapping = argument 4.3 Adding layers with geom_*() functions 4.4 Arguments inside and outside of aes() 4.5 Some further reading", " 4 Data visualization basics with ggplot2 R includes extremely powerful utilities for data visualization, but most modern applications make use of the tidyverse package ggplot2. A quick word about base R plotting–I don’t mean to declare that you can’t use base R plotting for your projects at all, and I have published several papers using base R plots. Particularly as you are using R for your own data exploration (not meant for sharing outside your team, say), base utilities like plot() will be very useful for quick insight. ggplot2 provides a standardized, programmatic interface for data visualization, in contrast to the piecemeal approach common to base R graphics plotting. This means that, while the syntax itself can be challenging to learn, syntax for different tasks differs in logical and predictable ways and, together with other tidyverse principles (like select() and filter() approaches), ggplot2 makes it easy to make publication-quality visualizations with relative ease. In general, ggplot2 works best with data in “long” or “tidy” format, such as that resulting from the output of pivot_longer(). The The schematic elements of a ggplot are as follows: # The ggplot() function creates your plotting environment. We usually save it to a variable in R so that we can use the plug-n-play functionality of ggplot without retyping a bunch of nonsense p &lt;- ggplot(mapping = aes(x = &lt;a variable&gt;, y = &lt;another variable&gt;, ...), data = &lt;your data&gt;) # Then, you can add various ways of plotting data to make different visualizations. p + geom_&lt;your chosen way of plotting&gt;(...) + theme_&lt;your chosen theme&gt; + ... In graphical form, the following diagram (from VT Professor JP Gannon) gives an intuition of what is happening: Basic ggplot mappings. Color boxes indicate where the elements go in the function and in the plot. 4.1 Your first ggplot() Our beer data is already relatively tidy, so we will begin by making an example ggplot() to demonstrate how it works. beer_data %&gt;% ggplot(mapping = aes(x = rating, y = overall)) + # Here we set up the base plot geom_point() # Here we tell our base plot to add points This doesn’t look all that impressive–partly because the data being plotted itself isn’t that sensible, and partly because we haven’t made many changes. But before we start looking into that, let’s break down the parts of this command. 4.2 The aes() function and mapping = argument The ggplot() function takes two arguments that are essential, as well as some others you’ll rarely use. The first, data =, is straightforward, and you’ll usually be passing data to the function at the end of some pipeline using %&gt;% The second, mapping =, is less clear. This argument requires the aes() function, which can be read as the “aesthetic” function. The way that this function works is quite complex, and really not worth digging into here, but I understand it in my head as telling ggplot() what part of my data is going to connect to what part of the plot. So, if we write aes(x = rating), we can read this in our heads as “the values of x will be mapped from the ‘rating’ column”. This sentence tells us the other important thing about ggplot() and the aes() mappings: mapped variables each have to be in their own column. This is another reason that ggplot() requires tidy data. 4.3 Adding layers with geom_*() functions In the above example, we added (literally, using +) a function called geom_point() to the base ggplot() call. This is functionally a “layer” of our plot, that tells ggplot2 how to actually visualize the elements specified in the aes() function–in the case of geom_point(), we create a point for each row’s combination of x = rating and y = overall. beer_data %&gt;% select(rating, overall) ## # A tibble: 20,359 × 2 ## rating overall ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.46 5 ## 2 3.74 4 ## 3 3.26 3 ## 4 3.5 3.5 ## 5 3.86 3.5 ## 6 3.11 3.5 ## 7 3.55 3.5 ## 8 3.06 3 ## 9 4 4 ## 10 3.21 3.5 ## # ℹ 20,349 more rows There are many geom_*() functions in ggplot2, and many others defined in other accessory packages. These are the heart of visualizations. We can swap them out to get different results: beer_data %&gt;% ggplot(mapping = aes(x = rating, y = overall)) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; (#fig:changing the geom changes the way the data map)switching geom_() switches the way the data map Here we fit a smoothed line to our data using the default methods in geom_smooth() (which in this case heuristically defaults to a General Additive Model). We can also combine layers, as the term implies: beer_data %&gt;% ggplot(mapping = aes(x = rating, y = overall)) + geom_jitter() + # add some random noise to show overlapping points geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; (#fig:geoms are layers in a plot)geom_()s are layers in a plot Note that we don’t need to tell either geom_smooth() or geom_jitter() what x and y are–they “inherit” them from the ggplot() function to which they are added (+), which defines the plot itself. What other arguments can be set to aesthetics? Well, we can set other visual properties like color, size, transparency (called “alpha”), and so on. For example, let’s try to look at whether there is a relationship between ABV and perceived quality. beer_data %&gt;% # mutate a new variable for plotting mutate(high_abv = ifelse(abv &gt; 9, &quot;yes&quot;, &quot;no&quot;)) %&gt;% drop_na(high_abv) %&gt;% ggplot(mapping = aes(x = rating, y = overall, color = high_abv)) + geom_jitter(alpha = 1/4) + scale_color_viridis_d() + theme_bw() We can see that most of the yellow “yes” dots are in the top right of the figure–people rate more alcoholic beer as higher quality for both overall and rating. 4.4 Arguments inside and outside of aes() In the last plot, we saw an example in the geom_jitter(alpha = 1/4) function of setting the alpha (transparency) aesthetic element directly, without using aes() to map a variable to this aesthetic. That is why this is not wrapped in the aes() function. In ggplot2, this is how we set aesthetics to fixed values. Alternatively, we could have mapped this to a variable, just like color: beer_data %&gt;% drop_na(abv) %&gt;% ggplot(aes(x = rating, y = overall)) + # We can set new aes() mappings in individual layers, as well as the plot itself geom_jitter(aes(alpha = abv)) + theme_bw() (#fig:using the aes function)using the aes() function As an aside, we can see the same relationship noted above: higher ABV is associated with higher ratings. 4.4.1 Using theme_*() to change visual options quickly In the last several plots, notice that we have changed from the default (and to my mind unattractive) grey background of ggplot2 to a black and white theme. This is by adding a theme_bw() call to the list of commands. ggplot2 includes a number of default theme_*() functions, and you can get many more through other R packages. They can have subtle to dramatic effects: beer_data %&gt;% drop_na() %&gt;% ggplot(aes(x = rating, y = overall)) + geom_jitter() + theme_void() (#fig:using the theme functions)using the theme_*() functions You can also edit every last element of the plot’s theme using the base theme() function, which is powerful but a little bit tricky to use. 4.4.2 Changing aesthetic elements with scale_*() functions Finally, say we didn’t like the default color set for the points. How can we manipulate the colors that are plotted? The way in which mapped, aesthetic variables are assigned to visual elements is controlled by the scale_*() functions. In my experience, the most frequently encountered scales are those for color: either scale_fill_*() for solid objects (like the bars in a histogram) or scale_color_*() for lines and points (like the outlines of the histogram bars). Scale functions work by telling ggplot() how to map aesthetic variables to visual elements. You may have noticed that I added a scale_color_viridis_d() function to the end of the ABV plot. This function uses the viridis package, which has color-blind and (theoretically) print-safe color scales. p &lt;- beer_data %&gt;% # This block gets us a subset of beer styles for clear visualization group_by(style) %&gt;% nest(data = -style) %&gt;% ungroup() %&gt;% slice_sample(n = 10) %&gt;% unnest(everything()) %&gt;% # And now we can go back to plotting ggplot(aes(x = rating, group = style)) + # Density plots are smoothed histograms geom_density(aes(fill = style), alpha = 1/4, color = NA) + theme_bw() p We can take a saved plot (like p) and use scales to change how it is visualized. p + scale_fill_viridis_d() ggplot2 has a broad range of built-in options for scales, but there are many others available in add-on packages that build on top of it. You can also build your own scales using the scale_*_manual() functions, in which you give a vector of the same length as your mapped aesthetic variable in order to set up the visual assignment. That sounds jargon-y, so here is an example: # We&#39;ll pick 14 random colors from the colors R knows about random_colors &lt;- print(colors()[sample(x = 1:length(colors()), size = 10)]) ## [1] &quot;chartreuse2&quot; &quot;peru&quot; &quot;lightcyan4&quot; &quot;coral4&quot; &quot;grey81&quot; ## [6] &quot;mediumpurple&quot; &quot;grey93&quot; &quot;orchid&quot; &quot;gray36&quot; &quot;slategray1&quot; p + scale_fill_manual(values = random_colors) 4.4.3 Finally, facet_*() The last powerful tool I want to show off is the ability of ggplot2 to make whatEdward Tufte called “small multiples”: breaking out the data into multiple, identical plots by some categorical classifier in order to show trends more effectively. So far we’ve seen how to visualize ratings in our beer data by ABV and by style. We could combine these in one plot by assigning ABV to one aesthetic, style to another, and so on. But our plots might get messy. Instead, let’s see how we can break out, for example, different ABVs into different, “small multiple” facet plots to get a look at trends in liking. A plausible sensory hypothesis is that the palate variable in particular will change by ABV. So we are going to take a couple steps here: We will use mutate() to split abv into low, medium, and high (using tertile splits) We will plot the relationship of palate to ABV as a density plot We will then look at this as a single plot vs a facetted plot p &lt;- beer_data %&gt;% # Step 1: make abv tertiles mutate(abv_tertile = as.factor(ntile(abv, 3))) %&gt;% # Step 2: plot ggplot(aes(x = palate, group = abv_tertile)) + geom_density(aes(fill = abv_tertile), alpha = 1/4, color = NA, adjust = 3) + theme_classic() # Unfacetted plot p It looks like the expected trend is present, but it’s a bit hard to see. Let’s see what happens if we break this out into “facets”: p + facet_wrap(~abv_tertile, nrow = 4) + theme(legend.position = &quot;none&quot;) (#fig:splitting the plot into 3 small multiples)now we split the plot into 3 “small multiples” with facet_wrap() By splitting into facets we can see that there is much more density towards the higher palate ratings for the higher abv tertiles. This may help explain the positive relationship between abv and rating in the overall dataset: the consumers in this sample clearly appreciate the mouthfeel associated with higher alcohol contents. 4.5 Some further reading This has been a lightning tour of ggplot2 as preparatory material for our core material on text analysis; it barely scratches the surface. If you’re interested in learning more, I recommend taking a look at the following sources: Kieran Healy’s “Data Visualization: a Practical Introduction”. The plotting section of R for Data Science. Hadley Wickham’s core reference textbook on ggplot2. "],["text-analysis.html", "5 Text Analysis 5.1 Text Analysis Pt. I: Getting your text data 5.2 Text Analysis Pt. II: Basic text analysis approaches using tokens 5.3 Text Analysis Pt. III: Sentiment analysis 5.4 Text Analysis Pt. IV: n-gram models", " 5 Text Analysis At this point we’ve gone over the fundamentals of data wrangling and analysis in R. We really haven’t touched on statistics and modeling, as that is outside the scope of this workshop, but I will be glad to chat about those with anyone outside of the main workshop if you have questions. Now we’re going to turn to our key focus: text analysis in R. We are going to focus on 3 broad areas: Importing, cleaning, and wrangling text data and dealing with character type data. Basic text modeling using the TF-IDF framework Application: Sentiment Analysis It’s not a bad idea to restart your R session here. Make sure to save your work, but a clean Environment is great when we’re shifting topics. You can accomplish this by going to Session &gt; Restart R in the menu. Then, we want to make sure to re-load our packages and import our data. # The packages we&#39;re using library(tidyverse) library(tidytext) # The dataset beer_data &lt;- read_csv(&quot;data/ba_2002.csv&quot;) ## Rows: 20359 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): reviews, beer_name, brewery_name, style, user_id ## dbl (9): beer_id, brewery_id, abv, appearance, aroma, palate, taste, overall... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 5.1 Text Analysis Pt. I: Getting your text data There are many sources of text data, which is one of the strengths of this approach. Some quick and obvious examples include: Open-comment responses on a survey (sensory study in lab, remote survey, etc) Comment cards or customer responses Qualitative data (transcripts from interviews and focus groups) Compiled/database from previous studies Scraped data from websites and/or social media The last (#5) is often the most interesting data source, and there are a number of papers in recent years that have discussed the use of this sort of data. I am not going to discuss how to obtain text data in this presentation, because it is its own, rich topic. However, you can review my presentation and tutorial from Eurosense 2020 if you want a quick crash course on web scraping for sensory evaluation. 5.1.1 Basic import and cleaning with text data So far we’ve focused our attention on the beer_data data set on the various (numeric) rating variables. But the data in the reviews column is clearly more rich. set.seed(2) reviews_example &lt;- beer_data %&gt;% slice_sample(n = 5) reviews_example %&gt;% select(beer_name, reviews) %&gt;% gt::gt() #qlirkacvpr table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #qlirkacvpr thead, #qlirkacvpr tbody, #qlirkacvpr tfoot, #qlirkacvpr tr, #qlirkacvpr td, #qlirkacvpr th { border-style: none; } #qlirkacvpr p { margin: 0; padding: 0; } #qlirkacvpr .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qlirkacvpr .gt_caption { padding-top: 4px; padding-bottom: 4px; } #qlirkacvpr .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qlirkacvpr .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #qlirkacvpr .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qlirkacvpr .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qlirkacvpr .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qlirkacvpr .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qlirkacvpr .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qlirkacvpr .gt_column_spanner_outer:first-child { padding-left: 0; } #qlirkacvpr .gt_column_spanner_outer:last-child { padding-right: 0; } #qlirkacvpr .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qlirkacvpr .gt_spanner_row { border-bottom-style: hidden; } #qlirkacvpr .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #qlirkacvpr .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qlirkacvpr .gt_from_md > :first-child { margin-top: 0; } #qlirkacvpr .gt_from_md > :last-child { margin-bottom: 0; } #qlirkacvpr .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qlirkacvpr .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #qlirkacvpr .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #qlirkacvpr .gt_row_group_first td { border-top-width: 2px; } #qlirkacvpr .gt_row_group_first th { border-top-width: 2px; } #qlirkacvpr .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qlirkacvpr .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #qlirkacvpr .gt_first_summary_row.thick { border-top-width: 2px; } #qlirkacvpr .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qlirkacvpr .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qlirkacvpr .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qlirkacvpr .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #qlirkacvpr .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qlirkacvpr .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qlirkacvpr .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qlirkacvpr .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qlirkacvpr .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qlirkacvpr .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qlirkacvpr .gt_left { text-align: left; } #qlirkacvpr .gt_center { text-align: center; } #qlirkacvpr .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qlirkacvpr .gt_font_normal { font-weight: normal; } #qlirkacvpr .gt_font_bold { font-weight: bold; } #qlirkacvpr .gt_font_italic { font-style: italic; } #qlirkacvpr .gt_super { font-size: 65%; } #qlirkacvpr .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #qlirkacvpr .gt_asterisk { font-size: 100%; vertical-align: 0; } #qlirkacvpr .gt_indent_1 { text-indent: 5px; } #qlirkacvpr .gt_indent_2 { text-indent: 10px; } #qlirkacvpr .gt_indent_3 { text-indent: 15px; } #qlirkacvpr .gt_indent_4 { text-indent: 20px; } #qlirkacvpr .gt_indent_5 { text-indent: 25px; } beer_name reviews Stoudt's Weizen Medium straw and cloudy with thick head and lace. Fresh smelling with slight lemon zest aroma. Taste is rich and fulfilling with strong lemon flavor. Also notable is the strong, almost overwhealming clove flavor. Fresh and thirst quenching. Goes great with brats at a German festival. Black Bear Ale (3-31-03): Got a new bottle thanks to johnrobe, and unfortunately it was just as bad. This brewery either has ongoing issues or the beer is really supposed to taste this way. I am guessing it is the former. (12-09-02): Acquired via trade. This may be a good beer and I may have gotten a bad bottle but the vinegar aroma ruined this beer. I was given pause before I even had a chance to take a sip because the vinegar aroma is so potent. Plugging my nose and drinking did not help much as the vinegar was prevalent in the flavor as well. I will give this one another shot down the road but I had to pour this one out. Kozel A truly good, authentic Pilsner, bordering on great. This is the type of Pilsner Beck's or Labatts (and most North American brewers) wished they could produce. They can't, this brew took time. Brew time and cellar time. The quality of ingredients and process are there for the palate to enjoy on this Pilsner. Soft barley and malt tastes balanced with those heavenly saaz bitters. A great example of Czech pilsen style lagers. This one has a distinguishing quality though. It has everything required of a good Pilsner, flowery hopping evident to the nose and palate, sharp crisp start, tart/malty/grassy in the mouth dry finish except, this one has no lingering &amp;quot;tinny&amp;quot; taste to the tongue from the saaz hops....probably not pasteurized. I liked this Pilsner very much and that takes something for this fanatical &amp;quot;Pils head&amp;quot;. The only draw back is the price. $2.85 per 500mil bottle. I was surprised this beer was so fresh tasting, date code was near expired and it sat unrefrigerated on the LCBO (our state liquor monopoly) shelf. A must try if you like Pilsners. Odell Cutthroat Pale Ale This beer pours with a deep thick copper color and a thick foamy white head. The huge spicy aroma has just a hint of malt. The smooth medium body initiates an up-front caramel malt sweetness followed by a sharp spicy bitter bite. Very tasty. Pike XXXXX Stout Nice pour, deep opaque black with an easy, thick creamy head that clings to the sides of the glass. It's not exactly long-lasting, but it never fully fades (ends up thin cream layer that's too thick for a film, but kind of like a like coat of moss or lichens). The aroma is a little weak, but roasty. The flavor is as strong as it should be but it dominated and unbalanced by too much burned bitterness. A bit of smokiness lingers underneath. The lack of balance makes this less than a joy to drink, but intriguing at the same time. However, this has some classic features of scraped text data–notice the “&amp;quot”, for example–this is HTML placeholder for a quotation mark. Other common problems that you will see with text data include “parsing errors” between different forms of character data–the most common is between ASCII and UTF-8 standards. You will of course also encounter errors with typos, and perhaps idiosyncratic formatting characteristic of different text sources: for example, the use of @usernames and #hashtags in Twitter data. We will use the textclean package to do some basic clean up, but this kind of deep data cleaning may take more steps and will change from project to project. library(textclean) cleaned_reviews_example &lt;- reviews_example %&gt;% mutate(cleaned_review = reviews %&gt;% # Fix some of the HTML placeholders replace_html(symbol = FALSE) %&gt;% # Replace sequences of &quot;...&quot; replace_incomplete(replacement = &quot; &quot;) %&gt;% # Replace less-common or misformatted HTML str_replace_all(&quot;#.{1,4};&quot;, &quot; &quot;) %&gt;% # Replace some common non-A-Z, 1-9 symbols replace_symbol() %&gt;% # Remove non-word text like &quot;:)&quot; replace_emoticon() %&gt;% # Remove numbers from the reviews, as they are not useful str_remove_all(&quot;[0-9]&quot;)) cleaned_reviews_example %&gt;% select(beer_name, cleaned_review) %&gt;% gt::gt() #swhgavipdk table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #swhgavipdk thead, #swhgavipdk tbody, #swhgavipdk tfoot, #swhgavipdk tr, #swhgavipdk td, #swhgavipdk th { border-style: none; } #swhgavipdk p { margin: 0; padding: 0; } #swhgavipdk .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #swhgavipdk .gt_caption { padding-top: 4px; padding-bottom: 4px; } #swhgavipdk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #swhgavipdk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #swhgavipdk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #swhgavipdk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #swhgavipdk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #swhgavipdk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #swhgavipdk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #swhgavipdk .gt_column_spanner_outer:first-child { padding-left: 0; } #swhgavipdk .gt_column_spanner_outer:last-child { padding-right: 0; } #swhgavipdk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #swhgavipdk .gt_spanner_row { border-bottom-style: hidden; } #swhgavipdk .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #swhgavipdk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #swhgavipdk .gt_from_md > :first-child { margin-top: 0; } #swhgavipdk .gt_from_md > :last-child { margin-bottom: 0; } #swhgavipdk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #swhgavipdk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #swhgavipdk .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #swhgavipdk .gt_row_group_first td { border-top-width: 2px; } #swhgavipdk .gt_row_group_first th { border-top-width: 2px; } #swhgavipdk .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #swhgavipdk .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #swhgavipdk .gt_first_summary_row.thick { border-top-width: 2px; } #swhgavipdk .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #swhgavipdk .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #swhgavipdk .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #swhgavipdk .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #swhgavipdk .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #swhgavipdk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #swhgavipdk .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #swhgavipdk .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #swhgavipdk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #swhgavipdk .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #swhgavipdk .gt_left { text-align: left; } #swhgavipdk .gt_center { text-align: center; } #swhgavipdk .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #swhgavipdk .gt_font_normal { font-weight: normal; } #swhgavipdk .gt_font_bold { font-weight: bold; } #swhgavipdk .gt_font_italic { font-style: italic; } #swhgavipdk .gt_super { font-size: 65%; } #swhgavipdk .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #swhgavipdk .gt_asterisk { font-size: 100%; vertical-align: 0; } #swhgavipdk .gt_indent_1 { text-indent: 5px; } #swhgavipdk .gt_indent_2 { text-indent: 10px; } #swhgavipdk .gt_indent_3 { text-indent: 15px; } #swhgavipdk .gt_indent_4 { text-indent: 20px; } #swhgavipdk .gt_indent_5 { text-indent: 25px; } beer_name cleaned_review Stoudt's Weizen Medium straw and cloudy with thick head and lace. Fresh smelling with slight lemon zest aroma. Taste is rich and fulfilling with strong lemon flavor. Also notable is the strong, almost overwhealming clove flavor. Fresh and thirst quenching. Goes great with brats at a German festival. Black Bear Ale (--): Got a new bottle thanks to johnrobe, and unfortunately it was just as bad. This brewery either has ongoing issues or the beer is really supposed to taste this way. I am guessing it is the former. (--): Acquired via trade. This may be a good beer and I may have gotten a bad bottle but the vinegar aroma ruined this beer. I was given pause before I even had a chance to take a sip because the vinegar aroma is so potent. Plugging my nose and drinking did not help much as the vinegar was prevalent in the flavor as well. I will give this one another shot down the road but I had to pour this one out. Kozel A truly good, authentic Pilsner, bordering on great. This is the type of Pilsner Beck's or Labatts (and most North American brewers) wished they could produce. They can't, this brew took time. Brew time and cellar time. The quality of ingredients and process are there for the palate to enjoy on this Pilsner. Soft barley and malt tastes balanced with those heavenly saaz bitters. A great example of Czech pilsen style lagers. This one has a distinguishing quality though. It has everything required of a good Pilsner, flowery hopping evident to the nose and palate, sharp crisp start, tart/malty/grassy in the mouth dry finish except, this one has no lingering tinny taste to the tongue from the saaz hops probably not pasteurized. I liked this Pilsner very much and that takes something for this fanatical Pils head . The only draw back is the price. dollar . per mil bottle. I was surprised this beer was so fresh tasting, date code was near e tongue sticking out ired and it sat unrefrigerated on the LCBO (our state liquor monopoly) shelf. A must try if you like Pilsners. Odell Cutthroat Pale Ale This beer pours with a deep thick copper color and a thick foamy white head. The huge spicy aroma has just a hint of malt. The smooth medium body initiates an up-front caramel malt sweetness followed by a sharp spicy bitter bite. Very tasty. Pike XXXXX Stout Nice pour, deep opaque black with an easy, thick creamy head that clings to the sides of the glass. It's not exactly long-lasting, but it never fully fades (ends up thin cream layer that's too thick for a film, but kind of like a like coat of moss or lichens). The aroma is a little weak, but roasty. The flavor is as strong as it should be but it dominated and unbalanced by too much burned bitterness. A bit of smokiness lingers underneath. The lack of balance makes this less than a joy to drink, but intriguing at the same time. This will take slightly longer to run on the full data set, but should improve our overall data quality. beer_data &lt;- beer_data %&gt;% mutate(cleaned_review = reviews %&gt;% replace_html(symbol = FALSE) %&gt;% replace_incomplete(replacement = &quot; &quot;) %&gt;% str_replace_all(&quot;#.{1,4};&quot;, &quot; &quot;) %&gt;% replace_symbol() %&gt;% replace_emoticon() %&gt;% str_remove_all(&quot;[0-9]&quot;)) Again, this is not an exhaustive cleaning step–rather, these are some basic steps that I took based on common parsing errors I observed. Each particular text dataset will come with its own challenges and require a bit of time to develop the cleaning step. 5.1.2 Where is the data in character? A tidy approach. As humans who understand English, we can see that meaning is easily found from these reviews. For example, we can guess that the rating of Black Bear Ale will be a low number (negative), while the rating of Kozel will be much more positive. And, indeed, this is the case: cleaned_reviews_example %&gt;% select(beer_name, appearance:rating) ## # A tibble: 5 × 7 ## beer_name appearance aroma palate taste overall rating ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Stoudt&#39;s Weizen 4 3.5 4 4 4 3.88 ## 2 Black Bear Ale 3 1 1 1.5 1.5 1.42 ## 3 Kozel 4 4 4.5 4.5 4 4.25 ## 4 Odell Cutthroat Pale Ale 4 4 4 4 4 4 ## 5 Pike XXXXX Stout 4.5 3 4 3.5 2.5 3.29 But what part of the structure of the reviews text actually tells us this? This is a complicated topic that is well beyond the scope of this workshop–we are going to propose a single approach based on tokenization that is effective, but is certainly not exhaustive. If you are interested in a broader view of approaches to text analysis, I recommend the seminal textbook from Jurafsky and Martin, Speech and Language Processing, especially chapters 2-6. The draft version is freely available on the web as of the date of this workshop. In the first half of this workshop we reviewed the tidyverse approach to data, in which we emphasized an approach to data in which: Each variable is a column Each observation is a row Each type of observational unit is a table This type of approach can be applied to text data if we can specify a way to standardize the “observations” within the text. We will be applying and demonstrating the approach defined and promoted by Silge and Robinson in Text Mining with R, in which we will focus on the following syllogism to create tidy text data: observation == token A token is a meaningful unit of text, usually but not always a single word, which will be the observation for us. In our example data, let’s identify some possible tokens: cleaned_reviews_example$cleaned_review[1] ## [1] &quot;Medium straw and cloudy with thick head and lace. Fresh smelling with slight lemon zest aroma. Taste is rich and fulfilling with strong lemon flavor. Also notable is the strong, almost overwhealming clove flavor. Fresh and thirst quenching. Goes great with brats at a German festival.&quot; We will start with only examining single words, also called “unigrams” in the linguistics jargon. Thus, Medium, straw, and cloudy are the first tokens in this dataset. We will mostly ignore punctuation and spacing, which means we are giving up some meaning for convenience. We could figure out how to manually break this up into tokens with enough work, using functions like str_separate(), but happily there are a large set of competing R tools for tokenization, for example: cleaned_reviews_example$cleaned_review[1] %&gt;% tokenizers::tokenize_words(simplify = TRUE) ## [1] &quot;medium&quot; &quot;straw&quot; &quot;and&quot; &quot;cloudy&quot; ## [5] &quot;with&quot; &quot;thick&quot; &quot;head&quot; &quot;and&quot; ## [9] &quot;lace&quot; &quot;fresh&quot; &quot;smelling&quot; &quot;with&quot; ## [13] &quot;slight&quot; &quot;lemon&quot; &quot;zest&quot; &quot;aroma&quot; ## [17] &quot;taste&quot; &quot;is&quot; &quot;rich&quot; &quot;and&quot; ## [21] &quot;fulfilling&quot; &quot;with&quot; &quot;strong&quot; &quot;lemon&quot; ## [25] &quot;flavor&quot; &quot;also&quot; &quot;notable&quot; &quot;is&quot; ## [29] &quot;the&quot; &quot;strong&quot; &quot;almost&quot; &quot;overwhealming&quot; ## [33] &quot;clove&quot; &quot;flavor&quot; &quot;fresh&quot; &quot;and&quot; ## [37] &quot;thirst&quot; &quot;quenching&quot; &quot;goes&quot; &quot;great&quot; ## [41] &quot;with&quot; &quot;brats&quot; &quot;at&quot; &quot;a&quot; ## [45] &quot;german&quot; &quot;festival&quot; Note that this function also (by default) turns every word into its lowercase version (e.g., Medium becomes medium) and strips out punctuation. This is because, to a program like R, upper- and lowercase versions of the same string are not equivalent. If we have reason to believe preserving this difference is important, we might want to rethink allowing this behavior. Now, we have 46 observed tokens for our first review in our example dataset. But of course this is not super interesting–we need to apply this kind of transformation to every single one of our ~20,000 reviews. With some fiddling around with mutate() we might be able to come up with a solution, but happily this is where we start using the tidytext package. The unnest_tokens() function built into that package will allow us to transform our text directly into the format we want, with very little effort. beer_data_tokenized &lt;- beer_data %&gt;% # We may want to keep track of a unique ID for each review mutate(review_id = row_number()) %&gt;% unnest_tokens(output = token, input = cleaned_review, token = &quot;words&quot;) %&gt;% # Here we do a bit of extra cleaning mutate(token = str_remove_all(token, &quot;\\\\.|_&quot;)) beer_data_tokenized %&gt;% # We show just a few of the columns for printing&#39;s sake select(beer_name, rating, token) ## # A tibble: 1,770,136 × 3 ## beer_name rating token ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Caffrey&#39;s Irish Ale 4.46 this ## 2 Caffrey&#39;s Irish Ale 4.46 is ## 3 Caffrey&#39;s Irish Ale 4.46 a ## 4 Caffrey&#39;s Irish Ale 4.46 very ## 5 Caffrey&#39;s Irish Ale 4.46 good ## 6 Caffrey&#39;s Irish Ale 4.46 irish ## 7 Caffrey&#39;s Irish Ale 4.46 ale ## 8 Caffrey&#39;s Irish Ale 4.46 i&#39;m ## 9 Caffrey&#39;s Irish Ale 4.46 not ## 10 Caffrey&#39;s Irish Ale 4.46 sure ## # ℹ 1,770,126 more rows The unnest_tokens() function actually applies the tokenizers::tokenize_words() function in an efficient, easy way: it takes a column of raw character data and then tokenizes it as specified, outputting a tidy format of 1-token-per-row. Now we have our observations (tokens) each on its own line, ready for further analysis. In order to make what we’re doing easier to follow, let’s also take a look at our example data. cleaned_reviews_tokenized &lt;- cleaned_reviews_example %&gt;% unnest_tokens(input = cleaned_review, output = token) %&gt;% # Here we do a bit of extra cleaning mutate(token = str_remove_all(token, &quot;\\\\.|_&quot;)) cleaned_reviews_tokenized %&gt;% filter(beer_name == &quot;Kozel&quot;) %&gt;% # Let&#39;s select a few variables for printing select(beer_name, rating, token) ## # A tibble: 187 × 3 ## beer_name rating token ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Kozel 4.25 a ## 2 Kozel 4.25 truly ## 3 Kozel 4.25 good ## 4 Kozel 4.25 authentic ## 5 Kozel 4.25 pilsner ## 6 Kozel 4.25 bordering ## 7 Kozel 4.25 on ## 8 Kozel 4.25 great ## 9 Kozel 4.25 this ## 10 Kozel 4.25 is ## # ℹ 177 more rows When we use unnest_tokens(), all of the non-text data gets treated as information about each token–so beer_name, rating, etc are duplicated for each token that now has its own row. This will allow us to perform a number of types of text analysis. 5.1.3 A quick note: saving data We’ve talked about wanting to clear our workspace from R in between sessions to make sure our work and code are reproducible. But at this point we’ve done a lot of work, and we might want to save this work for later. Or, we might want to share our work with others. The tidyverse has a good utility for this when we’re working with data frames and tibbles: the write_csv() function is the opposite of the read_csv() function we’ve gotten comfortable with: it takes a data frame and writes it into a .csv file that is easily shared with others or reloaded in a later session. # Let&#39;s store our cleaned and tokenized reviews write_csv(x = beer_data_tokenized, file = &quot;data/tokenized_beer_reviews.csv&quot;) 5.2 Text Analysis Pt. II: Basic text analysis approaches using tokens We’ve now seen how to import and clean text, and to transform text data into one useful format for analysis: a tidy table of tokens. (There are certainly other useful formats, but we will not be covering them here.) Now we get to the actually interesting part. We’re going to tackle some basic but powerful approaches for parsing the meaning of large volumes of text. 5.2.1 Word counts A common approach for getting a quick, analytic summary of the nature of the tokens (observations) in our reviews might be to look at the frequency of word use across reviews: this is pretty closely equivalent to a Check-All-That-Apply (CATA) framework: we will look how often each term is used, and will then extend this to categories of interest to our analysis. So, for example, we might have hypotheses like the following that we could hope to investigate using word frequencies: Overall, flavor words are the most frequently used words in reviews of beer online. The frequency of flavor-related words will be different for major categories of beer, like “hop”-related words for IPAs and “chocolate”/“coffee” terms for stouts. The top flavor words will be qualitatively more positive for reviews associated with the top 10% of ratings, and negative for reviews associated with the bottom 10%. I will not be exploring each of these in this section of the workshop because of time constraints, but hopefully you’ll be able to use the tools I will demonstrate to solve these problems on your own. Let’s start by combining our tidy data wrangling skills from tidyverse with our newly tidied text data from unnest_tokens() to try to test the first answer. beer_data_tokenized %&gt;% # The count() function gives the number of rows that contain unique values count(token) %&gt;% # get the 20 most frequently occurring words slice_max(order_by = n, n = 20) %&gt;% # Here we do some wrangling for visualization mutate(token = as.factor(token) %&gt;% fct_reorder(n)) %&gt;% # let&#39;s visualize this just for fun ggplot(aes(x = token, y = n)) + geom_col() + coord_flip() + theme_bw() + labs(x = NULL, y = NULL, title = &quot;The 20 most frequent words are not that useful.&quot;) Unfortunately for our first hypothesis, we have quickly encountered a common problem in token-based text analysis. The most frequent words in most languages are “helper” words that are necessary for linguistic structure and communication, but are not unique to a particular topic or context. For example, in English the articles a, and the tend to be the most frequent tokens in any text because they are so ubiquitous. It takes us until the 15th most frequent word, head, to find a word that might have sensory or product implications. So, in order to find meaning in our text, we need to find some way to look past these terms, whose frequency vastly outnumbers words we are actually interested in. 5.2.1.1 “stop words” In computational linguistics, this kind of word is often called a stop word: a word that is functionally important but does not tell us much about the meaning of the text. There are many common lists of such stop words, and in fact the tidytext package provides one such list in the stop_words tibble: stop_words ## # A tibble: 1,149 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 a SMART ## 2 a&#39;s SMART ## 3 able SMART ## 4 about SMART ## 5 above SMART ## 6 according SMART ## 7 accordingly SMART ## 8 across SMART ## 9 actually SMART ## 10 after SMART ## # ℹ 1,139 more rows The most basic approach to dealing with stop words is just to outright remove them from our data. This is easy when we’ve got a tidy, one-token-per-row structure. We could use some version of filter() to remove stop words from our beer_data_tokenized tibble, but this is exactly what the anti_join() function (familiar to those of you who have used SQL) will do: with two tibbles, X and Y, anti_join(X, Y) will remove all rows in X that match rows in Y. beer_data_tokenized %&gt;% # &quot;by = &quot; tells what column to look for in X and Y anti_join(y = stop_words, by = c(&quot;token&quot; = &quot;word&quot;)) %&gt;% count(token) %&gt;% # get the 20 most frequently occurring words slice_max(order_by = n, n = 20) %&gt;% # Here we do some wrangling for visualization mutate(token = as.factor(token) %&gt;% fct_reorder(n)) %&gt;% # let&#39;s visualize this just for fun ggplot(aes(x = token, y = n)) + geom_col() + coord_flip() + theme_bw() + labs(x = NULL, y = NULL, title = &quot;These tokens are much more relevant.&quot;, subtitle = &quot;We removed ~1200 stop words.&quot;) Now we are able to address our very first hypothesis: while the most common term (beer) might be in fact a stop word for this data set, the next 11 top terms all seem to have quite a lot to do with flavor. We can extend this approach just a bit to provide the start of one answer to our second question: are there different most-used flavor terms for different beer styles? We can start to tackle this by first defining a couple of categories (since there are a lot of different styles in this dataset): beer_data %&gt;% count(style) ## # A tibble: 99 × 2 ## style n ## &lt;chr&gt; &lt;int&gt; ## 1 Altbier 186 ## 2 American Adjunct Lager 961 ## 3 American Amber / Red Ale 653 ## 4 American Amber / Red Lager 235 ## 5 American Barleywine 187 ## 6 American Black Ale 10 ## 7 American Blonde Ale 251 ## 8 American Brown Ale 166 ## 9 American Dark Wheat Ale 11 ## 10 American Double / Imperial IPA 151 ## # ℹ 89 more rows Let’s just look at the categories I suggested, plus “lager”. We will do this by using mutate to create a simple_style column: # First, we&#39;ll make sure our approach works beer_data %&gt;% # First we will set our style to lower case to make matching easier mutate(style = tolower(style), # Then we will use a conditional match to create our new style simple_style = case_when(str_detect(style, &quot;lager&quot;) ~ &quot;lager&quot;, str_detect(style, &quot;ipa&quot;) ~ &quot;IPA&quot;, str_detect(style, &quot;stout|porter&quot;) ~ &quot;dark&quot;, TRUE ~ &quot;other&quot;)) %&gt;% count(simple_style) ## # A tibble: 4 × 2 ## simple_style n ## &lt;chr&gt; &lt;int&gt; ## 1 IPA 1274 ## 2 dark 2562 ## 3 lager 3245 ## 4 other 13278 # Then we&#39;ll implement the approach for our tokenized data beer_data_tokenized &lt;- beer_data_tokenized %&gt;% mutate(style = tolower(style), # Then we will use a conditional match to create our new style simple_style = case_when(str_detect(style, &quot;lager&quot;) ~ &quot;lager&quot;, str_detect(style, &quot;ipa&quot;) ~ &quot;IPA&quot;, str_detect(style, &quot;stout|porter&quot;) ~ &quot;dark&quot;, TRUE ~ &quot;other&quot;)) # Finally, we&#39;ll plot the most frequent terms for each simple_style beer_data_tokenized %&gt;% # filter out stop words anti_join(stop_words, by = c(&quot;token&quot; = &quot;word&quot;)) %&gt;% # This time we count tokens WITHIN simple_style count(simple_style, token) %&gt;% # Then we will group_by the simple_style group_by(simple_style) %&gt;% slice_max(order_by = n, n = 20) %&gt;% # Removing the group_by is necessary for some steps ungroup() %&gt;% # A bit of wrangling for plotting mutate(token = as.factor(token) %&gt;% reorder_within(by = n, within = simple_style)) %&gt;% ggplot(aes(x = token, y = n, fill = simple_style)) + geom_col(show.legend = FALSE) + facet_wrap(~simple_style, scales = &quot;free&quot;, ncol = 4) + scale_x_reordered() + theme_bw() + coord_flip() + labs(x = NULL, y = NULL, title = &quot;Different (sensory) words tend to be used with different styles.&quot;) + theme(axis.text.x = element_text(angle = 90)) 5.2.2 Term Frequency-Inverse Document Frequency (TF-IDF) We have seen that removing stop words dramatically increases the quality of our analysis in regards to our specific answers. However, you may be asking yourself: “how do I know what counts as as a stop word?” For example, we see that “beer” is near the top of the list of most frequent tokens for all 4 categories we defined–it is not useful to us–but it is not part of the stop_words tibble. We could, of course, manually remove it (something like filter(token != \"beer\")). But removing each stop word manually requires us to make a priori judgments about our data, which is not ideal. One approach that takes a statistical approach to identifying and filtering stop words without the need to define them a priori is the tf-idf statistic, which stands for Term Frequency-Inverse Document Frequency. tf-idf requires not just defining tokens, as we have done already using the unnest_tokens() function, but defining “documents” for your specific dataset. The term “document” is somewhat misleading, as it is merely a categorical variable that tells us distinct units in which we want to compare the frequency of our tokens. So in our dataset, simple_style is one such document categorization (representing 4 distinct “documents”); we could also use style, representing 99 distinct categories (“documents”), or any other categorical variable. The reason that we need to define a “document” variable for our dataset is that td-idf will answer, in general, the following question: Given a set of “documents”, what tokens are most frequent within each unique document, but are not common between all documents? Defined this way, it is clear that tf-idf has an empirical kinship with all kinds of within/between statistics that we use on a daily basis. In practice, tf-idf is a simple product of two empirical properties of each token in the dataset. 5.2.2.1 Term Frequency The tf is defined simply for each combination of document and token as the raw count of that token in that document, divided by the sum of raw counts of all tokens in that document. \\(\\text{tf}(t, d) = \\frac{count_{t}}{\\sum_{t \\in d}{count_{t}}}\\) This quantity may be modified (for example, by using smoothing with \\(\\log{(1 + count_{t})}\\)). Exact implementations can vary. 5.2.2.2 Inverse Document Frequency The idf is an empirical estimate of how good a particular token is at distinguishing one “document” from another. It is defined as the logarithm of the total number of documents, divided by the the number of documents that contain the term in question. \\(\\text{idf}(t, D) = \\log{\\frac{N_D}{|\\{{d \\in D \\ : \\ t \\in d\\}|}}}\\) Where \\(D\\) is the set of documents in the dataset. Remember that we obtain the tf-idf by multiplying the two terms, which explains the inverse fraction–if we imagined a “tf/df” with division, the definition of the document frequency might be more intuitive but less numerically tractable. 5.2.2.3 Applying tf-idf in tidytext Overall, the tf part of tf-idf is just a scaled what we’ve been doing so far. But the idf part provides a measure of validation. For our very frequent terms, that we have been removing as stop words, the tf might be high (they occur a lot) but the idf will be extremely small–all documents use tokens like the and is, so they provide little discriminatory power between documents. Ideally, then, tf-idf will give us a way to drop out stop words without the need to specify them a priori. Happily, we don’t have to figure out a function for calculating tf-idf ourselves; tidytext provides the bind_tf_idf() function. The only requirement is that we have a data frame that provides a per-“document” count for each token, which we have already done above: # Let&#39;s first experiment with our &quot;simple_styles&quot; beer_styles_tf_idf &lt;- beer_data_tokenized %&gt;% count(simple_style, token) %&gt;% # And now we can directly add on tf-idf bind_tf_idf(term = token, document = simple_style, n = n) beer_styles_tf_idf %&gt;% # let&#39;s look at some stop words filter(token %in% c(&quot;is&quot;, &quot;a&quot;, &quot;the&quot;, &quot;beer&quot;)) %&gt;% gt::gt() #finhpmnpfs table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #finhpmnpfs thead, #finhpmnpfs tbody, #finhpmnpfs tfoot, #finhpmnpfs tr, #finhpmnpfs td, #finhpmnpfs th { border-style: none; } #finhpmnpfs p { margin: 0; padding: 0; } #finhpmnpfs .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #finhpmnpfs .gt_caption { padding-top: 4px; padding-bottom: 4px; } #finhpmnpfs .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #finhpmnpfs .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #finhpmnpfs .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #finhpmnpfs .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #finhpmnpfs .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #finhpmnpfs .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #finhpmnpfs .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #finhpmnpfs .gt_column_spanner_outer:first-child { padding-left: 0; } #finhpmnpfs .gt_column_spanner_outer:last-child { padding-right: 0; } #finhpmnpfs .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #finhpmnpfs .gt_spanner_row { border-bottom-style: hidden; } #finhpmnpfs .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #finhpmnpfs .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #finhpmnpfs .gt_from_md > :first-child { margin-top: 0; } #finhpmnpfs .gt_from_md > :last-child { margin-bottom: 0; } #finhpmnpfs .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #finhpmnpfs .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #finhpmnpfs .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #finhpmnpfs .gt_row_group_first td { border-top-width: 2px; } #finhpmnpfs .gt_row_group_first th { border-top-width: 2px; } #finhpmnpfs .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #finhpmnpfs .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #finhpmnpfs .gt_first_summary_row.thick { border-top-width: 2px; } #finhpmnpfs .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #finhpmnpfs .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #finhpmnpfs .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #finhpmnpfs .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #finhpmnpfs .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #finhpmnpfs .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #finhpmnpfs .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #finhpmnpfs .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #finhpmnpfs .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #finhpmnpfs .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #finhpmnpfs .gt_left { text-align: left; } #finhpmnpfs .gt_center { text-align: center; } #finhpmnpfs .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #finhpmnpfs .gt_font_normal { font-weight: normal; } #finhpmnpfs .gt_font_bold { font-weight: bold; } #finhpmnpfs .gt_font_italic { font-style: italic; } #finhpmnpfs .gt_super { font-size: 65%; } #finhpmnpfs .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #finhpmnpfs .gt_asterisk { font-size: 100%; vertical-align: 0; } #finhpmnpfs .gt_indent_1 { text-indent: 5px; } #finhpmnpfs .gt_indent_2 { text-indent: 10px; } #finhpmnpfs .gt_indent_3 { text-indent: 15px; } #finhpmnpfs .gt_indent_4 { text-indent: 20px; } #finhpmnpfs .gt_indent_5 { text-indent: 25px; } simple_style token n tf idf tf_idf IPA a 5285 0.045000937 0 0 IPA beer 1151 0.009800582 0 0 IPA is 2364 0.020129085 0 0 IPA the 5630 0.047938557 0 0 dark a 10322 0.045447741 0 0 dark beer 2041 0.008986518 0 0 dark is 4682 0.020614835 0 0 dark the 10610 0.046715804 0 0 lager a 11868 0.045791277 0 0 lager beer 3773 0.014557675 0 0 lager is 5628 0.021714974 0 0 lager the 10734 0.041415872 0 0 other a 55217 0.047339678 0 0 other beer 12491 0.010709019 0 0 other is 23222 0.019909122 0 0 other the 53203 0.045612997 0 0 We can see that, despite very high raw counts for all of these terms, the tf-idf is 0! They do not discriminate across our document categories at all, and so if we start looking for terms with high tf-idf, these will drop out completely. Let’s take a look at the same visualization we made before with raw counts, but now with tf-idf. beer_styles_tf_idf %&gt;% # We still group by simple_style group_by(simple_style) %&gt;% # Now we want tf-idf, not raw count slice_max(order_by = tf_idf, n = 20) %&gt;% ungroup() %&gt;% # A bit of wrangling for plotting mutate(token = as.factor(token) %&gt;% reorder_within(by = tf_idf, within = simple_style)) %&gt;% ggplot(aes(x = token, y = tf_idf, fill = simple_style)) + geom_col(show.legend = FALSE) + facet_wrap(~simple_style, scales = &quot;free&quot;, ncol = 4) + scale_x_reordered() + theme_bw() + coord_flip() + labs(x = NULL, y = NULL, title = &quot;With tf-idf we get much more specific terms.&quot;, subtitle = &quot;For example, &#39;oatmeal&#39; for stouts, &#39;grapefruity&#39; for IPAs, and so on.&quot;) + theme(axis.text.x = element_blank()) Thus, tf-idf gives us a flexible, empirical (data-based) model that will surface unique terms for us directly from the data. We can apply the same approach, for example, with a bit of extra wrangling, to see what terms are most associated with beers from the bottom and top deciles by rating (starting to address hypothesis #3): beer_data_tokenized %&gt;% # First we get deciles of rating mutate(rating_decile = ntile(rating, n = 10)) %&gt;% # And we&#39;ll select just the top 2 and bottom 2 deciles %&gt;% filter(rating_decile %in% c(1, 2, 9, 10)) %&gt;% # Then we follow the exact same pipeline to get tf-idf count(rating_decile, token) %&gt;% bind_tf_idf(term = token, document = rating_decile, n = n) %&gt;% group_by(rating_decile) %&gt;% # Since we have more groups, we&#39;ll just look at 10 tokens slice_max(order_by = tf_idf, n = 10) %&gt;% ungroup() %&gt;% # A bit of wrangling for plotting mutate(token = as.factor(token) %&gt;% reorder_within(by = tf_idf, within = rating_decile)) %&gt;% ggplot(aes(x = token, y = tf_idf, fill = rating_decile)) + geom_col(show.legend = FALSE) + facet_wrap(~rating_decile, scales = &quot;free&quot;, ncol = 4) + scale_x_reordered() + scale_fill_viridis_c() + theme_bw() + coord_flip() + labs(x = NULL, y = NULL, subtitle = &quot;When we compare the top 2 and bottom 2 deciles, we see much more affective language.&quot;) + theme(axis.text.x = element_blank()) And important point to keep in mind about tf-idf is that it is data-based: the numbers are only meaningful within the specific set of tokens and documents. Thus, if I repeat the above pipeline but include all 10 deciles, we will not see the same words (and we will see that the model immediately struggles to find meaningful terms at all). This is because I am no longer calculating with 4 documents, but 10. These statistics are best thought of as descriptive, especially when we are repeatedly using them to explore a dataset. (#fig:tf-idf is data-based so it will change with document choice)tf-idf is data-based so it will change with “document” choice We’ve now practiced some basic tools for first wrangling text data into a form with which we can work, and then applying some simple, empirical statistics to start to extract meaning. We’ll discuss sentiment analysis, a broad suite of tools that attempts to impute some sort of emotional or affective weight to words, and then produce scores for texts based on these weights. 5.3 Text Analysis Pt. III: Sentiment analysis In recent years, sentiment analysis has gotten a lot of attention in consumer sciences, and it’s one of the topics that has penetrated the furthest into sensory science, because its goals are easily understood in terms of our typical goals: quantifying consumer affective responses to a set of products based on consumption experiences. In sentiment analysis, we attempt to replicate the human inferential process, in which we, as readers, are able to infer–without necessarily being able to explicitly describe how we can tell–the emotional tone of a piece of writing. We understand implicitly whether the author is trying to convey various emotional states, like disgust, appreciation, joy, etc. In recent years, there have been a number of approaches to sentiment analysis. The current state of the art is to use some kind of machine learning, such as random forests (“shallow learning”) or pre-trained neural networks (“deep learning”; usually convolutional, but sometimes recursive) to learn about a large batch of similar texts and then to process texts of interest. Not to plug myself too much, but we have a poster on such an approach at this conference, which may be of interest to some of you. Machine learning: just throw algebra at the problem! (via XKCD) While these state-of-the-art approaches are outside of the scope of this workshop, older techniques that use pre-defined dictionaries of sentiment words are easy to implement with a tidy text format, and can be very useful for the basic exploration of text data. These have been used in recent publications to great effect, such as in the recent paper from Luc et al. (2020). The tidytext package includes the sentiments tibble by default, which is a version of that published by Hu &amp; Liu (2004). sentiments ## # A tibble: 6,786 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative ## # ℹ 6,776 more rows count(sentiments, sentiment) ## # A tibble: 2 × 2 ## sentiment n ## &lt;chr&gt; &lt;int&gt; ## 1 negative 4781 ## 2 positive 2005 This is a very commonly used lexicon, which has shown to perform reasonably well on online-review based texts. The tidytext package also gives easy access to a few other lexicons through the get_sentiment() function, which may prompt for downloading some of the lexicons in order to avoid copyright issues. For the purpose of this workshop, we’ll just work with the sentiments tibble, which can also be accessed using get_sentiment(\"bing\"). The structure of tidy data with tokens makes it very easy to use these dictionary-based sentiment analysis approaches. To do so, all we need to do is use a left_join() function, which is similar to the anti_join() function we used for stop words. In this case, for data tables X and Y, left_join(X, Y) finds rows in Y that match X, and imports all the columns from Y for those matches. beer_sentiments &lt;- beer_data_tokenized %&gt;% left_join(sentiments, by = c(&quot;token&quot; = &quot;word&quot;)) ## Warning in left_join(., sentiments, by = c(token = &quot;word&quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 1164310 of `x` matches multiple rows in `y`. ## ℹ Row 4229 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. beer_sentiments %&gt;% select(review_id, beer_name, style, rating, token, sentiment) ## # A tibble: 1,770,138 × 6 ## review_id beer_name style rating token sentiment ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Caffrey&#39;s Irish Ale irish red ale 4.46 this &lt;NA&gt; ## 2 1 Caffrey&#39;s Irish Ale irish red ale 4.46 is &lt;NA&gt; ## 3 1 Caffrey&#39;s Irish Ale irish red ale 4.46 a &lt;NA&gt; ## 4 1 Caffrey&#39;s Irish Ale irish red ale 4.46 very &lt;NA&gt; ## 5 1 Caffrey&#39;s Irish Ale irish red ale 4.46 good positive ## 6 1 Caffrey&#39;s Irish Ale irish red ale 4.46 irish &lt;NA&gt; ## 7 1 Caffrey&#39;s Irish Ale irish red ale 4.46 ale &lt;NA&gt; ## 8 1 Caffrey&#39;s Irish Ale irish red ale 4.46 i&#39;m &lt;NA&gt; ## 9 1 Caffrey&#39;s Irish Ale irish red ale 4.46 not &lt;NA&gt; ## 10 1 Caffrey&#39;s Irish Ale irish red ale 4.46 sure &lt;NA&gt; ## # ℹ 1,770,128 more rows It is immediately apparent how sparse the sentiment lexicons are compared to our actual data. This is one key problem with dictionary-based approaches–we often don’t have application- or domain-specific lexicons, and the lexicons that exist are often not well calibrated to our data. We can perform a simple count() and group_by() operation to get a rough sentiment score. sentiment_ratings &lt;- beer_sentiments %&gt;% count(review_id, sentiment) %&gt;% drop_na() %&gt;% group_by(review_id) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) sentiment_ratings ## # A tibble: 20,328 × 4 ## # Groups: review_id [20,328] ## review_id negative positive sentiment ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 9 8 ## 2 2 3 4 1 ## 3 3 1 6 5 ## 4 4 1 5 4 ## 5 5 0 1 1 ## 6 6 1 4 3 ## 7 7 1 9 8 ## 8 8 1 1 0 ## 9 9 0 5 5 ## 10 10 3 3 0 ## # ℹ 20,318 more rows In this case, we’re being very rough: we’re not taking into account any of the non-sentiment words, for example. Nevertheless, let’s take a look at what we’ve gotten: beer_data %&gt;% mutate(review_id = row_number()) %&gt;% left_join(sentiment_ratings) %&gt;% ggplot(aes(x = sentiment, y = rating)) + geom_jitter(alpha = 0.2, size = 1) + geom_smooth(method = &quot;lm&quot;) + coord_cartesian(xlim = c(-11, 30), ylim = c(1, 5)) + theme_bw() It does appear that there is a positive, probably non-linear relationship between sentiment and rating. We could do some reshaping (normalizing, possibly exponentiating or otherwise transforming the subsequent score) of the sentiment scores to get a better relationship. beer_data &lt;- beer_data %&gt;% mutate(review_id = row_number()) %&gt;% left_join(sentiment_ratings) ## Joining with `by = join_by(review_id)` beer_data %&gt;% select(beer_name, style, rating, sentiment) %&gt;% pivot_longer(cols = c(rating, sentiment), names_to = &quot;scale&quot;, values_to = &quot;value&quot;) %&gt;% group_by(style, scale) %&gt;% summarize(mean_value = mean(value, na.rm = TRUE), se_value = sd(value, na.rm = TRUE) / sqrt(n())) %&gt;% group_by(scale) %&gt;% slice_max(order_by = mean_value, n = 10) %&gt;% ungroup() %&gt;% mutate(style = factor(style) %&gt;% reorder_within(by = mean_value, within = scale)) %&gt;% ggplot(aes(x = style, y = mean_value, fill = scale)) + geom_col(position = &quot;dodge&quot;, show.legend = FALSE) + scale_x_reordered() + coord_flip() + facet_wrap(~scale, scales = &quot;free&quot;) ## `summarise()` has grouped output by &#39;style&#39;. You can override using the ## `.groups` argument. We can see that we get quite different rankings from sentiment scores and from ratings. It might behoove us to examine some of those mismatches in order to identify where they originate from. # Here are the reviews where the ratings most disagree with the sentiment beer_data %&gt;% # normalize sentiment and ratings so we can find the largest mismatch mutate(rating_norm = rating / max(rating, na.rm = TRUE), sentiment_norm = sentiment / max(sentiment, na.rm = TRUE), diff = rating_norm - sentiment_norm) %&gt;% select(review_id, beer_name, sentiment_norm, rating_norm, diff, cleaned_review) %&gt;% slice_max(order_by = diff, n = 2) %&gt;% gt::gt() #hmfbwrctca table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #hmfbwrctca thead, #hmfbwrctca tbody, #hmfbwrctca tfoot, #hmfbwrctca tr, #hmfbwrctca td, #hmfbwrctca th { border-style: none; } #hmfbwrctca p { margin: 0; padding: 0; } #hmfbwrctca .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #hmfbwrctca .gt_caption { padding-top: 4px; padding-bottom: 4px; } #hmfbwrctca .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #hmfbwrctca .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #hmfbwrctca .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hmfbwrctca .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hmfbwrctca .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hmfbwrctca .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #hmfbwrctca .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #hmfbwrctca .gt_column_spanner_outer:first-child { padding-left: 0; } #hmfbwrctca .gt_column_spanner_outer:last-child { padding-right: 0; } #hmfbwrctca .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #hmfbwrctca .gt_spanner_row { border-bottom-style: hidden; } #hmfbwrctca .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #hmfbwrctca .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #hmfbwrctca .gt_from_md > :first-child { margin-top: 0; } #hmfbwrctca .gt_from_md > :last-child { margin-bottom: 0; } #hmfbwrctca .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #hmfbwrctca .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #hmfbwrctca .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #hmfbwrctca .gt_row_group_first td { border-top-width: 2px; } #hmfbwrctca .gt_row_group_first th { border-top-width: 2px; } #hmfbwrctca .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hmfbwrctca .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #hmfbwrctca .gt_first_summary_row.thick { border-top-width: 2px; } #hmfbwrctca .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hmfbwrctca .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hmfbwrctca .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #hmfbwrctca .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #hmfbwrctca .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #hmfbwrctca .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hmfbwrctca .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hmfbwrctca .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hmfbwrctca .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hmfbwrctca .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hmfbwrctca .gt_left { text-align: left; } #hmfbwrctca .gt_center { text-align: center; } #hmfbwrctca .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #hmfbwrctca .gt_font_normal { font-weight: normal; } #hmfbwrctca .gt_font_bold { font-weight: bold; } #hmfbwrctca .gt_font_italic { font-style: italic; } #hmfbwrctca .gt_super { font-size: 65%; } #hmfbwrctca .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #hmfbwrctca .gt_asterisk { font-size: 100%; vertical-align: 0; } #hmfbwrctca .gt_indent_1 { text-indent: 5px; } #hmfbwrctca .gt_indent_2 { text-indent: 10px; } #hmfbwrctca .gt_indent_3 { text-indent: 15px; } #hmfbwrctca .gt_indent_4 { text-indent: 20px; } #hmfbwrctca .gt_indent_5 { text-indent: 25px; } review_id beer_name sentiment_norm rating_norm diff cleaned_review 16065 Imperial Stout -0.3448276 0.980 1.324828 : The color is of black paint. The charcoal-brown head disappears quickly. The smell is of bitterness. Deep, unabiding bitterness of tortured souls. Peets espresso Burning pine forests Sticky, toung-lashing bitterness with a monster grip forces flavors of dried fig, cigar, hazelnut and bitter chocolate (i.e. Valrhona percent Guanaja ). This is a big, big beer. Not for the timid. Primordial, bowels-of-the-earth type energy is packed in to this now larger, oz. bottle. 15808 Adam -0.3448276 0.974 1.318828 I sampled batch number , I'm gonna put one of these guys away and try it again in a year.Pours syrupy with no head, especially if one follows the directions on the bottle indicating to pour slowly, and I am a sucker for directions on the bottle. Color is incredibly dark brown, completely opaque. Leaves a slight lace as one slowly sips this beer down.Smell is quite complex and sorting it out completely is above the ability of my paltry nose. I would describe it as a sweet and slightly fruity (think figs, raisins). The enigmatic smell seems to call out drink me, and so I listen.Overtones of alcohol do not seem to affect the complex flavor, even though you do feel your BA levels rising as you sip this bad boy down. It seems the smell did not lie, as the taste of the beer is itself sweet and slightly fruity (again I'm thinking of figs and raisins), but there seems to be even more to sort out. There is a bitterness which comes initially and seems to be countered immediately by the sweet flavor described earlier. Finally, at the end of the sip one nutty and coffee flavors resonate and the mild bitterness is back.Overall:Easily the most drinkable double digit ABV beer I have had. Not for beginners, like Stone Arrogant Bastard, this beer just wants to kick your ass. My suggestion is to let it. # And here are the reviews where the sentiment most disagreed with the rating beer_data %&gt;% # normalize sentiment and ratings so we can find the largest mismatch mutate(rating_norm = rating / max(rating, na.rm = TRUE), sentiment_norm = sentiment / max(sentiment, na.rm = TRUE), diff = sentiment_norm - rating_norm) %&gt;% select(review_id, beer_name, sentiment_norm, rating_norm, diff, cleaned_review) %&gt;% slice_max(order_by = diff, n = 2) %&gt;% gt::gt() #wpimoarhdm table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #wpimoarhdm thead, #wpimoarhdm tbody, #wpimoarhdm tfoot, #wpimoarhdm tr, #wpimoarhdm td, #wpimoarhdm th { border-style: none; } #wpimoarhdm p { margin: 0; padding: 0; } #wpimoarhdm .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wpimoarhdm .gt_caption { padding-top: 4px; padding-bottom: 4px; } #wpimoarhdm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wpimoarhdm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #wpimoarhdm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wpimoarhdm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wpimoarhdm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wpimoarhdm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wpimoarhdm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wpimoarhdm .gt_column_spanner_outer:first-child { padding-left: 0; } #wpimoarhdm .gt_column_spanner_outer:last-child { padding-right: 0; } #wpimoarhdm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #wpimoarhdm .gt_spanner_row { border-bottom-style: hidden; } #wpimoarhdm .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #wpimoarhdm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wpimoarhdm .gt_from_md > :first-child { margin-top: 0; } #wpimoarhdm .gt_from_md > :last-child { margin-bottom: 0; } #wpimoarhdm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wpimoarhdm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #wpimoarhdm .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #wpimoarhdm .gt_row_group_first td { border-top-width: 2px; } #wpimoarhdm .gt_row_group_first th { border-top-width: 2px; } #wpimoarhdm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wpimoarhdm .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #wpimoarhdm .gt_first_summary_row.thick { border-top-width: 2px; } #wpimoarhdm .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wpimoarhdm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wpimoarhdm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wpimoarhdm .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #wpimoarhdm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wpimoarhdm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wpimoarhdm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wpimoarhdm .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wpimoarhdm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wpimoarhdm .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wpimoarhdm .gt_left { text-align: left; } #wpimoarhdm .gt_center { text-align: center; } #wpimoarhdm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wpimoarhdm .gt_font_normal { font-weight: normal; } #wpimoarhdm .gt_font_bold { font-weight: bold; } #wpimoarhdm .gt_font_italic { font-style: italic; } #wpimoarhdm .gt_super { font-size: 65%; } #wpimoarhdm .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #wpimoarhdm .gt_asterisk { font-size: 100%; vertical-align: 0; } #wpimoarhdm .gt_indent_1 { text-indent: 5px; } #wpimoarhdm .gt_indent_2 { text-indent: 10px; } #wpimoarhdm .gt_indent_3 { text-indent: 15px; } #wpimoarhdm .gt_indent_4 { text-indent: 20px; } #wpimoarhdm .gt_indent_5 { text-indent: 25px; } review_id beer_name sentiment_norm rating_norm diff cleaned_review 1477 Harvest Ale (Limited Edition) 1.0000000 0.874 0.1260000 Bottle reviews (JAN) Another nice bier to warm up with on a cold, snowy night! Label states st December . Bottling date, or brew date? Anyhow, label also states best before Nov . Oooops, just a bit late. Oh well. Better nate than lever Into the snifter she goes, and it's a dark brown, opaque brew, with a very thin, wispy head congregating only around the rim of the glass. Upon closer inspection, with a little light turned on, I see that the Harvest has a deep ruby hue. Quite nice! Nose is a potent, sweet, rich combo of dark fruit and malty sweetness. Mmmm! Body is more than medium, almost chewy, and silky smooth. Oh yeah! And the flavor is replete with fruity notes, like plum and raisin. Not quite as sweet as the nose suggests, but bulging with dark fruit notes. The high abv is oh-so-well concealed! Don't feel any real warmth, though it's early yet! Along with the fruit is an undercurrent of caramel/toffee sweetness. Real nice! Gotta sip it though; take your time! Tasty bier!! overall: .appearance: | smell: . | taste: . | mouthfeel: . | drinkability: *** ***Bottle (FEB) This is a ml bottle of vintage. Poured a very rich, dark brown color, with little transparency to it. Head is a finely bubbled, tan layer, with more head ringing the outer rim of the snifter. It leaves a fair amount of lace, yet nothing spectacular. Upon taking a whiff I am inundated by raisiny, dark fruit aromas, as well as the unmistakable nose of alcohol. Sweet and rich quite nice! The body is medium to medium-plus, and the biers' passage across the tongue ruffles nary a feather nice and smooth! The taste itself is loaded with creamy, rich, sweet flavors lots of dark fruit and some toffee notes as well. A verrry nice sipping bier, which has aged well since '. Cheers!! (././././. = .)*** ***Cask sample from MAY This beer was sampled at the Boston Beer Summit. The following, courtesy of Matthias Neidhart of B. Unite tongue sticking out It was vintage , . percent alc./vol. The wooden cask was primed with E.Dupont'scalvados from Normandie,France, prior to its filled with JW Lees Harvest Ale. Lovely mahogany-like color; minimal head and lace, though. Assertive sweet nose, with alcohol quite present. Very nice body, and the smoothness caresses the tongue. Rich, sweet, malty, oaky, nutty flavors with the ever-present alcohol. A wonderful brew! (././././. = .) 16339 XS Old Crustacean 0.7931034 0.686 0.1071034 Vintage: This is likely a legend in the making, but it is only at AA ball right now. Like any prospect, it needs time to develop, but many of the skills are already in place.Muddy brown-orange. Frothy, creamy khaki head retains well for a brew of this size.Aroma is big on hops, with pine being the dominate persuasion. Some toffee and chocolate hues are noted as well. Taste is, in a word, raw. Ragged, piney hops seem to pierce through everything. Bitterness is huge. Toastedness and chocolate stuggle for survival. There is a solid carmel/toffee base, but this cannot weather the hop melee. Solvent-like alcohol pops up late, forming an oppresive alliance. Faint dark fruits, and perhaps even tropical fruits linger in the distance, too frightened to approached due to the menacing regime. In a year or two (or five), the malts will begin to win this war, and make this much more balanced. But the hops and alcohol are too aggressive at this point to make any headway. The body is in place, smooth and viscious. Chewable, if somewhat syrup-like.Presently, it is only drinkable to witness the carnage. I'd suggest giving this some time to achieve stability. Psychotic hopheads excluded.With some rounding, this will be in the majors someday. A rough diamond as it is, but capable of aspiring to it's big brother. The bigger frame (oz vs oz) helps.Thanks, Bighuge. But I should have let this one develop. Hope to find another one to cellar.-- Vintage: I am truely stunned by this amazing beverage where to start? How about the f'king oz's I want oz bottles But at least I have heard the latest batch comes in them.This is my favorite Barleywine. Fantastic integration after years but this could go strong for several more. Flavor upon flavor. Malt lays down some sweet carmel and (slightly) bready notes. The hops jump on top with some piney and splendedly bitter tones. I also detected some minty hues as well. Sweetness dominates, but in a lovely way. Balance is extraordinary. My lord, is this good.Only drawback (as previously mentioned) is the dollar . for a oz bottle. But still unquestionably worth it. One of the absolute best. Bargain at any price.I am amazed that I can still find this batch, but it is still available in stores in the Madison area. I won't say which ones out of selfishness so you better act fast!!(note to self: pick up more bottles of Ol Crust tomorrow) Interestingly, we see here some evidence that some kind of normalization for review length might be necessary. Let’s take one more look into this area: beer_data %&gt;% mutate(word_count = tokenizers::count_words(cleaned_review)) %&gt;% select(review_id, rating, sentiment, word_count) %&gt;% pivot_longer(cols = c(rating, sentiment)) %&gt;% ggplot(aes(x = word_count, y = value, color = name)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + facet_wrap(~name, scales = &quot;free&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) It appears that both rating and sentiment are positively correlated with word count, but that (unsurprisingly) sentiment is more strongly correlated. Going forward, we might want to consider some way to normalize for word count in our sentiment scores. 5.4 Text Analysis Pt. IV: n-gram models An obvious and valid critique of sentiment analysis as implemented above is the focus on single words, or unigrams. It is very clear that English (and most languages) have levels of meaning that go beyond the single word, so focusing our analysis on single words as are tokens/observations will lose some (or a lot!) of meaning. It is common in modern text analysis to look at units beyond the single word: bi-, tri-, or n-grams, for example, or so-called “skipgrams” for the popular and powerful word2vec family of algorithms. Tools like convolutional and recursive neural networks will look at larger chunks of texts, combined in different ways. Let’s take a look at bigrams in our data-set: tokens that are made up of 2 adjacent words. We can get them using the same unnest_tokens() function, but with the request to get token = \"ngrams\" and the number of tokens set to n = 2. beer_bigrams &lt;- beer_data %&gt;% unnest_tokens(output = bigram, input = cleaned_review, token = &quot;ngrams&quot;, n = 2) beer_bigrams %&gt;% filter(review_id == 1) %&gt;% select(beer_name, bigram) ## # A tibble: 115 × 2 ## beer_name bigram ## &lt;chr&gt; &lt;chr&gt; ## 1 Caffrey&#39;s Irish Ale this is ## 2 Caffrey&#39;s Irish Ale is a ## 3 Caffrey&#39;s Irish Ale a very ## 4 Caffrey&#39;s Irish Ale very good ## 5 Caffrey&#39;s Irish Ale good irish ## 6 Caffrey&#39;s Irish Ale irish ale ## 7 Caffrey&#39;s Irish Ale ale i&#39;m ## 8 Caffrey&#39;s Irish Ale i&#39;m not ## 9 Caffrey&#39;s Irish Ale not sure ## 10 Caffrey&#39;s Irish Ale sure if ## # ℹ 105 more rows We can see that, in this first review, we already have a bigram with strong semantic content: not sure should tell us that looking only at the unigram sure will give us misleading results. We can use some more tidyverse helpers to get a better idea of the scale of the problem. The separate() function breaks one character vector into multiple columns, according to whatever separating characters you specify. beer_bigrams %&gt;% separate(col = bigram, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% # Now we will first filter for bigrams starting with &quot;not&quot; filter(word1 == &quot;not&quot;) %&gt;% # And then we&#39;ll count up the most frequent pairs of negated biterms count(word1, word2) %&gt;% arrange(-n) ## # A tibble: 1,298 × 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 not a 1511 ## 2 not as 777 ## 3 not much 709 ## 4 not bad 522 ## 5 not too 473 ## 6 not the 376 ## 7 not quite 310 ## 8 not to 283 ## 9 not very 260 ## 10 not sure 251 ## # ℹ 1,288 more rows We can see that the 4th most frequent pair is “not bad”, which means that bad, classified as a negative token in the sentiments tibble, is actually being overcounted as negative in our simple unigram analysis. We can actually look more closely at this with just a little bit more wrangling: beer_bigrams %&gt;% separate(col = bigram, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(sentiments, by = c(&quot;word2&quot; = &quot;word&quot;)) %&gt;% count(word1, word2, sort = TRUE) ## # A tibble: 303 × 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 not bad 522 ## 2 not enough 111 ## 3 not overwhelming 104 ## 4 not great 90 ## 5 not worth 73 ## 6 not good 65 ## 7 not like 65 ## 8 not outstanding 46 ## 9 not bitter 44 ## 10 not overbearing 43 ## # ℹ 293 more rows We could use such an approach to try to get a handle on the problem of context in our sentiment analysis, by inverting the sentiment score of any word that is part of a “not bigram”. Of course, such negations can be expressed over entire sentences, or even flavor the entire text (as in the use of sarcasm). There are entire analysis workflows built on this kind of context flow. For example, the sentiment package, which was used in the recent Luc et al. (2020) paper on free JAR analysis, takes just such an approach. We can quickly experiment with this package as a demonstration. library(sentimentr) polarity_sentiment &lt;- cleaned_reviews_example %&gt;% select(beer_name, rating, cleaned_review) %&gt;% get_sentences() %&gt;% sentiment_by(by = &quot;beer_name&quot;) polarity_sentiment ## beer_name word_count sd ave_sentiment ## 1: Black Bear Ale 119 0.2304159 0.007328644 ## 2: Kozel 187 0.2651420 0.306529868 ## 3: Odell Cutthroat Pale Ale 45 0.5829988 0.435923231 ## 4: Pike XXXXX Stout 102 0.2946472 -0.027863992 ## 5: Stoudt&#39;s Weizen 46 0.2081545 0.155677045 We can visualize how the sentimentr algorithm “sees” the sentences by using the highlight(polarity_sentiment) call, but since this outputs HTML I will embed it as an image here. sentimentr sentence-based sentiment analysis, using the dictionary-based, polarity-shifting algorithm. As a note, this is only one example of such an algorithm, and it may or may not be the best one. While Luc et al. (2020) found good results, we (again plugging our poster) found that sentimentr didn’t outperform simpler algorithms. However, this may have to do with the set up of lexicons, stop-word lists, etc. "],["wrap-up-and-further-resources.html", "6 Wrap-up and further resources 6.1 Getting help 6.2 Learning more with Sensometrics Society 6.3 Further reading/resources 6.4 Questions/Comments", " 6 Wrap-up and further resources Let’s look back at what we were aiming to do today: In this tutorial, we will introduce the audience to the R statistical programming environment and the RStudio Interactive Development Environment (IDE) with the aim of developing sufficient basic skills to conduct a text analysis on sensory-relevant text data. We will provide a learning dataset of text data for the analysis—a set of food-product free-comment reviews that are associated with overall liking scores. This will allow us to demonstrate connections between text analysis methods and basic sensory and consumer science approaches. We will also provide an R script that walks through all steps of importing, manipulating, and analysing the test dataset. The tutorial will have 2 sections. In the first section of the tutorial, we will introduce R and RStudio, we will cover the basic commands of R, and we will cover key, user-friendly conventions of ”tidy” R programming for importing, manipulating, and plotting data using the “tidyverse” packages. In the second section we will use the “tidytext” package to conduct basic text analysis, including text tokenization, text modeling using TF-IDF, and basic lexicon-based sentiment analysis. We have managed to touch on all of these topics, but of course we have taken the most cursory look at each. I hope what we’ve gone over today has inspired you, sure, but I mostly hope it has shown you how much you can do with just a little knowledge. My journey in terms of learning data science with R, and text analysis in particular, has been all about building my coding ability incrementally. My code looks more like this than anything else, but I am able to get so much done: What does good code even look like? (via XKCD) By developing your ability to code (in R or Python, or whatever works for you–Julia?) you will open up a whole set of analyses that you would otherwise be unable to access. 6.1 Getting help Look up the help file for whatever you’re doing. Do this by using the syntax ?&lt;search item&gt; (for example ?c gets help on the vector command) as a shortcut on the console. Search the help files for a term you think is related. Can’t remember the command for making a sequence of integers? Go to the “Help” pane in RStudio and search in the search box for “sequence”. See if some of the top results get you what you need. The internet. Seriously. I am not kidding even a little bit. R has one of the most active and (surprisingly) helpful user communities I’ve ever encountered. Try going to google and searching for “How do I make a sequence of numbers in R?” You will find quite a bit of useful help. I find the following sites particularly helpful Stack Overflow Cross Validated/Stack Exchange Seriously, Google will get you most of the way to helpful answers for many basic R questions. I want to emphasize that looking up help is normal. I do it all the time. Learning to ask questions in helpful ways, how to quickly parse the information you find, and how to slightly alter the answers to suit your particular situation are key skills. 6.2 Learning more with Sensometrics Society This workshop was organized and sponsored by the Sensometrics Society. Want to learn more? We are hosting our biennial conference online 15-17 November, 2022: Sensometrics Society biannual conference banner In general, Sensometrics has a focus on methods and skills for the analysis of sensory data. If you’re interested in contributing to this focus (or just participating), please see our call for papers: Please submit an abstract for either an oral or poster presentation on the conference website at www.sensometrics2022.com. All submissions need to be received by October 17, 2022. Notification about acceptance will be made by October 24, 2022. All accepted contributions are invited to submit a full paper for inclusion in a virtual special issue of Food Quality and Preference (FQAP). You can contact us for more information or with questions at sensometrics2022@gmail.com. 6.3 Further reading/resources General R programming Data Carpentry’s R for Social Scientists (and, really the courses from The Carpentries in general) Wickham &amp; Grolemund’s R for Data Science The stat545 course website Healy’s Data Visualization My own (somewhat opinionated and eccentric) course from VT: FST 5984 Text analysis Jurafsky &amp; Martin’s seminal textbook, Speech &amp; Language Processing Silge &amp; Robinson’s Text Mining with R Chollet et al.’s Deep Learning with R 6.4 Questions/Comments If you end up being confused, come talk to me, or email me at jlahne at vt dot edu. I’d love to learn about what you’re working on! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
